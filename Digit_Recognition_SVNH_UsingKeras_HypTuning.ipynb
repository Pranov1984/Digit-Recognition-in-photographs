{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Digit Recognition_SVNH_UsingKeras_HypTuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranov1984/Digit-Recognition-in-photographs/blob/master/Digit_Recognition_SVNH_UsingKeras_HypTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AINLtxx-YDdE",
        "colab_type": "text"
      },
      "source": [
        "It was seen (in the other jupyter notebook uploaded) that KNN is not doing very well with classification of images and it is taking a lot of time for the algorithm to run and classify.\n",
        "\n",
        "In order for a robot or a computer to perform tasks, it must recognize what it is looking at. Given an image a computer must be able to classify what the image represents. While this is a fairly simple task for humans, it is not an easy task for computers. Computers must go through a series of steps in order to classify a single image.\n",
        "\n",
        "Object recognition is challenging for several reasons. The first and most obvious reason is that there are about 10,000 to 30,000 different object categories. The second reason is the viewpoint variation where many objects can look different from different angles. The third reason is illumination in which lighting makes the same objects look like different objects. The fourth reason is background clutter in which the classifier cannot distinguish the object from its background. Other challenges include scale deformation, occlusion, and intra-class variation. Hence classical algorithms like KNN don't do well.\n",
        "\n",
        "Deep Learning uses forward and backward propagation to be able to handle these challenges. The ability of the neural networks to compare the predicted with the actual and then use the difference or loss to send signal backwards (into the netork) for parameters to be adjusted helps to give better results.\n",
        "\n",
        "Here we the accuracy was seen to improve gradually we introduced non-linearity, batchnormalization, drop out and data augmentation. When convolution networks were added to the mix, the best scores were achieved. We achieved an accuracy of 91%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s77TUEzHVvst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l6HlsaaWTIz",
        "colab_type": "code",
        "outputId": "8619d31e-b10d-41d3-d6a6-d46f74fb8dd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJel-s2GWpNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the data\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "ds=h5py.File('/content/drive/My Drive/Colab Notebooks/AIML/ANN/Project SVNH - NN & DL/SVHN_single_grey1.h5','r')\n",
        "\n",
        "X_train = ds['X_train'][:]\n",
        "y_train = ds['y_train'][:]\n",
        "X_test = ds['X_test'][:]\n",
        "y_test = ds['y_test'][:]\n",
        "\n",
        "# Close this file\n",
        "ds.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75IEn0o2Ymin",
        "colab_type": "code",
        "outputId": "f6899358-78ca-4c68-d2cd-f862a6faccff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape , X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((42000, 32, 32), (18000, 32, 32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tESs-aDXYvgY",
        "colab_type": "code",
        "outputId": "015ca622-cbf0-49d9-985f-6aadc1aea41a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0O2b61IZfle",
        "colab_type": "code",
        "outputId": "825462f9-3527-4dc9-beb8-b237f202a7bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# visualizing the data\n",
        "plt.imshow(X_train[0])\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 31.5, 31.5, -0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAKOx52baHT-",
        "colab_type": "code",
        "outputId": "59f7b717-6b0c-4b60-f1d6-bdd8dd074198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        }
      },
      "source": [
        "plt.figure(figsize=(10,1))\n",
        "for i in range(10):\n",
        "  plt.subplot(1,10,i+1)\n",
        "  plt.imshow(X_train[i], cmap=\"gray\")\n",
        "  plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print('label for each of the above image: %s' % (y_train[0:10]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAA9CAYAAACpzLMWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhqM4BwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x72 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPnMYZh2cEkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshaping X data: (n, 32, 32) => (n, 1024)\n",
        "X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy-AZeeBcVAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting y data into categorical (one-hot encoding)\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgxGXVMNj0r1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw35xxZyj9rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, random_state=1, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3smfemFcyQX",
        "colab_type": "code",
        "outputId": "1f4a6348-ca00-4697-db32-f8b79b8da5b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "X_train.shape, X_test.shape, X_val.shape, y_test.shape, y_train.shape, y_val.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((33600, 1024),\n",
              " (18000, 1024),\n",
              " (8400, 1024),\n",
              " (18000, 10),\n",
              " (33600, 10),\n",
              " (8400, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThX-ScEToMxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalize the data\n",
        "\n",
        "X_train = X_train/255\n",
        "X_test = X_test/255\n",
        "X_val = X_val/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isd0WJv5gYos",
        "colab_type": "text"
      },
      "source": [
        "Build a Basic NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYLegAcrgXMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Activation, Dense\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ol8wKSWg15u",
        "colab_type": "code",
        "outputId": "ef858047-438f-4ac0-9fb1-e8a3308796dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeCYg7BRg8Ze",
        "colab_type": "code",
        "outputId": "3744045f-70be-4e62-bdd3-c00b8a2ba823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model.add(Dense(50,input_shape=(1024,)))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(50))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWYOnVUYh-9r",
        "colab_type": "code",
        "outputId": "e892cb8b-ae0b-44de-94c0-579b27f10fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "sgd=optimizers.SGD(lr=0.01)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtTEm7B2irP-",
        "colab_type": "code",
        "outputId": "6bab0df5-d0c2-408b-8804-8bbd527be0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fit=model.fit(X_train, y_train, batch_size=200, epochs=100, verbose=1,  validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "33600/33600 [==============================] - 10s 309us/step - loss: 2.3397 - acc: 0.0983 - val_loss: 2.3037 - val_acc: 0.1008\n",
            "Epoch 2/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3032 - acc: 0.1006 - val_loss: 2.3030 - val_acc: 0.1019\n",
            "Epoch 3/100\n",
            "33600/33600 [==============================] - 1s 19us/step - loss: 2.3030 - acc: 0.1005 - val_loss: 2.3030 - val_acc: 0.0998\n",
            "Epoch 4/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3030 - acc: 0.1008 - val_loss: 2.3031 - val_acc: 0.1012\n",
            "Epoch 5/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3029 - acc: 0.1028 - val_loss: 2.3030 - val_acc: 0.1041\n",
            "Epoch 6/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3030 - acc: 0.1003 - val_loss: 2.3029 - val_acc: 0.1039\n",
            "Epoch 7/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3029 - acc: 0.1002 - val_loss: 2.3030 - val_acc: 0.0996\n",
            "Epoch 8/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3029 - acc: 0.1030 - val_loss: 2.3030 - val_acc: 0.0954\n",
            "Epoch 9/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3028 - acc: 0.1026 - val_loss: 2.3028 - val_acc: 0.1022\n",
            "Epoch 10/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3028 - acc: 0.1001 - val_loss: 2.3028 - val_acc: 0.0985\n",
            "Epoch 11/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3028 - acc: 0.1015 - val_loss: 2.3028 - val_acc: 0.1108\n",
            "Epoch 12/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3028 - acc: 0.1024 - val_loss: 2.3029 - val_acc: 0.1014\n",
            "Epoch 13/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3028 - acc: 0.0996 - val_loss: 2.3028 - val_acc: 0.0995\n",
            "Epoch 14/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3027 - acc: 0.1021 - val_loss: 2.3029 - val_acc: 0.0970\n",
            "Epoch 15/100\n",
            "33600/33600 [==============================] - 1s 22us/step - loss: 2.3027 - acc: 0.0993 - val_loss: 2.3028 - val_acc: 0.0989\n",
            "Epoch 16/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3026 - acc: 0.1021 - val_loss: 2.3028 - val_acc: 0.0967\n",
            "Epoch 17/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3026 - acc: 0.1014 - val_loss: 2.3028 - val_acc: 0.0997\n",
            "Epoch 18/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3026 - acc: 0.1035 - val_loss: 2.3026 - val_acc: 0.1017\n",
            "Epoch 19/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3026 - acc: 0.1021 - val_loss: 2.3029 - val_acc: 0.0969\n",
            "Epoch 20/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3026 - acc: 0.1030 - val_loss: 2.3026 - val_acc: 0.0988\n",
            "Epoch 21/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3025 - acc: 0.1034 - val_loss: 2.3027 - val_acc: 0.0996\n",
            "Epoch 22/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3025 - acc: 0.1016 - val_loss: 2.3026 - val_acc: 0.0974\n",
            "Epoch 23/100\n",
            "33600/33600 [==============================] - 1s 19us/step - loss: 2.3024 - acc: 0.1024 - val_loss: 2.3027 - val_acc: 0.0989\n",
            "Epoch 24/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3025 - acc: 0.1020 - val_loss: 2.3026 - val_acc: 0.1019\n",
            "Epoch 25/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3025 - acc: 0.1012 - val_loss: 2.3025 - val_acc: 0.1012\n",
            "Epoch 26/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3024 - acc: 0.1031 - val_loss: 2.3026 - val_acc: 0.1002\n",
            "Epoch 27/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3024 - acc: 0.1032 - val_loss: 2.3026 - val_acc: 0.0991\n",
            "Epoch 28/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3024 - acc: 0.1022 - val_loss: 2.3024 - val_acc: 0.0981\n",
            "Epoch 29/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3023 - acc: 0.1025 - val_loss: 2.3023 - val_acc: 0.1044\n",
            "Epoch 30/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3023 - acc: 0.1038 - val_loss: 2.3024 - val_acc: 0.1052\n",
            "Epoch 31/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3023 - acc: 0.1011 - val_loss: 2.3023 - val_acc: 0.0997\n",
            "Epoch 32/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3022 - acc: 0.1051 - val_loss: 2.3023 - val_acc: 0.1058\n",
            "Epoch 33/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3022 - acc: 0.1029 - val_loss: 2.3022 - val_acc: 0.1037\n",
            "Epoch 34/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3022 - acc: 0.1049 - val_loss: 2.3025 - val_acc: 0.0964\n",
            "Epoch 35/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3022 - acc: 0.1021 - val_loss: 2.3025 - val_acc: 0.0988\n",
            "Epoch 36/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3022 - acc: 0.1039 - val_loss: 2.3025 - val_acc: 0.1026\n",
            "Epoch 37/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3021 - acc: 0.1050 - val_loss: 2.3024 - val_acc: 0.1034\n",
            "Epoch 38/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3021 - acc: 0.1058 - val_loss: 2.3022 - val_acc: 0.1011\n",
            "Epoch 39/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3021 - acc: 0.1049 - val_loss: 2.3023 - val_acc: 0.1093\n",
            "Epoch 40/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3021 - acc: 0.1093 - val_loss: 2.3022 - val_acc: 0.1028\n",
            "Epoch 41/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3020 - acc: 0.1053 - val_loss: 2.3022 - val_acc: 0.1053\n",
            "Epoch 42/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3020 - acc: 0.1048 - val_loss: 2.3021 - val_acc: 0.0992\n",
            "Epoch 43/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3020 - acc: 0.1038 - val_loss: 2.3023 - val_acc: 0.1023\n",
            "Epoch 44/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3020 - acc: 0.1023 - val_loss: 2.3021 - val_acc: 0.1112\n",
            "Epoch 45/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3019 - acc: 0.1044 - val_loss: 2.3021 - val_acc: 0.1183\n",
            "Epoch 46/100\n",
            "33600/33600 [==============================] - 1s 19us/step - loss: 2.3020 - acc: 0.1077 - val_loss: 2.3020 - val_acc: 0.0988\n",
            "Epoch 47/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3019 - acc: 0.1054 - val_loss: 2.3021 - val_acc: 0.1077\n",
            "Epoch 48/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3019 - acc: 0.1091 - val_loss: 2.3022 - val_acc: 0.1005\n",
            "Epoch 49/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3019 - acc: 0.1057 - val_loss: 2.3021 - val_acc: 0.0998\n",
            "Epoch 50/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3019 - acc: 0.1054 - val_loss: 2.3020 - val_acc: 0.1050\n",
            "Epoch 51/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3018 - acc: 0.1069 - val_loss: 2.3021 - val_acc: 0.1023\n",
            "Epoch 52/100\n",
            "33600/33600 [==============================] - 1s 19us/step - loss: 2.3018 - acc: 0.1052 - val_loss: 2.3020 - val_acc: 0.1054\n",
            "Epoch 53/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3018 - acc: 0.1044 - val_loss: 2.3019 - val_acc: 0.1033\n",
            "Epoch 54/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3018 - acc: 0.1061 - val_loss: 2.3019 - val_acc: 0.1003\n",
            "Epoch 55/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3018 - acc: 0.1055 - val_loss: 2.3019 - val_acc: 0.1027\n",
            "Epoch 56/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3017 - acc: 0.1080 - val_loss: 2.3020 - val_acc: 0.1011\n",
            "Epoch 57/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3017 - acc: 0.1083 - val_loss: 2.3021 - val_acc: 0.0998\n",
            "Epoch 58/100\n",
            "33600/33600 [==============================] - 1s 19us/step - loss: 2.3017 - acc: 0.1087 - val_loss: 2.3019 - val_acc: 0.1027\n",
            "Epoch 59/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3017 - acc: 0.1064 - val_loss: 2.3018 - val_acc: 0.1153\n",
            "Epoch 60/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3016 - acc: 0.1074 - val_loss: 2.3017 - val_acc: 0.1028\n",
            "Epoch 61/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3017 - acc: 0.1057 - val_loss: 2.3018 - val_acc: 0.1151\n",
            "Epoch 62/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3016 - acc: 0.1100 - val_loss: 2.3019 - val_acc: 0.0971\n",
            "Epoch 63/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3015 - acc: 0.1069 - val_loss: 2.3017 - val_acc: 0.1094\n",
            "Epoch 64/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3015 - acc: 0.1090 - val_loss: 2.3016 - val_acc: 0.1044\n",
            "Epoch 65/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3016 - acc: 0.1077 - val_loss: 2.3016 - val_acc: 0.1152\n",
            "Epoch 66/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3015 - acc: 0.1107 - val_loss: 2.3017 - val_acc: 0.1041\n",
            "Epoch 67/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3015 - acc: 0.1109 - val_loss: 2.3016 - val_acc: 0.1032\n",
            "Epoch 68/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3016 - val_acc: 0.1159\n",
            "Epoch 69/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3015 - acc: 0.1145 - val_loss: 2.3016 - val_acc: 0.1146\n",
            "Epoch 70/100\n",
            "33600/33600 [==============================] - 1s 19us/step - loss: 2.3014 - acc: 0.1054 - val_loss: 2.3016 - val_acc: 0.1128\n",
            "Epoch 71/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3014 - acc: 0.1121 - val_loss: 2.3016 - val_acc: 0.1083\n",
            "Epoch 72/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3014 - acc: 0.1110 - val_loss: 2.3017 - val_acc: 0.1066\n",
            "Epoch 73/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3014 - acc: 0.1092 - val_loss: 2.3016 - val_acc: 0.1110\n",
            "Epoch 74/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3013 - acc: 0.1120 - val_loss: 2.3017 - val_acc: 0.1143\n",
            "Epoch 75/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3013 - acc: 0.1104 - val_loss: 2.3014 - val_acc: 0.1087\n",
            "Epoch 76/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3013 - acc: 0.1109 - val_loss: 2.3014 - val_acc: 0.1132\n",
            "Epoch 77/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3013 - acc: 0.1115 - val_loss: 2.3015 - val_acc: 0.1083\n",
            "Epoch 78/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3012 - acc: 0.1119 - val_loss: 2.3016 - val_acc: 0.1119\n",
            "Epoch 79/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3012 - acc: 0.1105 - val_loss: 2.3014 - val_acc: 0.1117\n",
            "Epoch 80/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3012 - acc: 0.1141 - val_loss: 2.3014 - val_acc: 0.1134\n",
            "Epoch 81/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3012 - acc: 0.1115 - val_loss: 2.3014 - val_acc: 0.1159\n",
            "Epoch 82/100\n",
            "33600/33600 [==============================] - 1s 19us/step - loss: 2.3012 - acc: 0.1154 - val_loss: 2.3014 - val_acc: 0.1099\n",
            "Epoch 83/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3011 - acc: 0.1095 - val_loss: 2.3014 - val_acc: 0.1095\n",
            "Epoch 84/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3011 - acc: 0.1148 - val_loss: 2.3014 - val_acc: 0.1021\n",
            "Epoch 85/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3011 - acc: 0.1090 - val_loss: 2.3015 - val_acc: 0.1037\n",
            "Epoch 86/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3011 - acc: 0.1130 - val_loss: 2.3014 - val_acc: 0.1091\n",
            "Epoch 87/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3010 - acc: 0.1136 - val_loss: 2.3012 - val_acc: 0.1205\n",
            "Epoch 88/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3010 - acc: 0.1163 - val_loss: 2.3013 - val_acc: 0.1099\n",
            "Epoch 89/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3010 - acc: 0.1150 - val_loss: 2.3012 - val_acc: 0.1191\n",
            "Epoch 90/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3010 - acc: 0.1153 - val_loss: 2.3014 - val_acc: 0.1157\n",
            "Epoch 91/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3010 - acc: 0.1123 - val_loss: 2.3012 - val_acc: 0.1142\n",
            "Epoch 92/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3009 - acc: 0.1159 - val_loss: 2.3012 - val_acc: 0.1164\n",
            "Epoch 93/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3009 - acc: 0.1146 - val_loss: 2.3013 - val_acc: 0.1103\n",
            "Epoch 94/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3010 - acc: 0.1158 - val_loss: 2.3010 - val_acc: 0.1187\n",
            "Epoch 95/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3009 - acc: 0.1165 - val_loss: 2.3011 - val_acc: 0.1223\n",
            "Epoch 96/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3008 - acc: 0.1150 - val_loss: 2.3010 - val_acc: 0.1129\n",
            "Epoch 97/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3009 - acc: 0.1157 - val_loss: 2.3010 - val_acc: 0.1187\n",
            "Epoch 98/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3008 - acc: 0.1146 - val_loss: 2.3012 - val_acc: 0.1118\n",
            "Epoch 99/100\n",
            "33600/33600 [==============================] - 1s 21us/step - loss: 2.3008 - acc: 0.1149 - val_loss: 2.3011 - val_acc: 0.1141\n",
            "Epoch 100/100\n",
            "33600/33600 [==============================] - 1s 20us/step - loss: 2.3008 - acc: 0.1158 - val_loss: 2.3010 - val_acc: 0.1126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssAoAgPZjK8h",
        "colab_type": "code",
        "outputId": "f948800d-6e2d-4efa-c64c-9199972c468b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "results=model.evaluate(X_val, y_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 0s 36us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBsvW3T4jZLt",
        "colab_type": "code",
        "outputId": "b668f879-a1f6-4327-fbde-2c615b604b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('Accuracy:',results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPzj64QunXQf",
        "colab_type": "text"
      },
      "source": [
        "Enhance the model with weight initialization, nonlinearity, regularization with drop outs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqoRiMtGjn3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a function for training and printing results from evaluation on test set\n",
        "\n",
        "def mlp_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(100,input_shape=(1024,), kernel_initializer='he_normal'))\n",
        "  model.add(Activation('sigmoid'))\n",
        "  model.add(Dense(100,kernel_initializer='he_normal'))\n",
        "  model.add(Activation('sigmoid'))\n",
        "  model.add(Dense(100, kernel_initializer='he_normal'))\n",
        "  model.add(Activation('sigmoid'))\n",
        "  model.add(Dense(100, kernel_initializer='he_normal'))\n",
        "  model.add(Activation('sigmoid'))\n",
        "  model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  sgd=optimizers.SGD(lr=0.001)\n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYGPbjZDqN1V",
        "colab_type": "code",
        "outputId": "4e386960-e1a2-4467-a1a4-3945fb90d762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model=mlp_model()\n",
        "history=model.fit(X_train,y_train, batch_size=500, epochs=100, verbose=1,  validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/100\n",
            "33600/33600 [==============================] - 1s 18us/step - loss: 2.4312 - acc: 0.1001 - val_loss: 2.4105 - val_acc: 0.1016\n",
            "Epoch 2/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3964 - acc: 0.1001 - val_loss: 2.3825 - val_acc: 0.1016\n",
            "Epoch 3/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3723 - acc: 0.1001 - val_loss: 2.3626 - val_acc: 0.1016\n",
            "Epoch 4/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3548 - acc: 0.1001 - val_loss: 2.3475 - val_acc: 0.1016\n",
            "Epoch 5/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3415 - acc: 0.1001 - val_loss: 2.3361 - val_acc: 0.1016\n",
            "Epoch 6/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3314 - acc: 0.1001 - val_loss: 2.3273 - val_acc: 0.1016\n",
            "Epoch 7/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3236 - acc: 0.1001 - val_loss: 2.3207 - val_acc: 0.1016\n",
            "Epoch 8/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3179 - acc: 0.1001 - val_loss: 2.3158 - val_acc: 0.1016\n",
            "Epoch 9/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3136 - acc: 0.1001 - val_loss: 2.3121 - val_acc: 0.1016\n",
            "Epoch 10/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3104 - acc: 0.1001 - val_loss: 2.3094 - val_acc: 0.1016\n",
            "Epoch 11/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3081 - acc: 0.1001 - val_loss: 2.3074 - val_acc: 0.1016\n",
            "Epoch 12/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3064 - acc: 0.1002 - val_loss: 2.3060 - val_acc: 0.1017\n",
            "Epoch 13/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3052 - acc: 0.1001 - val_loss: 2.3050 - val_acc: 0.1012\n",
            "Epoch 14/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3044 - acc: 0.1001 - val_loss: 2.3043 - val_acc: 0.1008\n",
            "Epoch 15/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3038 - acc: 0.1003 - val_loss: 2.3038 - val_acc: 0.1005\n",
            "Epoch 16/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3034 - acc: 0.1017 - val_loss: 2.3034 - val_acc: 0.1006\n",
            "Epoch 17/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3031 - acc: 0.1016 - val_loss: 2.3032 - val_acc: 0.1008\n",
            "Epoch 18/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3030 - acc: 0.1021 - val_loss: 2.3031 - val_acc: 0.1007\n",
            "Epoch 19/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3029 - acc: 0.1011 - val_loss: 2.3030 - val_acc: 0.1007\n",
            "Epoch 20/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3028 - acc: 0.1008 - val_loss: 2.3029 - val_acc: 0.1002\n",
            "Epoch 21/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3027 - acc: 0.1001 - val_loss: 2.3029 - val_acc: 0.0993\n",
            "Epoch 22/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3027 - acc: 0.1007 - val_loss: 2.3028 - val_acc: 0.1002\n",
            "Epoch 23/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1007 - val_loss: 2.3028 - val_acc: 0.0992\n",
            "Epoch 24/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3026 - acc: 0.1011 - val_loss: 2.3028 - val_acc: 0.0983\n",
            "Epoch 25/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1005 - val_loss: 2.3028 - val_acc: 0.0973\n",
            "Epoch 26/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3028 - val_acc: 0.0990\n",
            "Epoch 27/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3027 - val_acc: 0.0983\n",
            "Epoch 28/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3027 - val_acc: 0.0974\n",
            "Epoch 29/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3026 - acc: 0.0993 - val_loss: 2.3027 - val_acc: 0.0984\n",
            "Epoch 30/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0991 - val_loss: 2.3028 - val_acc: 0.0977\n",
            "Epoch 31/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0993 - val_loss: 2.3028 - val_acc: 0.0978\n",
            "Epoch 32/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1010 - val_loss: 2.3028 - val_acc: 0.0984\n",
            "Epoch 33/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1000 - val_loss: 2.3027 - val_acc: 0.0978\n",
            "Epoch 34/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3026 - acc: 0.0999 - val_loss: 2.3027 - val_acc: 0.0981\n",
            "Epoch 35/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3026 - acc: 0.1011 - val_loss: 2.3027 - val_acc: 0.0979\n",
            "Epoch 36/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3026 - acc: 0.0991 - val_loss: 2.3027 - val_acc: 0.0977\n",
            "Epoch 37/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0996 - val_loss: 2.3027 - val_acc: 0.0973\n",
            "Epoch 38/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3027 - val_acc: 0.0979\n",
            "Epoch 39/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3026 - acc: 0.1010 - val_loss: 2.3027 - val_acc: 0.0986\n",
            "Epoch 40/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1021 - val_loss: 2.3027 - val_acc: 0.0976\n",
            "Epoch 41/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3026 - acc: 0.1007 - val_loss: 2.3028 - val_acc: 0.0989\n",
            "Epoch 42/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1008 - val_loss: 2.3027 - val_acc: 0.0984\n",
            "Epoch 43/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1004 - val_loss: 2.3027 - val_acc: 0.0982\n",
            "Epoch 44/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0999 - val_loss: 2.3028 - val_acc: 0.0984\n",
            "Epoch 45/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1007 - val_loss: 2.3028 - val_acc: 0.0982\n",
            "Epoch 46/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1024 - val_loss: 2.3027 - val_acc: 0.0992\n",
            "Epoch 47/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1008 - val_loss: 2.3027 - val_acc: 0.0986\n",
            "Epoch 48/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0998 - val_loss: 2.3027 - val_acc: 0.0974\n",
            "Epoch 49/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3026 - acc: 0.1004 - val_loss: 2.3027 - val_acc: 0.0972\n",
            "Epoch 50/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1007 - val_loss: 2.3027 - val_acc: 0.0981\n",
            "Epoch 51/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3026 - acc: 0.0998 - val_loss: 2.3027 - val_acc: 0.0981\n",
            "Epoch 52/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1013 - val_loss: 2.3027 - val_acc: 0.0982\n",
            "Epoch 53/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1012 - val_loss: 2.3027 - val_acc: 0.0979\n",
            "Epoch 54/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0991 - val_loss: 2.3027 - val_acc: 0.0977\n",
            "Epoch 55/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1006 - val_loss: 2.3027 - val_acc: 0.0983\n",
            "Epoch 56/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3026 - acc: 0.0987 - val_loss: 2.3027 - val_acc: 0.0975\n",
            "Epoch 57/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3026 - acc: 0.0998 - val_loss: 2.3027 - val_acc: 0.0982\n",
            "Epoch 58/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1003 - val_loss: 2.3027 - val_acc: 0.0987\n",
            "Epoch 59/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1013 - val_loss: 2.3027 - val_acc: 0.0989\n",
            "Epoch 60/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1000 - val_loss: 2.3027 - val_acc: 0.0992\n",
            "Epoch 61/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1013 - val_loss: 2.3027 - val_acc: 0.0998\n",
            "Epoch 62/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1010 - val_loss: 2.3027 - val_acc: 0.0990\n",
            "Epoch 63/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3026 - acc: 0.1002 - val_loss: 2.3027 - val_acc: 0.1001\n",
            "Epoch 64/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1001 - val_loss: 2.3027 - val_acc: 0.0998\n",
            "Epoch 65/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1002 - val_loss: 2.3027 - val_acc: 0.1007\n",
            "Epoch 66/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1004 - val_loss: 2.3027 - val_acc: 0.0999\n",
            "Epoch 67/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3026 - acc: 0.1003 - val_loss: 2.3027 - val_acc: 0.0991\n",
            "Epoch 68/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3026 - acc: 0.1004 - val_loss: 2.3027 - val_acc: 0.1003\n",
            "Epoch 69/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3026 - acc: 0.1000 - val_loss: 2.3027 - val_acc: 0.0996\n",
            "Epoch 70/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1000 - val_loss: 2.3027 - val_acc: 0.0992\n",
            "Epoch 71/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1004 - val_loss: 2.3027 - val_acc: 0.0991\n",
            "Epoch 72/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1004 - val_loss: 2.3027 - val_acc: 0.0997\n",
            "Epoch 73/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1001 - val_loss: 2.3027 - val_acc: 0.0978\n",
            "Epoch 74/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0995 - val_loss: 2.3027 - val_acc: 0.0991\n",
            "Epoch 75/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1004 - val_loss: 2.3027 - val_acc: 0.0979\n",
            "Epoch 76/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0996 - val_loss: 2.3027 - val_acc: 0.0977\n",
            "Epoch 77/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1003 - val_loss: 2.3028 - val_acc: 0.0975\n",
            "Epoch 78/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1004 - val_loss: 2.3028 - val_acc: 0.0984\n",
            "Epoch 79/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1012 - val_loss: 2.3028 - val_acc: 0.0979\n",
            "Epoch 80/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3026 - acc: 0.0990 - val_loss: 2.3027 - val_acc: 0.0979\n",
            "Epoch 81/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0996 - val_loss: 2.3027 - val_acc: 0.0979\n",
            "Epoch 82/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1006 - val_loss: 2.3027 - val_acc: 0.0981\n",
            "Epoch 83/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3026 - acc: 0.1003 - val_loss: 2.3027 - val_acc: 0.0978\n",
            "Epoch 84/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1017 - val_loss: 2.3027 - val_acc: 0.0976\n",
            "Epoch 85/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1002 - val_loss: 2.3027 - val_acc: 0.0997\n",
            "Epoch 86/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1007 - val_loss: 2.3028 - val_acc: 0.0988\n",
            "Epoch 87/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1013 - val_loss: 2.3027 - val_acc: 0.0982\n",
            "Epoch 88/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0986 - val_loss: 2.3027 - val_acc: 0.0981\n",
            "Epoch 89/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3027 - val_acc: 0.0974\n",
            "Epoch 90/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1001 - val_loss: 2.3027 - val_acc: 0.0979\n",
            "Epoch 91/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3026 - acc: 0.1009 - val_loss: 2.3027 - val_acc: 0.0976\n",
            "Epoch 92/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0990 - val_loss: 2.3027 - val_acc: 0.0985\n",
            "Epoch 93/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1009 - val_loss: 2.3027 - val_acc: 0.0978\n",
            "Epoch 94/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0983 - val_loss: 2.3027 - val_acc: 0.0983\n",
            "Epoch 95/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1008 - val_loss: 2.3027 - val_acc: 0.0974\n",
            "Epoch 96/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1006 - val_loss: 2.3027 - val_acc: 0.0973\n",
            "Epoch 97/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0995 - val_loss: 2.3027 - val_acc: 0.0969\n",
            "Epoch 98/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.0994 - val_loss: 2.3027 - val_acc: 0.0992\n",
            "Epoch 99/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.3026 - acc: 0.1000 - val_loss: 2.3027 - val_acc: 0.0987\n",
            "Epoch 100/100\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 2.3026 - acc: 0.1005 - val_loss: 2.3027 - val_acc: 0.0988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nBJVnJerPRq",
        "colab_type": "code",
        "outputId": "ff883d02-786a-434e-a48b-c26322b62a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "results=model.evaluate(X_val,y_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 0s 39us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct1R-0RJrYvj",
        "colab_type": "code",
        "outputId": "1791279e-2fdb-4b60-8473-836fb72f6105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:  0.09297619047619048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dha8wRsrdoZ",
        "colab_type": "code",
        "outputId": "eafe96a1-ac97-46a6-bfd2-899cff3cf18f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "NN_val_loss = history.history['val_loss']\n",
        "NN_train_loss = history.history['loss']\n",
        "epochs = range(1,101)\n",
        "\n",
        "plt.plot(epochs, NN_val_loss, 'b+', label='Validation Loss')\n",
        "plt.plot(epochs, NN_train_loss, 'bo', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOZRXxBBri2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Same rate of decrease in loss for both training and validation and both of them are underfitting"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIZkZsXwsOpN",
        "colab_type": "text"
      },
      "source": [
        "# Add advance non linearity through relu and increase the batchsize plus the epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gzCpa0IrdXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlp_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(50,input_shape=(1024,), kernel_initializer='he_normal'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(50,kernel_initializer='he_normal'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(50, kernel_initializer='he_normal'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(50, kernel_initializer='he_normal'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  sgd=optimizers.SGD(lr=0.001)\n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEIbwN0pseIC",
        "colab_type": "code",
        "outputId": "92d9146b-4602-4089-dfaf-cf3b41658416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=2048,epochs = 500, verbose = 1,  validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/500\n",
            "33600/33600 [==============================] - 1s 18us/step - loss: 2.3766 - acc: 0.1125 - val_loss: 2.3610 - val_acc: 0.1134\n",
            "Epoch 2/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3478 - acc: 0.1161 - val_loss: 2.3425 - val_acc: 0.1142\n",
            "Epoch 3/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3339 - acc: 0.1160 - val_loss: 2.3322 - val_acc: 0.1142\n",
            "Epoch 4/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3260 - acc: 0.1149 - val_loss: 2.3262 - val_acc: 0.1137\n",
            "Epoch 5/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3211 - acc: 0.1134 - val_loss: 2.3223 - val_acc: 0.1124\n",
            "Epoch 6/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3179 - acc: 0.1132 - val_loss: 2.3195 - val_acc: 0.1093\n",
            "Epoch 7/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3156 - acc: 0.1118 - val_loss: 2.3174 - val_acc: 0.1080\n",
            "Epoch 8/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3138 - acc: 0.1111 - val_loss: 2.3155 - val_acc: 0.1059\n",
            "Epoch 9/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3122 - acc: 0.1103 - val_loss: 2.3141 - val_acc: 0.1050\n",
            "Epoch 10/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3109 - acc: 0.1097 - val_loss: 2.3127 - val_acc: 0.1035\n",
            "Epoch 11/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3098 - acc: 0.1093 - val_loss: 2.3116 - val_acc: 0.1033\n",
            "Epoch 12/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.3087 - acc: 0.1100 - val_loss: 2.3106 - val_acc: 0.1028\n",
            "Epoch 13/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3079 - acc: 0.1110 - val_loss: 2.3097 - val_acc: 0.1033\n",
            "Epoch 14/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3071 - acc: 0.1108 - val_loss: 2.3090 - val_acc: 0.1034\n",
            "Epoch 15/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3065 - acc: 0.1118 - val_loss: 2.3083 - val_acc: 0.1043\n",
            "Epoch 16/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.3059 - acc: 0.1126 - val_loss: 2.3078 - val_acc: 0.1043\n",
            "Epoch 17/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3054 - acc: 0.1132 - val_loss: 2.3072 - val_acc: 0.1051\n",
            "Epoch 18/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3049 - acc: 0.1134 - val_loss: 2.3067 - val_acc: 0.1066\n",
            "Epoch 19/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3045 - acc: 0.1131 - val_loss: 2.3063 - val_acc: 0.1068\n",
            "Epoch 20/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3041 - acc: 0.1136 - val_loss: 2.3058 - val_acc: 0.1066\n",
            "Epoch 21/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3037 - acc: 0.1139 - val_loss: 2.3054 - val_acc: 0.1071\n",
            "Epoch 22/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3034 - acc: 0.1137 - val_loss: 2.3051 - val_acc: 0.1072\n",
            "Epoch 23/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3031 - acc: 0.1137 - val_loss: 2.3047 - val_acc: 0.1071\n",
            "Epoch 24/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3027 - acc: 0.1139 - val_loss: 2.3044 - val_acc: 0.1067\n",
            "Epoch 25/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3024 - acc: 0.1134 - val_loss: 2.3040 - val_acc: 0.1070\n",
            "Epoch 26/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3021 - acc: 0.1140 - val_loss: 2.3037 - val_acc: 0.1073\n",
            "Epoch 27/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3018 - acc: 0.1146 - val_loss: 2.3034 - val_acc: 0.1075\n",
            "Epoch 28/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3015 - acc: 0.1149 - val_loss: 2.3030 - val_acc: 0.1071\n",
            "Epoch 29/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3012 - acc: 0.1154 - val_loss: 2.3027 - val_acc: 0.1070\n",
            "Epoch 30/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3008 - acc: 0.1159 - val_loss: 2.3024 - val_acc: 0.1075\n",
            "Epoch 31/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3005 - acc: 0.1162 - val_loss: 2.3020 - val_acc: 0.1081\n",
            "Epoch 32/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.3002 - acc: 0.1167 - val_loss: 2.3017 - val_acc: 0.1078\n",
            "Epoch 33/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2999 - acc: 0.1165 - val_loss: 2.3013 - val_acc: 0.1087\n",
            "Epoch 34/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2996 - acc: 0.1161 - val_loss: 2.3010 - val_acc: 0.1093\n",
            "Epoch 35/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2993 - acc: 0.1172 - val_loss: 2.3007 - val_acc: 0.1106\n",
            "Epoch 36/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2990 - acc: 0.1174 - val_loss: 2.3003 - val_acc: 0.1120\n",
            "Epoch 37/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2986 - acc: 0.1183 - val_loss: 2.3000 - val_acc: 0.1117\n",
            "Epoch 38/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2983 - acc: 0.1187 - val_loss: 2.2996 - val_acc: 0.1112\n",
            "Epoch 39/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2980 - acc: 0.1189 - val_loss: 2.2993 - val_acc: 0.1129\n",
            "Epoch 40/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2977 - acc: 0.1196 - val_loss: 2.2990 - val_acc: 0.1140\n",
            "Epoch 41/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2975 - acc: 0.1202 - val_loss: 2.2987 - val_acc: 0.1141\n",
            "Epoch 42/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2972 - acc: 0.1199 - val_loss: 2.2984 - val_acc: 0.1146\n",
            "Epoch 43/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2970 - acc: 0.1205 - val_loss: 2.2982 - val_acc: 0.1146\n",
            "Epoch 44/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2968 - acc: 0.1211 - val_loss: 2.2979 - val_acc: 0.1155\n",
            "Epoch 45/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2966 - acc: 0.1219 - val_loss: 2.2977 - val_acc: 0.1160\n",
            "Epoch 46/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2964 - acc: 0.1218 - val_loss: 2.2975 - val_acc: 0.1176\n",
            "Epoch 47/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2962 - acc: 0.1225 - val_loss: 2.2972 - val_acc: 0.1182\n",
            "Epoch 48/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2960 - acc: 0.1232 - val_loss: 2.2970 - val_acc: 0.1193\n",
            "Epoch 49/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2958 - acc: 0.1243 - val_loss: 2.2968 - val_acc: 0.1200\n",
            "Epoch 50/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2956 - acc: 0.1251 - val_loss: 2.2967 - val_acc: 0.1208\n",
            "Epoch 51/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2954 - acc: 0.1259 - val_loss: 2.2965 - val_acc: 0.1207\n",
            "Epoch 52/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2953 - acc: 0.1269 - val_loss: 2.2963 - val_acc: 0.1219\n",
            "Epoch 53/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2951 - acc: 0.1279 - val_loss: 2.2961 - val_acc: 0.1224\n",
            "Epoch 54/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2950 - acc: 0.1283 - val_loss: 2.2960 - val_acc: 0.1238\n",
            "Epoch 55/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2948 - acc: 0.1294 - val_loss: 2.2958 - val_acc: 0.1241\n",
            "Epoch 56/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2947 - acc: 0.1303 - val_loss: 2.2957 - val_acc: 0.1244\n",
            "Epoch 57/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2945 - acc: 0.1310 - val_loss: 2.2956 - val_acc: 0.1252\n",
            "Epoch 58/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2944 - acc: 0.1310 - val_loss: 2.2954 - val_acc: 0.1257\n",
            "Epoch 59/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2943 - acc: 0.1315 - val_loss: 2.2953 - val_acc: 0.1264\n",
            "Epoch 60/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2941 - acc: 0.1318 - val_loss: 2.2951 - val_acc: 0.1261\n",
            "Epoch 61/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2940 - acc: 0.1323 - val_loss: 2.2950 - val_acc: 0.1261\n",
            "Epoch 62/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2939 - acc: 0.1330 - val_loss: 2.2949 - val_acc: 0.1259\n",
            "Epoch 63/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2937 - acc: 0.1333 - val_loss: 2.2948 - val_acc: 0.1266\n",
            "Epoch 64/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2936 - acc: 0.1341 - val_loss: 2.2946 - val_acc: 0.1272\n",
            "Epoch 65/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2935 - acc: 0.1336 - val_loss: 2.2945 - val_acc: 0.1277\n",
            "Epoch 66/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2934 - acc: 0.1344 - val_loss: 2.2944 - val_acc: 0.1286\n",
            "Epoch 67/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2932 - acc: 0.1349 - val_loss: 2.2943 - val_acc: 0.1288\n",
            "Epoch 68/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2931 - acc: 0.1350 - val_loss: 2.2941 - val_acc: 0.1295\n",
            "Epoch 69/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2930 - acc: 0.1351 - val_loss: 2.2940 - val_acc: 0.1300\n",
            "Epoch 70/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2929 - acc: 0.1348 - val_loss: 2.2939 - val_acc: 0.1298\n",
            "Epoch 71/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2927 - acc: 0.1349 - val_loss: 2.2938 - val_acc: 0.1299\n",
            "Epoch 72/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2926 - acc: 0.1353 - val_loss: 2.2936 - val_acc: 0.1309\n",
            "Epoch 73/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2925 - acc: 0.1354 - val_loss: 2.2935 - val_acc: 0.1313\n",
            "Epoch 74/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2924 - acc: 0.1358 - val_loss: 2.2934 - val_acc: 0.1312\n",
            "Epoch 75/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2922 - acc: 0.1359 - val_loss: 2.2932 - val_acc: 0.1319\n",
            "Epoch 76/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2921 - acc: 0.1363 - val_loss: 2.2931 - val_acc: 0.1326\n",
            "Epoch 77/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2920 - acc: 0.1371 - val_loss: 2.2930 - val_acc: 0.1326\n",
            "Epoch 78/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2919 - acc: 0.1367 - val_loss: 2.2929 - val_acc: 0.1328\n",
            "Epoch 79/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2918 - acc: 0.1379 - val_loss: 2.2927 - val_acc: 0.1328\n",
            "Epoch 80/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2916 - acc: 0.1377 - val_loss: 2.2926 - val_acc: 0.1332\n",
            "Epoch 81/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2915 - acc: 0.1380 - val_loss: 2.2925 - val_acc: 0.1327\n",
            "Epoch 82/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2914 - acc: 0.1386 - val_loss: 2.2923 - val_acc: 0.1331\n",
            "Epoch 83/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2913 - acc: 0.1387 - val_loss: 2.2922 - val_acc: 0.1331\n",
            "Epoch 84/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2911 - acc: 0.1385 - val_loss: 2.2921 - val_acc: 0.1342\n",
            "Epoch 85/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2910 - acc: 0.1397 - val_loss: 2.2919 - val_acc: 0.1346\n",
            "Epoch 86/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2909 - acc: 0.1394 - val_loss: 2.2918 - val_acc: 0.1349\n",
            "Epoch 87/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2908 - acc: 0.1400 - val_loss: 2.2917 - val_acc: 0.1354\n",
            "Epoch 88/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2906 - acc: 0.1406 - val_loss: 2.2915 - val_acc: 0.1363\n",
            "Epoch 89/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2905 - acc: 0.1408 - val_loss: 2.2914 - val_acc: 0.1371\n",
            "Epoch 90/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2904 - acc: 0.1404 - val_loss: 2.2913 - val_acc: 0.1374\n",
            "Epoch 91/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2902 - acc: 0.1413 - val_loss: 2.2911 - val_acc: 0.1379\n",
            "Epoch 92/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2901 - acc: 0.1406 - val_loss: 2.2910 - val_acc: 0.1384\n",
            "Epoch 93/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2900 - acc: 0.1415 - val_loss: 2.2909 - val_acc: 0.1390\n",
            "Epoch 94/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2899 - acc: 0.1417 - val_loss: 2.2907 - val_acc: 0.1389\n",
            "Epoch 95/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2897 - acc: 0.1426 - val_loss: 2.2906 - val_acc: 0.1386\n",
            "Epoch 96/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2896 - acc: 0.1424 - val_loss: 2.2904 - val_acc: 0.1388\n",
            "Epoch 97/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2894 - acc: 0.1431 - val_loss: 2.2903 - val_acc: 0.1395\n",
            "Epoch 98/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2893 - acc: 0.1439 - val_loss: 2.2902 - val_acc: 0.1385\n",
            "Epoch 99/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2892 - acc: 0.1429 - val_loss: 2.2900 - val_acc: 0.1391\n",
            "Epoch 100/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2890 - acc: 0.1427 - val_loss: 2.2898 - val_acc: 0.1394\n",
            "Epoch 101/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2889 - acc: 0.1437 - val_loss: 2.2897 - val_acc: 0.1396\n",
            "Epoch 102/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2887 - acc: 0.1431 - val_loss: 2.2895 - val_acc: 0.1401\n",
            "Epoch 103/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2886 - acc: 0.1439 - val_loss: 2.2893 - val_acc: 0.1407\n",
            "Epoch 104/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2884 - acc: 0.1432 - val_loss: 2.2892 - val_acc: 0.1408\n",
            "Epoch 105/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2883 - acc: 0.1443 - val_loss: 2.2890 - val_acc: 0.1417\n",
            "Epoch 106/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2881 - acc: 0.1443 - val_loss: 2.2889 - val_acc: 0.1414\n",
            "Epoch 107/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2879 - acc: 0.1445 - val_loss: 2.2887 - val_acc: 0.1416\n",
            "Epoch 108/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2878 - acc: 0.1444 - val_loss: 2.2885 - val_acc: 0.1417\n",
            "Epoch 109/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2876 - acc: 0.1450 - val_loss: 2.2884 - val_acc: 0.1419\n",
            "Epoch 110/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2874 - acc: 0.1449 - val_loss: 2.2882 - val_acc: 0.1416\n",
            "Epoch 111/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2873 - acc: 0.1448 - val_loss: 2.2880 - val_acc: 0.1427\n",
            "Epoch 112/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2871 - acc: 0.1449 - val_loss: 2.2878 - val_acc: 0.1423\n",
            "Epoch 113/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2870 - acc: 0.1451 - val_loss: 2.2877 - val_acc: 0.1414\n",
            "Epoch 114/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2868 - acc: 0.1449 - val_loss: 2.2875 - val_acc: 0.1413\n",
            "Epoch 115/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2866 - acc: 0.1451 - val_loss: 2.2873 - val_acc: 0.1412\n",
            "Epoch 116/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2864 - acc: 0.1454 - val_loss: 2.2871 - val_acc: 0.1419\n",
            "Epoch 117/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2862 - acc: 0.1448 - val_loss: 2.2869 - val_acc: 0.1418\n",
            "Epoch 118/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2860 - acc: 0.1454 - val_loss: 2.2867 - val_acc: 0.1434\n",
            "Epoch 119/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2858 - acc: 0.1455 - val_loss: 2.2865 - val_acc: 0.1431\n",
            "Epoch 120/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2857 - acc: 0.1455 - val_loss: 2.2863 - val_acc: 0.1432\n",
            "Epoch 121/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2855 - acc: 0.1451 - val_loss: 2.2861 - val_acc: 0.1442\n",
            "Epoch 122/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2852 - acc: 0.1456 - val_loss: 2.2859 - val_acc: 0.1444\n",
            "Epoch 123/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2850 - acc: 0.1456 - val_loss: 2.2856 - val_acc: 0.1449\n",
            "Epoch 124/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2848 - acc: 0.1458 - val_loss: 2.2854 - val_acc: 0.1454\n",
            "Epoch 125/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2846 - acc: 0.1460 - val_loss: 2.2852 - val_acc: 0.1448\n",
            "Epoch 126/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2844 - acc: 0.1467 - val_loss: 2.2850 - val_acc: 0.1443\n",
            "Epoch 127/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2842 - acc: 0.1457 - val_loss: 2.2848 - val_acc: 0.1444\n",
            "Epoch 128/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2840 - acc: 0.1465 - val_loss: 2.2846 - val_acc: 0.1453\n",
            "Epoch 129/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2838 - acc: 0.1461 - val_loss: 2.2844 - val_acc: 0.1451\n",
            "Epoch 130/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2836 - acc: 0.1467 - val_loss: 2.2842 - val_acc: 0.1459\n",
            "Epoch 131/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2835 - acc: 0.1468 - val_loss: 2.2840 - val_acc: 0.1457\n",
            "Epoch 132/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2833 - acc: 0.1469 - val_loss: 2.2839 - val_acc: 0.1461\n",
            "Epoch 133/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2831 - acc: 0.1471 - val_loss: 2.2837 - val_acc: 0.1462\n",
            "Epoch 134/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2829 - acc: 0.1472 - val_loss: 2.2835 - val_acc: 0.1469\n",
            "Epoch 135/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2828 - acc: 0.1471 - val_loss: 2.2834 - val_acc: 0.1469\n",
            "Epoch 136/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2826 - acc: 0.1476 - val_loss: 2.2832 - val_acc: 0.1464\n",
            "Epoch 137/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2824 - acc: 0.1481 - val_loss: 2.2830 - val_acc: 0.1470\n",
            "Epoch 138/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2822 - acc: 0.1483 - val_loss: 2.2829 - val_acc: 0.1474\n",
            "Epoch 139/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2821 - acc: 0.1482 - val_loss: 2.2827 - val_acc: 0.1474\n",
            "Epoch 140/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2819 - acc: 0.1484 - val_loss: 2.2825 - val_acc: 0.1480\n",
            "Epoch 141/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2817 - acc: 0.1488 - val_loss: 2.2824 - val_acc: 0.1488\n",
            "Epoch 142/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2816 - acc: 0.1486 - val_loss: 2.2822 - val_acc: 0.1485\n",
            "Epoch 143/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2814 - acc: 0.1490 - val_loss: 2.2820 - val_acc: 0.1490\n",
            "Epoch 144/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2812 - acc: 0.1490 - val_loss: 2.2819 - val_acc: 0.1479\n",
            "Epoch 145/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2810 - acc: 0.1490 - val_loss: 2.2817 - val_acc: 0.1486\n",
            "Epoch 146/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2809 - acc: 0.1494 - val_loss: 2.2815 - val_acc: 0.1496\n",
            "Epoch 147/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2807 - acc: 0.1500 - val_loss: 2.2814 - val_acc: 0.1489\n",
            "Epoch 148/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2805 - acc: 0.1498 - val_loss: 2.2812 - val_acc: 0.1493\n",
            "Epoch 149/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2803 - acc: 0.1500 - val_loss: 2.2811 - val_acc: 0.1486\n",
            "Epoch 150/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2802 - acc: 0.1502 - val_loss: 2.2809 - val_acc: 0.1494\n",
            "Epoch 151/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2800 - acc: 0.1504 - val_loss: 2.2807 - val_acc: 0.1497\n",
            "Epoch 152/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2798 - acc: 0.1500 - val_loss: 2.2805 - val_acc: 0.1499\n",
            "Epoch 153/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2796 - acc: 0.1501 - val_loss: 2.2803 - val_acc: 0.1501\n",
            "Epoch 154/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2795 - acc: 0.1507 - val_loss: 2.2802 - val_acc: 0.1496\n",
            "Epoch 155/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2793 - acc: 0.1503 - val_loss: 2.2800 - val_acc: 0.1498\n",
            "Epoch 156/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2791 - acc: 0.1504 - val_loss: 2.2799 - val_acc: 0.1498\n",
            "Epoch 157/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2789 - acc: 0.1510 - val_loss: 2.2797 - val_acc: 0.1500\n",
            "Epoch 158/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2787 - acc: 0.1506 - val_loss: 2.2795 - val_acc: 0.1499\n",
            "Epoch 159/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2786 - acc: 0.1509 - val_loss: 2.2793 - val_acc: 0.1500\n",
            "Epoch 160/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2784 - acc: 0.1518 - val_loss: 2.2791 - val_acc: 0.1508\n",
            "Epoch 161/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2782 - acc: 0.1515 - val_loss: 2.2789 - val_acc: 0.1508\n",
            "Epoch 162/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2780 - acc: 0.1515 - val_loss: 2.2788 - val_acc: 0.1504\n",
            "Epoch 163/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2778 - acc: 0.1520 - val_loss: 2.2786 - val_acc: 0.1510\n",
            "Epoch 164/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2776 - acc: 0.1526 - val_loss: 2.2785 - val_acc: 0.1501\n",
            "Epoch 165/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2775 - acc: 0.1523 - val_loss: 2.2783 - val_acc: 0.1504\n",
            "Epoch 166/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2773 - acc: 0.1528 - val_loss: 2.2781 - val_acc: 0.1513\n",
            "Epoch 167/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2771 - acc: 0.1532 - val_loss: 2.2779 - val_acc: 0.1515\n",
            "Epoch 168/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2769 - acc: 0.1540 - val_loss: 2.2777 - val_acc: 0.1515\n",
            "Epoch 169/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2767 - acc: 0.1533 - val_loss: 2.2775 - val_acc: 0.1514\n",
            "Epoch 170/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2765 - acc: 0.1538 - val_loss: 2.2774 - val_acc: 0.1512\n",
            "Epoch 171/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2763 - acc: 0.1537 - val_loss: 2.2772 - val_acc: 0.1521\n",
            "Epoch 172/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2761 - acc: 0.1537 - val_loss: 2.2770 - val_acc: 0.1520\n",
            "Epoch 173/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2760 - acc: 0.1541 - val_loss: 2.2768 - val_acc: 0.1523\n",
            "Epoch 174/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2758 - acc: 0.1548 - val_loss: 2.2766 - val_acc: 0.1524\n",
            "Epoch 175/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2756 - acc: 0.1543 - val_loss: 2.2764 - val_acc: 0.1526\n",
            "Epoch 176/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2754 - acc: 0.1545 - val_loss: 2.2762 - val_acc: 0.1526\n",
            "Epoch 177/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2752 - acc: 0.1545 - val_loss: 2.2760 - val_acc: 0.1522\n",
            "Epoch 178/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2750 - acc: 0.1549 - val_loss: 2.2759 - val_acc: 0.1521\n",
            "Epoch 179/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2748 - acc: 0.1552 - val_loss: 2.2757 - val_acc: 0.1522\n",
            "Epoch 180/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2746 - acc: 0.1555 - val_loss: 2.2755 - val_acc: 0.1524\n",
            "Epoch 181/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2744 - acc: 0.1562 - val_loss: 2.2753 - val_acc: 0.1530\n",
            "Epoch 182/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2742 - acc: 0.1559 - val_loss: 2.2751 - val_acc: 0.1528\n",
            "Epoch 183/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2740 - acc: 0.1561 - val_loss: 2.2749 - val_acc: 0.1525\n",
            "Epoch 184/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2738 - acc: 0.1560 - val_loss: 2.2747 - val_acc: 0.1532\n",
            "Epoch 185/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2736 - acc: 0.1569 - val_loss: 2.2745 - val_acc: 0.1526\n",
            "Epoch 186/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2734 - acc: 0.1572 - val_loss: 2.2743 - val_acc: 0.1527\n",
            "Epoch 187/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2732 - acc: 0.1571 - val_loss: 2.2741 - val_acc: 0.1538\n",
            "Epoch 188/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2730 - acc: 0.1584 - val_loss: 2.2739 - val_acc: 0.1526\n",
            "Epoch 189/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2728 - acc: 0.1585 - val_loss: 2.2738 - val_acc: 0.1520\n",
            "Epoch 190/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2726 - acc: 0.1590 - val_loss: 2.2736 - val_acc: 0.1521\n",
            "Epoch 191/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2724 - acc: 0.1582 - val_loss: 2.2733 - val_acc: 0.1524\n",
            "Epoch 192/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2722 - acc: 0.1587 - val_loss: 2.2731 - val_acc: 0.1539\n",
            "Epoch 193/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2719 - acc: 0.1592 - val_loss: 2.2729 - val_acc: 0.1547\n",
            "Epoch 194/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2717 - acc: 0.1595 - val_loss: 2.2727 - val_acc: 0.1532\n",
            "Epoch 195/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2715 - acc: 0.1591 - val_loss: 2.2725 - val_acc: 0.1538\n",
            "Epoch 196/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2713 - acc: 0.1594 - val_loss: 2.2723 - val_acc: 0.1546\n",
            "Epoch 197/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2711 - acc: 0.1594 - val_loss: 2.2721 - val_acc: 0.1551\n",
            "Epoch 198/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2709 - acc: 0.1601 - val_loss: 2.2719 - val_acc: 0.1552\n",
            "Epoch 199/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2707 - acc: 0.1596 - val_loss: 2.2717 - val_acc: 0.1550\n",
            "Epoch 200/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2704 - acc: 0.1597 - val_loss: 2.2715 - val_acc: 0.1555\n",
            "Epoch 201/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2702 - acc: 0.1604 - val_loss: 2.2713 - val_acc: 0.1559\n",
            "Epoch 202/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2700 - acc: 0.1605 - val_loss: 2.2711 - val_acc: 0.1567\n",
            "Epoch 203/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2698 - acc: 0.1607 - val_loss: 2.2708 - val_acc: 0.1576\n",
            "Epoch 204/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2696 - acc: 0.1602 - val_loss: 2.2706 - val_acc: 0.1571\n",
            "Epoch 205/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2694 - acc: 0.1606 - val_loss: 2.2704 - val_acc: 0.1567\n",
            "Epoch 206/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2691 - acc: 0.1618 - val_loss: 2.2702 - val_acc: 0.1578\n",
            "Epoch 207/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2689 - acc: 0.1615 - val_loss: 2.2700 - val_acc: 0.1569\n",
            "Epoch 208/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2687 - acc: 0.1617 - val_loss: 2.2698 - val_acc: 0.1578\n",
            "Epoch 209/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2685 - acc: 0.1615 - val_loss: 2.2695 - val_acc: 0.1587\n",
            "Epoch 210/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2682 - acc: 0.1622 - val_loss: 2.2693 - val_acc: 0.1592\n",
            "Epoch 211/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2680 - acc: 0.1629 - val_loss: 2.2691 - val_acc: 0.1582\n",
            "Epoch 212/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2678 - acc: 0.1620 - val_loss: 2.2689 - val_acc: 0.1597\n",
            "Epoch 213/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2676 - acc: 0.1620 - val_loss: 2.2686 - val_acc: 0.1601\n",
            "Epoch 214/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2673 - acc: 0.1628 - val_loss: 2.2684 - val_acc: 0.1599\n",
            "Epoch 215/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2671 - acc: 0.1636 - val_loss: 2.2682 - val_acc: 0.1611\n",
            "Epoch 216/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2668 - acc: 0.1631 - val_loss: 2.2679 - val_acc: 0.1611\n",
            "Epoch 217/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2666 - acc: 0.1628 - val_loss: 2.2677 - val_acc: 0.1611\n",
            "Epoch 218/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2664 - acc: 0.1640 - val_loss: 2.2675 - val_acc: 0.1614\n",
            "Epoch 219/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2661 - acc: 0.1638 - val_loss: 2.2672 - val_acc: 0.1617\n",
            "Epoch 220/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2659 - acc: 0.1642 - val_loss: 2.2670 - val_acc: 0.1618\n",
            "Epoch 221/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2657 - acc: 0.1637 - val_loss: 2.2668 - val_acc: 0.1620\n",
            "Epoch 222/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2654 - acc: 0.1643 - val_loss: 2.2666 - val_acc: 0.1623\n",
            "Epoch 223/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2652 - acc: 0.1647 - val_loss: 2.2663 - val_acc: 0.1628\n",
            "Epoch 224/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2649 - acc: 0.1646 - val_loss: 2.2661 - val_acc: 0.1635\n",
            "Epoch 225/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2647 - acc: 0.1650 - val_loss: 2.2658 - val_acc: 0.1630\n",
            "Epoch 226/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2644 - acc: 0.1653 - val_loss: 2.2656 - val_acc: 0.1626\n",
            "Epoch 227/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2642 - acc: 0.1657 - val_loss: 2.2654 - val_acc: 0.1645\n",
            "Epoch 228/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2640 - acc: 0.1659 - val_loss: 2.2651 - val_acc: 0.1638\n",
            "Epoch 229/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2637 - acc: 0.1654 - val_loss: 2.2649 - val_acc: 0.1646\n",
            "Epoch 230/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2634 - acc: 0.1662 - val_loss: 2.2647 - val_acc: 0.1651\n",
            "Epoch 231/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2632 - acc: 0.1664 - val_loss: 2.2644 - val_acc: 0.1648\n",
            "Epoch 232/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2630 - acc: 0.1670 - val_loss: 2.2642 - val_acc: 0.1658\n",
            "Epoch 233/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2627 - acc: 0.1668 - val_loss: 2.2639 - val_acc: 0.1659\n",
            "Epoch 234/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2625 - acc: 0.1674 - val_loss: 2.2636 - val_acc: 0.1664\n",
            "Epoch 235/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2622 - acc: 0.1675 - val_loss: 2.2634 - val_acc: 0.1664\n",
            "Epoch 236/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2619 - acc: 0.1676 - val_loss: 2.2632 - val_acc: 0.1648\n",
            "Epoch 237/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2617 - acc: 0.1679 - val_loss: 2.2629 - val_acc: 0.1653\n",
            "Epoch 238/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2614 - acc: 0.1678 - val_loss: 2.2626 - val_acc: 0.1668\n",
            "Epoch 239/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2612 - acc: 0.1687 - val_loss: 2.2624 - val_acc: 0.1672\n",
            "Epoch 240/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2609 - acc: 0.1684 - val_loss: 2.2621 - val_acc: 0.1673\n",
            "Epoch 241/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2607 - acc: 0.1688 - val_loss: 2.2619 - val_acc: 0.1677\n",
            "Epoch 242/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2604 - acc: 0.1690 - val_loss: 2.2616 - val_acc: 0.1670\n",
            "Epoch 243/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2602 - acc: 0.1689 - val_loss: 2.2614 - val_acc: 0.1674\n",
            "Epoch 244/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2599 - acc: 0.1694 - val_loss: 2.2611 - val_acc: 0.1681\n",
            "Epoch 245/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2597 - acc: 0.1699 - val_loss: 2.2609 - val_acc: 0.1661\n",
            "Epoch 246/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2594 - acc: 0.1699 - val_loss: 2.2606 - val_acc: 0.1681\n",
            "Epoch 247/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2591 - acc: 0.1706 - val_loss: 2.2603 - val_acc: 0.1679\n",
            "Epoch 248/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2589 - acc: 0.1707 - val_loss: 2.2601 - val_acc: 0.1675\n",
            "Epoch 249/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2586 - acc: 0.1708 - val_loss: 2.2598 - val_acc: 0.1676\n",
            "Epoch 250/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2583 - acc: 0.1712 - val_loss: 2.2595 - val_acc: 0.1693\n",
            "Epoch 251/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2581 - acc: 0.1715 - val_loss: 2.2593 - val_acc: 0.1694\n",
            "Epoch 252/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2578 - acc: 0.1720 - val_loss: 2.2591 - val_acc: 0.1677\n",
            "Epoch 253/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2575 - acc: 0.1717 - val_loss: 2.2588 - val_acc: 0.1675\n",
            "Epoch 254/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2573 - acc: 0.1720 - val_loss: 2.2585 - val_acc: 0.1693\n",
            "Epoch 255/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2570 - acc: 0.1734 - val_loss: 2.2582 - val_acc: 0.1702\n",
            "Epoch 256/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2567 - acc: 0.1730 - val_loss: 2.2579 - val_acc: 0.1703\n",
            "Epoch 257/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2565 - acc: 0.1735 - val_loss: 2.2577 - val_acc: 0.1708\n",
            "Epoch 258/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2562 - acc: 0.1742 - val_loss: 2.2574 - val_acc: 0.1714\n",
            "Epoch 259/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2559 - acc: 0.1749 - val_loss: 2.2571 - val_acc: 0.1703\n",
            "Epoch 260/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2556 - acc: 0.1746 - val_loss: 2.2569 - val_acc: 0.1711\n",
            "Epoch 261/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2554 - acc: 0.1745 - val_loss: 2.2566 - val_acc: 0.1703\n",
            "Epoch 262/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2551 - acc: 0.1745 - val_loss: 2.2563 - val_acc: 0.1732\n",
            "Epoch 263/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2548 - acc: 0.1767 - val_loss: 2.2560 - val_acc: 0.1712\n",
            "Epoch 264/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2545 - acc: 0.1757 - val_loss: 2.2558 - val_acc: 0.1719\n",
            "Epoch 265/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2542 - acc: 0.1763 - val_loss: 2.2555 - val_acc: 0.1708\n",
            "Epoch 266/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2540 - acc: 0.1765 - val_loss: 2.2552 - val_acc: 0.1716\n",
            "Epoch 267/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2537 - acc: 0.1763 - val_loss: 2.2549 - val_acc: 0.1727\n",
            "Epoch 268/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2534 - acc: 0.1766 - val_loss: 2.2546 - val_acc: 0.1736\n",
            "Epoch 269/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2531 - acc: 0.1770 - val_loss: 2.2543 - val_acc: 0.1751\n",
            "Epoch 270/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2528 - acc: 0.1783 - val_loss: 2.2541 - val_acc: 0.1732\n",
            "Epoch 271/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2525 - acc: 0.1775 - val_loss: 2.2538 - val_acc: 0.1752\n",
            "Epoch 272/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2523 - acc: 0.1783 - val_loss: 2.2535 - val_acc: 0.1746\n",
            "Epoch 273/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2520 - acc: 0.1787 - val_loss: 2.2532 - val_acc: 0.1740\n",
            "Epoch 274/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2517 - acc: 0.1800 - val_loss: 2.2530 - val_acc: 0.1722\n",
            "Epoch 275/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2514 - acc: 0.1789 - val_loss: 2.2526 - val_acc: 0.1742\n",
            "Epoch 276/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2511 - acc: 0.1803 - val_loss: 2.2523 - val_acc: 0.1751\n",
            "Epoch 277/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2507 - acc: 0.1799 - val_loss: 2.2520 - val_acc: 0.1744\n",
            "Epoch 278/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2504 - acc: 0.1799 - val_loss: 2.2517 - val_acc: 0.1762\n",
            "Epoch 279/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2501 - acc: 0.1805 - val_loss: 2.2514 - val_acc: 0.1778\n",
            "Epoch 280/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2498 - acc: 0.1814 - val_loss: 2.2511 - val_acc: 0.1776\n",
            "Epoch 281/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2495 - acc: 0.1813 - val_loss: 2.2508 - val_acc: 0.1771\n",
            "Epoch 282/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2492 - acc: 0.1809 - val_loss: 2.2505 - val_acc: 0.1767\n",
            "Epoch 283/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2489 - acc: 0.1821 - val_loss: 2.2502 - val_acc: 0.1768\n",
            "Epoch 284/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2486 - acc: 0.1817 - val_loss: 2.2499 - val_acc: 0.1781\n",
            "Epoch 285/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2483 - acc: 0.1825 - val_loss: 2.2495 - val_acc: 0.1796\n",
            "Epoch 286/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2479 - acc: 0.1831 - val_loss: 2.2493 - val_acc: 0.1788\n",
            "Epoch 287/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2476 - acc: 0.1829 - val_loss: 2.2489 - val_acc: 0.1806\n",
            "Epoch 288/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2473 - acc: 0.1844 - val_loss: 2.2487 - val_acc: 0.1787\n",
            "Epoch 289/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2470 - acc: 0.1839 - val_loss: 2.2483 - val_acc: 0.1806\n",
            "Epoch 290/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2467 - acc: 0.1848 - val_loss: 2.2481 - val_acc: 0.1795\n",
            "Epoch 291/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2464 - acc: 0.1842 - val_loss: 2.2477 - val_acc: 0.1802\n",
            "Epoch 292/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2461 - acc: 0.1840 - val_loss: 2.2474 - val_acc: 0.1828\n",
            "Epoch 293/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2458 - acc: 0.1861 - val_loss: 2.2471 - val_acc: 0.1815\n",
            "Epoch 294/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2455 - acc: 0.1860 - val_loss: 2.2468 - val_acc: 0.1825\n",
            "Epoch 295/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2452 - acc: 0.1868 - val_loss: 2.2465 - val_acc: 0.1828\n",
            "Epoch 296/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2449 - acc: 0.1865 - val_loss: 2.2462 - val_acc: 0.1842\n",
            "Epoch 297/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2445 - acc: 0.1875 - val_loss: 2.2459 - val_acc: 0.1839\n",
            "Epoch 298/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2442 - acc: 0.1881 - val_loss: 2.2456 - val_acc: 0.1831\n",
            "Epoch 299/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2439 - acc: 0.1881 - val_loss: 2.2453 - val_acc: 0.1851\n",
            "Epoch 300/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2436 - acc: 0.1887 - val_loss: 2.2450 - val_acc: 0.1848\n",
            "Epoch 301/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2433 - acc: 0.1887 - val_loss: 2.2446 - val_acc: 0.1869\n",
            "Epoch 302/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2430 - acc: 0.1896 - val_loss: 2.2443 - val_acc: 0.1874\n",
            "Epoch 303/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2427 - acc: 0.1905 - val_loss: 2.2440 - val_acc: 0.1871\n",
            "Epoch 304/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2424 - acc: 0.1896 - val_loss: 2.2437 - val_acc: 0.1876\n",
            "Epoch 305/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2420 - acc: 0.1895 - val_loss: 2.2433 - val_acc: 0.1887\n",
            "Epoch 306/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2417 - acc: 0.1907 - val_loss: 2.2430 - val_acc: 0.1894\n",
            "Epoch 307/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2414 - acc: 0.1912 - val_loss: 2.2427 - val_acc: 0.1891\n",
            "Epoch 308/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2411 - acc: 0.1919 - val_loss: 2.2424 - val_acc: 0.1893\n",
            "Epoch 309/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2407 - acc: 0.1921 - val_loss: 2.2421 - val_acc: 0.1894\n",
            "Epoch 310/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2404 - acc: 0.1913 - val_loss: 2.2418 - val_acc: 0.1903\n",
            "Epoch 311/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2401 - acc: 0.1923 - val_loss: 2.2414 - val_acc: 0.1899\n",
            "Epoch 312/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2397 - acc: 0.1929 - val_loss: 2.2411 - val_acc: 0.1894\n",
            "Epoch 313/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2394 - acc: 0.1924 - val_loss: 2.2408 - val_acc: 0.1910\n",
            "Epoch 314/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2391 - acc: 0.1935 - val_loss: 2.2404 - val_acc: 0.1913\n",
            "Epoch 315/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2388 - acc: 0.1935 - val_loss: 2.2401 - val_acc: 0.1925\n",
            "Epoch 316/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2384 - acc: 0.1941 - val_loss: 2.2397 - val_acc: 0.1938\n",
            "Epoch 317/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2381 - acc: 0.1949 - val_loss: 2.2394 - val_acc: 0.1930\n",
            "Epoch 318/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2378 - acc: 0.1948 - val_loss: 2.2391 - val_acc: 0.1933\n",
            "Epoch 319/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2374 - acc: 0.1954 - val_loss: 2.2387 - val_acc: 0.1940\n",
            "Epoch 320/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2371 - acc: 0.1965 - val_loss: 2.2384 - val_acc: 0.1943\n",
            "Epoch 321/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2367 - acc: 0.1960 - val_loss: 2.2381 - val_acc: 0.1947\n",
            "Epoch 322/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2364 - acc: 0.1961 - val_loss: 2.2378 - val_acc: 0.1956\n",
            "Epoch 323/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2361 - acc: 0.1964 - val_loss: 2.2374 - val_acc: 0.1957\n",
            "Epoch 324/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2357 - acc: 0.1976 - val_loss: 2.2371 - val_acc: 0.1951\n",
            "Epoch 325/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2354 - acc: 0.1968 - val_loss: 2.2367 - val_acc: 0.1966\n",
            "Epoch 326/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2350 - acc: 0.1981 - val_loss: 2.2364 - val_acc: 0.1978\n",
            "Epoch 327/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2347 - acc: 0.1989 - val_loss: 2.2361 - val_acc: 0.1974\n",
            "Epoch 328/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2343 - acc: 0.1986 - val_loss: 2.2357 - val_acc: 0.1989\n",
            "Epoch 329/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2340 - acc: 0.2005 - val_loss: 2.2354 - val_acc: 0.1993\n",
            "Epoch 330/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2336 - acc: 0.2008 - val_loss: 2.2351 - val_acc: 0.1983\n",
            "Epoch 331/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2333 - acc: 0.2010 - val_loss: 2.2347 - val_acc: 0.1989\n",
            "Epoch 332/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2329 - acc: 0.2004 - val_loss: 2.2343 - val_acc: 0.2002\n",
            "Epoch 333/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2326 - acc: 0.2014 - val_loss: 2.2339 - val_acc: 0.2022\n",
            "Epoch 334/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2322 - acc: 0.2026 - val_loss: 2.2336 - val_acc: 0.1991\n",
            "Epoch 335/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2319 - acc: 0.2019 - val_loss: 2.2333 - val_acc: 0.1996\n",
            "Epoch 336/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2315 - acc: 0.2016 - val_loss: 2.2329 - val_acc: 0.1993\n",
            "Epoch 337/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2312 - acc: 0.2026 - val_loss: 2.2326 - val_acc: 0.2001\n",
            "Epoch 338/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2308 - acc: 0.2027 - val_loss: 2.2322 - val_acc: 0.2008\n",
            "Epoch 339/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2304 - acc: 0.2025 - val_loss: 2.2318 - val_acc: 0.2014\n",
            "Epoch 340/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2301 - acc: 0.2033 - val_loss: 2.2315 - val_acc: 0.2020\n",
            "Epoch 341/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2297 - acc: 0.2041 - val_loss: 2.2312 - val_acc: 0.2013\n",
            "Epoch 342/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2294 - acc: 0.2038 - val_loss: 2.2308 - val_acc: 0.2026\n",
            "Epoch 343/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2290 - acc: 0.2050 - val_loss: 2.2304 - val_acc: 0.2019\n",
            "Epoch 344/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2286 - acc: 0.2047 - val_loss: 2.2301 - val_acc: 0.2018\n",
            "Epoch 345/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2283 - acc: 0.2043 - val_loss: 2.2297 - val_acc: 0.2034\n",
            "Epoch 346/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2279 - acc: 0.2059 - val_loss: 2.2293 - val_acc: 0.2038\n",
            "Epoch 347/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2275 - acc: 0.2069 - val_loss: 2.2289 - val_acc: 0.2038\n",
            "Epoch 348/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2272 - acc: 0.2057 - val_loss: 2.2285 - val_acc: 0.2056\n",
            "Epoch 349/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2268 - acc: 0.2059 - val_loss: 2.2282 - val_acc: 0.2053\n",
            "Epoch 350/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2264 - acc: 0.2081 - val_loss: 2.2278 - val_acc: 0.2042\n",
            "Epoch 351/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2260 - acc: 0.2074 - val_loss: 2.2274 - val_acc: 0.2054\n",
            "Epoch 352/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2256 - acc: 0.2086 - val_loss: 2.2271 - val_acc: 0.2062\n",
            "Epoch 353/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2253 - acc: 0.2075 - val_loss: 2.2267 - val_acc: 0.2069\n",
            "Epoch 354/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2249 - acc: 0.2093 - val_loss: 2.2262 - val_acc: 0.2076\n",
            "Epoch 355/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2245 - acc: 0.2082 - val_loss: 2.2259 - val_acc: 0.2075\n",
            "Epoch 356/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2241 - acc: 0.2101 - val_loss: 2.2255 - val_acc: 0.2068\n",
            "Epoch 357/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2238 - acc: 0.2110 - val_loss: 2.2251 - val_acc: 0.2066\n",
            "Epoch 358/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2234 - acc: 0.2099 - val_loss: 2.2247 - val_acc: 0.2072\n",
            "Epoch 359/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2229 - acc: 0.2101 - val_loss: 2.2243 - val_acc: 0.2076\n",
            "Epoch 360/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2225 - acc: 0.2117 - val_loss: 2.2239 - val_acc: 0.2083\n",
            "Epoch 361/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2221 - acc: 0.2119 - val_loss: 2.2234 - val_acc: 0.2091\n",
            "Epoch 362/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2217 - acc: 0.2122 - val_loss: 2.2230 - val_acc: 0.2090\n",
            "Epoch 363/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2212 - acc: 0.2123 - val_loss: 2.2225 - val_acc: 0.2090\n",
            "Epoch 364/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2208 - acc: 0.2121 - val_loss: 2.2220 - val_acc: 0.2103\n",
            "Epoch 365/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2203 - acc: 0.2123 - val_loss: 2.2215 - val_acc: 0.2107\n",
            "Epoch 366/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2198 - acc: 0.2144 - val_loss: 2.2210 - val_acc: 0.2093\n",
            "Epoch 367/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2192 - acc: 0.2132 - val_loss: 2.2203 - val_acc: 0.2112\n",
            "Epoch 368/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2187 - acc: 0.2146 - val_loss: 2.2196 - val_acc: 0.2108\n",
            "Epoch 369/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2180 - acc: 0.2151 - val_loss: 2.2190 - val_acc: 0.2115\n",
            "Epoch 370/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2175 - acc: 0.2157 - val_loss: 2.2184 - val_acc: 0.2131\n",
            "Epoch 371/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2169 - acc: 0.2154 - val_loss: 2.2178 - val_acc: 0.2133\n",
            "Epoch 372/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2163 - acc: 0.2157 - val_loss: 2.2171 - val_acc: 0.2149\n",
            "Epoch 373/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2158 - acc: 0.2174 - val_loss: 2.2166 - val_acc: 0.2151\n",
            "Epoch 374/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2152 - acc: 0.2165 - val_loss: 2.2160 - val_acc: 0.2168\n",
            "Epoch 375/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2147 - acc: 0.2177 - val_loss: 2.2154 - val_acc: 0.2174\n",
            "Epoch 376/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2141 - acc: 0.2198 - val_loss: 2.2150 - val_acc: 0.2139\n",
            "Epoch 377/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2135 - acc: 0.2181 - val_loss: 2.2143 - val_acc: 0.2149\n",
            "Epoch 378/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2129 - acc: 0.2191 - val_loss: 2.2137 - val_acc: 0.2163\n",
            "Epoch 379/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2123 - acc: 0.2213 - val_loss: 2.2132 - val_acc: 0.2148\n",
            "Epoch 380/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2118 - acc: 0.2200 - val_loss: 2.2126 - val_acc: 0.2181\n",
            "Epoch 381/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2113 - acc: 0.2217 - val_loss: 2.2121 - val_acc: 0.2178\n",
            "Epoch 382/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2107 - acc: 0.2218 - val_loss: 2.2115 - val_acc: 0.2198\n",
            "Epoch 383/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2102 - acc: 0.2226 - val_loss: 2.2110 - val_acc: 0.2201\n",
            "Epoch 384/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2097 - acc: 0.2232 - val_loss: 2.2105 - val_acc: 0.2216\n",
            "Epoch 385/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2093 - acc: 0.2244 - val_loss: 2.2100 - val_acc: 0.2206\n",
            "Epoch 386/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2088 - acc: 0.2243 - val_loss: 2.2096 - val_acc: 0.2197\n",
            "Epoch 387/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2083 - acc: 0.2242 - val_loss: 2.2091 - val_acc: 0.2191\n",
            "Epoch 388/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2078 - acc: 0.2229 - val_loss: 2.2086 - val_acc: 0.2214\n",
            "Epoch 389/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.2073 - acc: 0.2242 - val_loss: 2.2082 - val_acc: 0.2223\n",
            "Epoch 390/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2068 - acc: 0.2242 - val_loss: 2.2077 - val_acc: 0.2234\n",
            "Epoch 391/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2064 - acc: 0.2256 - val_loss: 2.2072 - val_acc: 0.2224\n",
            "Epoch 392/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2059 - acc: 0.2256 - val_loss: 2.2067 - val_acc: 0.2236\n",
            "Epoch 393/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2054 - acc: 0.2257 - val_loss: 2.2062 - val_acc: 0.2237\n",
            "Epoch 394/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2049 - acc: 0.2260 - val_loss: 2.2057 - val_acc: 0.2244\n",
            "Epoch 395/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2044 - acc: 0.2258 - val_loss: 2.2053 - val_acc: 0.2257\n",
            "Epoch 396/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2040 - acc: 0.2279 - val_loss: 2.2048 - val_acc: 0.2242\n",
            "Epoch 397/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2035 - acc: 0.2268 - val_loss: 2.2043 - val_acc: 0.2248\n",
            "Epoch 398/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2030 - acc: 0.2271 - val_loss: 2.2038 - val_acc: 0.2268\n",
            "Epoch 399/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2025 - acc: 0.2280 - val_loss: 2.2034 - val_acc: 0.2248\n",
            "Epoch 400/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2020 - acc: 0.2276 - val_loss: 2.2029 - val_acc: 0.2271\n",
            "Epoch 401/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2015 - acc: 0.2283 - val_loss: 2.2023 - val_acc: 0.2271\n",
            "Epoch 402/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2010 - acc: 0.2293 - val_loss: 2.2019 - val_acc: 0.2268\n",
            "Epoch 403/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2005 - acc: 0.2295 - val_loss: 2.2014 - val_acc: 0.2268\n",
            "Epoch 404/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.2000 - acc: 0.2294 - val_loss: 2.2009 - val_acc: 0.2266\n",
            "Epoch 405/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1996 - acc: 0.2293 - val_loss: 2.2004 - val_acc: 0.2272\n",
            "Epoch 406/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1991 - acc: 0.2291 - val_loss: 2.1999 - val_acc: 0.2289\n",
            "Epoch 407/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.1986 - acc: 0.2305 - val_loss: 2.1996 - val_acc: 0.2271\n",
            "Epoch 408/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1981 - acc: 0.2294 - val_loss: 2.1991 - val_acc: 0.2277\n",
            "Epoch 409/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1976 - acc: 0.2307 - val_loss: 2.1985 - val_acc: 0.2268\n",
            "Epoch 410/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1971 - acc: 0.2312 - val_loss: 2.1979 - val_acc: 0.2302\n",
            "Epoch 411/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1966 - acc: 0.2324 - val_loss: 2.1975 - val_acc: 0.2276\n",
            "Epoch 412/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.1961 - acc: 0.2307 - val_loss: 2.1969 - val_acc: 0.2313\n",
            "Epoch 413/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1956 - acc: 0.2324 - val_loss: 2.1964 - val_acc: 0.2305\n",
            "Epoch 414/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1951 - acc: 0.2324 - val_loss: 2.1960 - val_acc: 0.2299\n",
            "Epoch 415/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1946 - acc: 0.2322 - val_loss: 2.1954 - val_acc: 0.2329\n",
            "Epoch 416/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1941 - acc: 0.2334 - val_loss: 2.1949 - val_acc: 0.2318\n",
            "Epoch 417/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1936 - acc: 0.2330 - val_loss: 2.1944 - val_acc: 0.2328\n",
            "Epoch 418/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1931 - acc: 0.2342 - val_loss: 2.1939 - val_acc: 0.2332\n",
            "Epoch 419/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1926 - acc: 0.2345 - val_loss: 2.1935 - val_acc: 0.2311\n",
            "Epoch 420/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1921 - acc: 0.2340 - val_loss: 2.1930 - val_acc: 0.2330\n",
            "Epoch 421/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.1916 - acc: 0.2352 - val_loss: 2.1925 - val_acc: 0.2324\n",
            "Epoch 422/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.1911 - acc: 0.2343 - val_loss: 2.1920 - val_acc: 0.2321\n",
            "Epoch 423/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1906 - acc: 0.2348 - val_loss: 2.1915 - val_acc: 0.2329\n",
            "Epoch 424/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1901 - acc: 0.2348 - val_loss: 2.1909 - val_acc: 0.2338\n",
            "Epoch 425/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1895 - acc: 0.2358 - val_loss: 2.1905 - val_acc: 0.2336\n",
            "Epoch 426/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1890 - acc: 0.2368 - val_loss: 2.1900 - val_acc: 0.2342\n",
            "Epoch 427/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1886 - acc: 0.2360 - val_loss: 2.1894 - val_acc: 0.2351\n",
            "Epoch 428/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1881 - acc: 0.2373 - val_loss: 2.1889 - val_acc: 0.2353\n",
            "Epoch 429/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1875 - acc: 0.2372 - val_loss: 2.1885 - val_acc: 0.2334\n",
            "Epoch 430/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1870 - acc: 0.2363 - val_loss: 2.1878 - val_acc: 0.2372\n",
            "Epoch 431/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1865 - acc: 0.2365 - val_loss: 2.1873 - val_acc: 0.2386\n",
            "Epoch 432/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1860 - acc: 0.2379 - val_loss: 2.1868 - val_acc: 0.2383\n",
            "Epoch 433/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1855 - acc: 0.2393 - val_loss: 2.1864 - val_acc: 0.2348\n",
            "Epoch 434/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1850 - acc: 0.2387 - val_loss: 2.1858 - val_acc: 0.2367\n",
            "Epoch 435/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.1845 - acc: 0.2378 - val_loss: 2.1853 - val_acc: 0.2388\n",
            "Epoch 436/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1840 - acc: 0.2404 - val_loss: 2.1848 - val_acc: 0.2362\n",
            "Epoch 437/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1834 - acc: 0.2383 - val_loss: 2.1842 - val_acc: 0.2379\n",
            "Epoch 438/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1829 - acc: 0.2399 - val_loss: 2.1838 - val_acc: 0.2354\n",
            "Epoch 439/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1824 - acc: 0.2398 - val_loss: 2.1833 - val_acc: 0.2354\n",
            "Epoch 440/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1819 - acc: 0.2399 - val_loss: 2.1829 - val_acc: 0.2342\n",
            "Epoch 441/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1814 - acc: 0.2391 - val_loss: 2.1822 - val_acc: 0.2386\n",
            "Epoch 442/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1809 - acc: 0.2411 - val_loss: 2.1818 - val_acc: 0.2378\n",
            "Epoch 443/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1803 - acc: 0.2409 - val_loss: 2.1811 - val_acc: 0.2376\n",
            "Epoch 444/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1798 - acc: 0.2411 - val_loss: 2.1807 - val_acc: 0.2387\n",
            "Epoch 445/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1793 - acc: 0.2417 - val_loss: 2.1802 - val_acc: 0.2385\n",
            "Epoch 446/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.1788 - acc: 0.2415 - val_loss: 2.1797 - val_acc: 0.2392\n",
            "Epoch 447/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1783 - acc: 0.2415 - val_loss: 2.1791 - val_acc: 0.2406\n",
            "Epoch 448/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1777 - acc: 0.2428 - val_loss: 2.1785 - val_acc: 0.2393\n",
            "Epoch 449/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1772 - acc: 0.2432 - val_loss: 2.1780 - val_acc: 0.2406\n",
            "Epoch 450/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1767 - acc: 0.2427 - val_loss: 2.1774 - val_acc: 0.2423\n",
            "Epoch 451/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.1762 - acc: 0.2435 - val_loss: 2.1770 - val_acc: 0.2421\n",
            "Epoch 452/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1757 - acc: 0.2430 - val_loss: 2.1764 - val_acc: 0.2411\n",
            "Epoch 453/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1751 - acc: 0.2437 - val_loss: 2.1759 - val_acc: 0.2432\n",
            "Epoch 454/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1746 - acc: 0.2453 - val_loss: 2.1753 - val_acc: 0.2423\n",
            "Epoch 455/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1741 - acc: 0.2448 - val_loss: 2.1748 - val_acc: 0.2437\n",
            "Epoch 456/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1735 - acc: 0.2459 - val_loss: 2.1743 - val_acc: 0.2422\n",
            "Epoch 457/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1730 - acc: 0.2442 - val_loss: 2.1737 - val_acc: 0.2432\n",
            "Epoch 458/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1725 - acc: 0.2459 - val_loss: 2.1733 - val_acc: 0.2431\n",
            "Epoch 459/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1719 - acc: 0.2452 - val_loss: 2.1727 - val_acc: 0.2427\n",
            "Epoch 460/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1714 - acc: 0.2456 - val_loss: 2.1721 - val_acc: 0.2446\n",
            "Epoch 461/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1709 - acc: 0.2458 - val_loss: 2.1716 - val_acc: 0.2458\n",
            "Epoch 462/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1704 - acc: 0.2470 - val_loss: 2.1711 - val_acc: 0.2440\n",
            "Epoch 463/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.1699 - acc: 0.2468 - val_loss: 2.1705 - val_acc: 0.2458\n",
            "Epoch 464/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.1693 - acc: 0.2482 - val_loss: 2.1701 - val_acc: 0.2458\n",
            "Epoch 465/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1688 - acc: 0.2471 - val_loss: 2.1695 - val_acc: 0.2461\n",
            "Epoch 466/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1683 - acc: 0.2487 - val_loss: 2.1690 - val_acc: 0.2451\n",
            "Epoch 467/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1677 - acc: 0.2469 - val_loss: 2.1684 - val_acc: 0.2461\n",
            "Epoch 468/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1672 - acc: 0.2471 - val_loss: 2.1679 - val_acc: 0.2476\n",
            "Epoch 469/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1667 - acc: 0.2481 - val_loss: 2.1673 - val_acc: 0.2469\n",
            "Epoch 470/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1661 - acc: 0.2500 - val_loss: 2.1669 - val_acc: 0.2464\n",
            "Epoch 471/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1656 - acc: 0.2481 - val_loss: 2.1662 - val_acc: 0.2484\n",
            "Epoch 472/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1650 - acc: 0.2496 - val_loss: 2.1656 - val_acc: 0.2481\n",
            "Epoch 473/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1645 - acc: 0.2500 - val_loss: 2.1651 - val_acc: 0.2480\n",
            "Epoch 474/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1640 - acc: 0.2499 - val_loss: 2.1646 - val_acc: 0.2480\n",
            "Epoch 475/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1634 - acc: 0.2502 - val_loss: 2.1640 - val_acc: 0.2493\n",
            "Epoch 476/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1629 - acc: 0.2516 - val_loss: 2.1635 - val_acc: 0.2480\n",
            "Epoch 477/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1623 - acc: 0.2501 - val_loss: 2.1630 - val_acc: 0.2478\n",
            "Epoch 478/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1618 - acc: 0.2496 - val_loss: 2.1624 - val_acc: 0.2489\n",
            "Epoch 479/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1613 - acc: 0.2512 - val_loss: 2.1618 - val_acc: 0.2502\n",
            "Epoch 480/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1607 - acc: 0.2516 - val_loss: 2.1613 - val_acc: 0.2498\n",
            "Epoch 481/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1601 - acc: 0.2523 - val_loss: 2.1608 - val_acc: 0.2486\n",
            "Epoch 482/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1596 - acc: 0.2509 - val_loss: 2.1602 - val_acc: 0.2509\n",
            "Epoch 483/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1591 - acc: 0.2521 - val_loss: 2.1596 - val_acc: 0.2506\n",
            "Epoch 484/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1585 - acc: 0.2530 - val_loss: 2.1591 - val_acc: 0.2499\n",
            "Epoch 485/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1580 - acc: 0.2526 - val_loss: 2.1585 - val_acc: 0.2505\n",
            "Epoch 486/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1575 - acc: 0.2527 - val_loss: 2.1580 - val_acc: 0.2497\n",
            "Epoch 487/500\n",
            "33600/33600 [==============================] - 0s 6us/step - loss: 2.1569 - acc: 0.2521 - val_loss: 2.1574 - val_acc: 0.2501\n",
            "Epoch 488/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1563 - acc: 0.2532 - val_loss: 2.1569 - val_acc: 0.2506\n",
            "Epoch 489/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1558 - acc: 0.2539 - val_loss: 2.1564 - val_acc: 0.2508\n",
            "Epoch 490/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1553 - acc: 0.2527 - val_loss: 2.1557 - val_acc: 0.2519\n",
            "Epoch 491/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1547 - acc: 0.2545 - val_loss: 2.1552 - val_acc: 0.2502\n",
            "Epoch 492/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1542 - acc: 0.2544 - val_loss: 2.1546 - val_acc: 0.2518\n",
            "Epoch 493/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1536 - acc: 0.2552 - val_loss: 2.1540 - val_acc: 0.2511\n",
            "Epoch 494/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1530 - acc: 0.2548 - val_loss: 2.1535 - val_acc: 0.2520\n",
            "Epoch 495/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1525 - acc: 0.2544 - val_loss: 2.1529 - val_acc: 0.2522\n",
            "Epoch 496/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1519 - acc: 0.2548 - val_loss: 2.1523 - val_acc: 0.2539\n",
            "Epoch 497/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1514 - acc: 0.2562 - val_loss: 2.1517 - val_acc: 0.2538\n",
            "Epoch 498/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1508 - acc: 0.2563 - val_loss: 2.1512 - val_acc: 0.2532\n",
            "Epoch 499/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1503 - acc: 0.2560 - val_loss: 2.1506 - val_acc: 0.2539\n",
            "Epoch 500/500\n",
            "33600/33600 [==============================] - 0s 5us/step - loss: 2.1497 - acc: 0.2568 - val_loss: 2.1501 - val_acc: 0.2539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaTKlcrtsun7",
        "colab_type": "code",
        "outputId": "01224ce6-8129-497b-b36a-5a7356071684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "results=model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 0s 41us/step\n",
            "Test accuracy:  0.2779761904761905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR9iC1dtsZQP",
        "colab_type": "code",
        "outputId": "00feb4e9-30a6-443f-c5c9-641011132fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "NN3_val_loss = history.history['val_loss']\n",
        "NN3_train_loss = history.history['loss']\n",
        "epochs = range(1,501)\n",
        "\n",
        "plt.plot(epochs, NN3_val_loss, 'b+', label='Validation Loss')\n",
        "plt.plot(epochs, NN3_train_loss, 'bo', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qte-p-12s4sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import BatchNormalization, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMcMuVf_vA92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlp_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(50,input_shape=(1024,), kernel_initializer='he_normal'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(50,kernel_initializer='he_normal'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(50, kernel_initializer='he_normal'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(50, kernel_initializer='he_normal'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  adam=optimizers.adam(lr=0.01)\n",
        "  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXLR_srlvQPe",
        "colab_type": "code",
        "outputId": "ad11a0dd-a171-4e69-c6ca-f0ee3a0f94f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train, batch_size=2048,epochs = 500, verbose = 1,  validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/500\n",
            "33600/33600 [==============================] - 1s 37us/step - loss: 2.1605 - acc: 0.2221 - val_loss: 2.0355 - val_acc: 0.2943\n",
            "Epoch 2/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.5282 - acc: 0.4797 - val_loss: 2.0606 - val_acc: 0.3534\n",
            "Epoch 3/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.2009 - acc: 0.6093 - val_loss: 1.7046 - val_acc: 0.4659\n",
            "Epoch 4/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.0376 - acc: 0.6671 - val_loss: 1.5014 - val_acc: 0.5331\n",
            "Epoch 5/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9438 - acc: 0.7000 - val_loss: 1.6171 - val_acc: 0.5072\n",
            "Epoch 6/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8813 - acc: 0.7221 - val_loss: 1.5107 - val_acc: 0.5156\n",
            "Epoch 7/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8174 - acc: 0.7423 - val_loss: 1.4591 - val_acc: 0.5342\n",
            "Epoch 8/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7821 - acc: 0.7530 - val_loss: 1.5645 - val_acc: 0.5304\n",
            "Epoch 9/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7641 - acc: 0.7571 - val_loss: 1.3656 - val_acc: 0.5758\n",
            "Epoch 10/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7266 - acc: 0.7689 - val_loss: 1.3243 - val_acc: 0.5908\n",
            "Epoch 11/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7044 - acc: 0.7754 - val_loss: 1.4006 - val_acc: 0.5861\n",
            "Epoch 12/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.6689 - acc: 0.7863 - val_loss: 1.3636 - val_acc: 0.5922\n",
            "Epoch 13/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.6532 - acc: 0.7920 - val_loss: 1.2114 - val_acc: 0.6369\n",
            "Epoch 14/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6255 - acc: 0.8015 - val_loss: 1.5120 - val_acc: 0.5647\n",
            "Epoch 15/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.6242 - acc: 0.7997 - val_loss: 1.2154 - val_acc: 0.6316\n",
            "Epoch 16/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.6028 - acc: 0.8079 - val_loss: 1.4377 - val_acc: 0.6141\n",
            "Epoch 17/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.5860 - acc: 0.8125 - val_loss: 1.4863 - val_acc: 0.6042\n",
            "Epoch 18/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.5695 - acc: 0.8179 - val_loss: 1.4249 - val_acc: 0.6027\n",
            "Epoch 19/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.5762 - acc: 0.8151 - val_loss: 1.2978 - val_acc: 0.6437\n",
            "Epoch 20/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.5424 - acc: 0.8255 - val_loss: 1.3884 - val_acc: 0.6064\n",
            "Epoch 21/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.5366 - acc: 0.8287 - val_loss: 1.6217 - val_acc: 0.6043\n",
            "Epoch 22/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.5224 - acc: 0.8331 - val_loss: 1.0994 - val_acc: 0.6799\n",
            "Epoch 23/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.5079 - acc: 0.8354 - val_loss: 1.1557 - val_acc: 0.6704\n",
            "Epoch 24/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4986 - acc: 0.8413 - val_loss: 1.1882 - val_acc: 0.6704\n",
            "Epoch 25/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4834 - acc: 0.8452 - val_loss: 1.3219 - val_acc: 0.6562\n",
            "Epoch 26/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4657 - acc: 0.8496 - val_loss: 1.0427 - val_acc: 0.7091\n",
            "Epoch 27/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4696 - acc: 0.8494 - val_loss: 1.2148 - val_acc: 0.6701\n",
            "Epoch 28/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4630 - acc: 0.8520 - val_loss: 1.1236 - val_acc: 0.6742\n",
            "Epoch 29/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4474 - acc: 0.8560 - val_loss: 1.2908 - val_acc: 0.6645\n",
            "Epoch 30/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4409 - acc: 0.8588 - val_loss: 1.4449 - val_acc: 0.6580\n",
            "Epoch 31/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4429 - acc: 0.8565 - val_loss: 1.1158 - val_acc: 0.6921\n",
            "Epoch 32/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4296 - acc: 0.8622 - val_loss: 1.3796 - val_acc: 0.6478\n",
            "Epoch 33/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4174 - acc: 0.8671 - val_loss: 1.2460 - val_acc: 0.6914\n",
            "Epoch 34/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4132 - acc: 0.8664 - val_loss: 1.3961 - val_acc: 0.6468\n",
            "Epoch 35/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4222 - acc: 0.8646 - val_loss: 1.1917 - val_acc: 0.6874\n",
            "Epoch 36/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3954 - acc: 0.8713 - val_loss: 1.3149 - val_acc: 0.6821\n",
            "Epoch 37/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.3843 - acc: 0.8760 - val_loss: 1.2124 - val_acc: 0.6764\n",
            "Epoch 38/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3823 - acc: 0.8771 - val_loss: 1.1718 - val_acc: 0.7024\n",
            "Epoch 39/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3839 - acc: 0.8755 - val_loss: 1.3980 - val_acc: 0.6413\n",
            "Epoch 40/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3848 - acc: 0.8745 - val_loss: 0.9890 - val_acc: 0.7399\n",
            "Epoch 41/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3767 - acc: 0.8770 - val_loss: 1.0754 - val_acc: 0.7107\n",
            "Epoch 42/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3716 - acc: 0.8787 - val_loss: 1.0920 - val_acc: 0.7304\n",
            "Epoch 43/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3509 - acc: 0.8858 - val_loss: 1.0567 - val_acc: 0.7283\n",
            "Epoch 44/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3468 - acc: 0.8891 - val_loss: 1.3383 - val_acc: 0.6864\n",
            "Epoch 45/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3386 - acc: 0.8907 - val_loss: 1.2256 - val_acc: 0.7034\n",
            "Epoch 46/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3298 - acc: 0.8941 - val_loss: 1.2090 - val_acc: 0.6849\n",
            "Epoch 47/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3382 - acc: 0.8896 - val_loss: 1.0428 - val_acc: 0.7393\n",
            "Epoch 48/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3329 - acc: 0.8915 - val_loss: 1.1726 - val_acc: 0.7231\n",
            "Epoch 49/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3311 - acc: 0.8912 - val_loss: 1.1831 - val_acc: 0.7007\n",
            "Epoch 50/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3424 - acc: 0.8882 - val_loss: 1.4238 - val_acc: 0.6717\n",
            "Epoch 51/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3306 - acc: 0.8901 - val_loss: 0.9818 - val_acc: 0.7520\n",
            "Epoch 52/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3272 - acc: 0.8931 - val_loss: 1.2552 - val_acc: 0.6842\n",
            "Epoch 53/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3174 - acc: 0.8976 - val_loss: 1.5607 - val_acc: 0.6632\n",
            "Epoch 54/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3006 - acc: 0.9022 - val_loss: 1.0180 - val_acc: 0.7527\n",
            "Epoch 55/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2920 - acc: 0.9050 - val_loss: 1.0171 - val_acc: 0.7591\n",
            "Epoch 56/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3032 - acc: 0.9016 - val_loss: 1.3974 - val_acc: 0.6902\n",
            "Epoch 57/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3065 - acc: 0.9004 - val_loss: 1.1735 - val_acc: 0.7264\n",
            "Epoch 58/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2998 - acc: 0.9018 - val_loss: 1.0032 - val_acc: 0.7620\n",
            "Epoch 59/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2876 - acc: 0.9054 - val_loss: 1.3483 - val_acc: 0.7067\n",
            "Epoch 60/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2899 - acc: 0.9039 - val_loss: 1.1015 - val_acc: 0.7552\n",
            "Epoch 61/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2967 - acc: 0.9019 - val_loss: 1.2437 - val_acc: 0.7251\n",
            "Epoch 62/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2868 - acc: 0.9057 - val_loss: 1.2568 - val_acc: 0.7106\n",
            "Epoch 63/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2896 - acc: 0.9051 - val_loss: 1.1592 - val_acc: 0.7319\n",
            "Epoch 64/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2786 - acc: 0.9107 - val_loss: 1.3910 - val_acc: 0.7000\n",
            "Epoch 65/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3028 - acc: 0.8987 - val_loss: 1.3959 - val_acc: 0.7052\n",
            "Epoch 66/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2819 - acc: 0.9069 - val_loss: 1.3916 - val_acc: 0.6953\n",
            "Epoch 67/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2683 - acc: 0.9113 - val_loss: 1.2859 - val_acc: 0.7064\n",
            "Epoch 68/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2647 - acc: 0.9130 - val_loss: 1.0993 - val_acc: 0.7494\n",
            "Epoch 69/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2620 - acc: 0.9138 - val_loss: 1.2992 - val_acc: 0.7251\n",
            "Epoch 70/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2759 - acc: 0.9092 - val_loss: 1.4836 - val_acc: 0.6871\n",
            "Epoch 71/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2797 - acc: 0.9083 - val_loss: 1.3321 - val_acc: 0.7338\n",
            "Epoch 72/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2698 - acc: 0.9109 - val_loss: 1.8430 - val_acc: 0.6583\n",
            "Epoch 73/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2598 - acc: 0.9136 - val_loss: 1.3014 - val_acc: 0.7365\n",
            "Epoch 74/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2584 - acc: 0.9158 - val_loss: 1.3740 - val_acc: 0.7121\n",
            "Epoch 75/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2492 - acc: 0.9177 - val_loss: 1.0818 - val_acc: 0.7605\n",
            "Epoch 76/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2504 - acc: 0.9178 - val_loss: 1.2378 - val_acc: 0.7395\n",
            "Epoch 77/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2388 - acc: 0.9223 - val_loss: 1.2855 - val_acc: 0.7346\n",
            "Epoch 78/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2538 - acc: 0.9160 - val_loss: 1.4854 - val_acc: 0.7018\n",
            "Epoch 79/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2463 - acc: 0.9190 - val_loss: 1.4337 - val_acc: 0.7074\n",
            "Epoch 80/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2522 - acc: 0.9155 - val_loss: 1.7843 - val_acc: 0.6743\n",
            "Epoch 81/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2556 - acc: 0.9146 - val_loss: 1.2828 - val_acc: 0.7363\n",
            "Epoch 82/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2519 - acc: 0.9168 - val_loss: 1.6694 - val_acc: 0.6871\n",
            "Epoch 83/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2445 - acc: 0.9207 - val_loss: 1.3583 - val_acc: 0.7302\n",
            "Epoch 84/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2285 - acc: 0.9240 - val_loss: 1.2201 - val_acc: 0.7521\n",
            "Epoch 85/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2266 - acc: 0.9264 - val_loss: 1.1022 - val_acc: 0.7598\n",
            "Epoch 86/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2141 - acc: 0.9294 - val_loss: 1.1721 - val_acc: 0.7526\n",
            "Epoch 87/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2200 - acc: 0.9264 - val_loss: 1.4108 - val_acc: 0.7169\n",
            "Epoch 88/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2308 - acc: 0.9224 - val_loss: 1.1985 - val_acc: 0.7534\n",
            "Epoch 89/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2231 - acc: 0.9254 - val_loss: 1.3486 - val_acc: 0.7423\n",
            "Epoch 90/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2186 - acc: 0.9276 - val_loss: 1.2948 - val_acc: 0.7344\n",
            "Epoch 91/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2216 - acc: 0.9263 - val_loss: 1.2662 - val_acc: 0.7436\n",
            "Epoch 92/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2360 - acc: 0.9227 - val_loss: 1.5481 - val_acc: 0.7057\n",
            "Epoch 93/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2266 - acc: 0.9243 - val_loss: 1.4210 - val_acc: 0.7333\n",
            "Epoch 94/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2266 - acc: 0.9239 - val_loss: 1.5711 - val_acc: 0.7138\n",
            "Epoch 95/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2294 - acc: 0.9234 - val_loss: 1.6590 - val_acc: 0.6932\n",
            "Epoch 96/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2441 - acc: 0.9180 - val_loss: 1.3625 - val_acc: 0.7453\n",
            "Epoch 97/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2204 - acc: 0.9256 - val_loss: 1.3062 - val_acc: 0.7350\n",
            "Epoch 98/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2082 - acc: 0.9304 - val_loss: 1.3255 - val_acc: 0.7593\n",
            "Epoch 99/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2111 - acc: 0.9295 - val_loss: 1.2757 - val_acc: 0.7412\n",
            "Epoch 100/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2003 - acc: 0.9331 - val_loss: 1.2854 - val_acc: 0.7400\n",
            "Epoch 101/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1988 - acc: 0.9341 - val_loss: 1.2555 - val_acc: 0.7536\n",
            "Epoch 102/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1899 - acc: 0.9374 - val_loss: 1.5224 - val_acc: 0.7306\n",
            "Epoch 103/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2003 - acc: 0.9321 - val_loss: 1.6399 - val_acc: 0.7128\n",
            "Epoch 104/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2028 - acc: 0.9345 - val_loss: 1.5409 - val_acc: 0.7161\n",
            "Epoch 105/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2112 - acc: 0.9295 - val_loss: 1.4470 - val_acc: 0.7252\n",
            "Epoch 106/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1919 - acc: 0.9362 - val_loss: 1.3468 - val_acc: 0.7419\n",
            "Epoch 107/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1860 - acc: 0.9381 - val_loss: 1.4592 - val_acc: 0.7379\n",
            "Epoch 108/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2068 - acc: 0.9318 - val_loss: 1.5021 - val_acc: 0.6943\n",
            "Epoch 109/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2202 - acc: 0.9260 - val_loss: 1.3659 - val_acc: 0.7477\n",
            "Epoch 110/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1959 - acc: 0.9357 - val_loss: 1.4340 - val_acc: 0.7402\n",
            "Epoch 111/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2010 - acc: 0.9340 - val_loss: 1.3989 - val_acc: 0.7240\n",
            "Epoch 112/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1919 - acc: 0.9349 - val_loss: 1.3245 - val_acc: 0.7428\n",
            "Epoch 113/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1815 - acc: 0.9397 - val_loss: 1.2477 - val_acc: 0.7624\n",
            "Epoch 114/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1782 - acc: 0.9407 - val_loss: 1.5814 - val_acc: 0.7213\n",
            "Epoch 115/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1941 - acc: 0.9350 - val_loss: 1.6159 - val_acc: 0.7113\n",
            "Epoch 116/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2005 - acc: 0.9318 - val_loss: 1.5681 - val_acc: 0.7291\n",
            "Epoch 117/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1928 - acc: 0.9345 - val_loss: 1.7319 - val_acc: 0.7156\n",
            "Epoch 118/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2072 - acc: 0.9290 - val_loss: 1.5496 - val_acc: 0.7105\n",
            "Epoch 119/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1991 - acc: 0.9339 - val_loss: 1.3834 - val_acc: 0.7660\n",
            "Epoch 120/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1883 - acc: 0.9365 - val_loss: 1.5211 - val_acc: 0.7193\n",
            "Epoch 121/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1768 - acc: 0.9413 - val_loss: 1.3393 - val_acc: 0.7509\n",
            "Epoch 122/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1701 - acc: 0.9443 - val_loss: 1.3909 - val_acc: 0.7492\n",
            "Epoch 123/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1623 - acc: 0.9458 - val_loss: 1.3948 - val_acc: 0.7431\n",
            "Epoch 124/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1667 - acc: 0.9438 - val_loss: 1.4249 - val_acc: 0.7469\n",
            "Epoch 125/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1801 - acc: 0.9401 - val_loss: 1.2422 - val_acc: 0.7691\n",
            "Epoch 126/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1739 - acc: 0.9412 - val_loss: 1.4018 - val_acc: 0.7503\n",
            "Epoch 127/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1815 - acc: 0.9406 - val_loss: 1.5868 - val_acc: 0.7098\n",
            "Epoch 128/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1710 - acc: 0.9426 - val_loss: 1.3924 - val_acc: 0.7642\n",
            "Epoch 129/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1650 - acc: 0.9455 - val_loss: 1.5204 - val_acc: 0.7313\n",
            "Epoch 130/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1812 - acc: 0.9393 - val_loss: 1.3806 - val_acc: 0.7577\n",
            "Epoch 131/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1706 - acc: 0.9442 - val_loss: 1.3651 - val_acc: 0.7542\n",
            "Epoch 132/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1680 - acc: 0.9425 - val_loss: 1.4317 - val_acc: 0.7426\n",
            "Epoch 133/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1668 - acc: 0.9437 - val_loss: 1.5770 - val_acc: 0.7398\n",
            "Epoch 134/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1702 - acc: 0.9421 - val_loss: 1.3424 - val_acc: 0.7684\n",
            "Epoch 135/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1512 - acc: 0.9512 - val_loss: 1.4164 - val_acc: 0.7551\n",
            "Epoch 136/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1560 - acc: 0.9474 - val_loss: 1.4404 - val_acc: 0.7448\n",
            "Epoch 137/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1549 - acc: 0.9487 - val_loss: 1.4243 - val_acc: 0.7549\n",
            "Epoch 138/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1638 - acc: 0.9444 - val_loss: 1.4112 - val_acc: 0.7501\n",
            "Epoch 139/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1639 - acc: 0.9455 - val_loss: 1.5485 - val_acc: 0.7497\n",
            "Epoch 140/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1571 - acc: 0.9476 - val_loss: 1.7013 - val_acc: 0.7327\n",
            "Epoch 141/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1492 - acc: 0.9497 - val_loss: 1.7813 - val_acc: 0.7182\n",
            "Epoch 142/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1819 - acc: 0.9389 - val_loss: 2.0076 - val_acc: 0.7167\n",
            "Epoch 143/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1942 - acc: 0.9352 - val_loss: 1.6048 - val_acc: 0.7217\n",
            "Epoch 144/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1824 - acc: 0.9376 - val_loss: 1.7705 - val_acc: 0.7058\n",
            "Epoch 145/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1838 - acc: 0.9365 - val_loss: 1.5847 - val_acc: 0.7457\n",
            "Epoch 146/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1776 - acc: 0.9401 - val_loss: 1.5335 - val_acc: 0.7372\n",
            "Epoch 147/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1589 - acc: 0.9455 - val_loss: 1.4882 - val_acc: 0.7501\n",
            "Epoch 148/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1516 - acc: 0.9494 - val_loss: 1.3667 - val_acc: 0.7587\n",
            "Epoch 149/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1487 - acc: 0.9490 - val_loss: 1.5270 - val_acc: 0.7278\n",
            "Epoch 150/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1601 - acc: 0.9460 - val_loss: 1.4129 - val_acc: 0.7579\n",
            "Epoch 151/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1432 - acc: 0.9517 - val_loss: 2.2885 - val_acc: 0.6843\n",
            "Epoch 152/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1614 - acc: 0.9457 - val_loss: 1.7052 - val_acc: 0.7246\n",
            "Epoch 153/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1678 - acc: 0.9426 - val_loss: 1.3507 - val_acc: 0.7656\n",
            "Epoch 154/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1581 - acc: 0.9468 - val_loss: 1.4387 - val_acc: 0.7539\n",
            "Epoch 155/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1475 - acc: 0.9513 - val_loss: 1.6614 - val_acc: 0.7332\n",
            "Epoch 156/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1496 - acc: 0.9498 - val_loss: 1.5034 - val_acc: 0.7398\n",
            "Epoch 157/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1460 - acc: 0.9504 - val_loss: 1.6078 - val_acc: 0.7367\n",
            "Epoch 158/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1396 - acc: 0.9543 - val_loss: 1.6467 - val_acc: 0.7392\n",
            "Epoch 159/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1395 - acc: 0.9543 - val_loss: 1.4094 - val_acc: 0.7599\n",
            "Epoch 160/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1352 - acc: 0.9534 - val_loss: 1.7863 - val_acc: 0.7229\n",
            "Epoch 161/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1403 - acc: 0.9509 - val_loss: 1.4844 - val_acc: 0.7561\n",
            "Epoch 162/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1493 - acc: 0.9493 - val_loss: 1.5028 - val_acc: 0.7574\n",
            "Epoch 163/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1307 - acc: 0.9550 - val_loss: 1.3711 - val_acc: 0.7692\n",
            "Epoch 164/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1209 - acc: 0.9594 - val_loss: 1.4822 - val_acc: 0.7607\n",
            "Epoch 165/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1245 - acc: 0.9593 - val_loss: 1.5552 - val_acc: 0.7540\n",
            "Epoch 166/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1285 - acc: 0.9564 - val_loss: 1.4741 - val_acc: 0.7569\n",
            "Epoch 167/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1594 - acc: 0.9460 - val_loss: 1.9403 - val_acc: 0.7084\n",
            "Epoch 168/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1570 - acc: 0.9465 - val_loss: 1.7106 - val_acc: 0.7309\n",
            "Epoch 169/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1473 - acc: 0.9497 - val_loss: 1.6033 - val_acc: 0.7466\n",
            "Epoch 170/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1340 - acc: 0.9542 - val_loss: 1.5753 - val_acc: 0.7531\n",
            "Epoch 171/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1254 - acc: 0.9578 - val_loss: 1.5542 - val_acc: 0.7447\n",
            "Epoch 172/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1217 - acc: 0.9576 - val_loss: 1.4990 - val_acc: 0.7607\n",
            "Epoch 173/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1248 - acc: 0.9587 - val_loss: 1.4464 - val_acc: 0.7694\n",
            "Epoch 174/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1223 - acc: 0.9591 - val_loss: 1.8286 - val_acc: 0.7286\n",
            "Epoch 175/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1400 - acc: 0.9533 - val_loss: 1.7647 - val_acc: 0.7239\n",
            "Epoch 176/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1553 - acc: 0.9473 - val_loss: 1.7551 - val_acc: 0.7329\n",
            "Epoch 177/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1642 - acc: 0.9427 - val_loss: 1.5869 - val_acc: 0.7581\n",
            "Epoch 178/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1330 - acc: 0.9544 - val_loss: 1.8276 - val_acc: 0.7153\n",
            "Epoch 179/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1264 - acc: 0.9559 - val_loss: 1.7912 - val_acc: 0.7384\n",
            "Epoch 180/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1310 - acc: 0.9552 - val_loss: 1.7036 - val_acc: 0.7373\n",
            "Epoch 181/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1327 - acc: 0.9544 - val_loss: 1.4939 - val_acc: 0.7619\n",
            "Epoch 182/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1219 - acc: 0.9578 - val_loss: 1.5326 - val_acc: 0.7566\n",
            "Epoch 183/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1212 - acc: 0.9584 - val_loss: 1.7959 - val_acc: 0.7276\n",
            "Epoch 184/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1310 - acc: 0.9556 - val_loss: 1.6764 - val_acc: 0.7408\n",
            "Epoch 185/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1300 - acc: 0.9563 - val_loss: 1.6930 - val_acc: 0.7481\n",
            "Epoch 186/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1322 - acc: 0.9543 - val_loss: 1.5688 - val_acc: 0.7464\n",
            "Epoch 187/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1401 - acc: 0.9520 - val_loss: 1.8456 - val_acc: 0.7348\n",
            "Epoch 188/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1368 - acc: 0.9519 - val_loss: 2.0586 - val_acc: 0.7040\n",
            "Epoch 189/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1212 - acc: 0.9580 - val_loss: 1.5544 - val_acc: 0.7582\n",
            "Epoch 190/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1211 - acc: 0.9596 - val_loss: 1.8107 - val_acc: 0.7210\n",
            "Epoch 191/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1619 - acc: 0.9460 - val_loss: 1.7759 - val_acc: 0.7219\n",
            "Epoch 192/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1531 - acc: 0.9481 - val_loss: 1.8729 - val_acc: 0.7291\n",
            "Epoch 193/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1423 - acc: 0.9507 - val_loss: 1.6538 - val_acc: 0.7375\n",
            "Epoch 194/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1268 - acc: 0.9569 - val_loss: 1.5208 - val_acc: 0.7604\n",
            "Epoch 195/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1206 - acc: 0.9602 - val_loss: 1.6436 - val_acc: 0.7490\n",
            "Epoch 196/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1160 - acc: 0.9597 - val_loss: 1.4556 - val_acc: 0.7573\n",
            "Epoch 197/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1053 - acc: 0.9654 - val_loss: 1.6509 - val_acc: 0.7498\n",
            "Epoch 198/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1203 - acc: 0.9596 - val_loss: 2.0440 - val_acc: 0.7063\n",
            "Epoch 199/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1326 - acc: 0.9553 - val_loss: 1.8406 - val_acc: 0.7309\n",
            "Epoch 200/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1191 - acc: 0.9592 - val_loss: 1.6553 - val_acc: 0.7417\n",
            "Epoch 201/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1156 - acc: 0.9604 - val_loss: 1.9153 - val_acc: 0.7259\n",
            "Epoch 202/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1221 - acc: 0.9587 - val_loss: 1.6756 - val_acc: 0.7572\n",
            "Epoch 203/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1236 - acc: 0.9581 - val_loss: 1.6927 - val_acc: 0.7501\n",
            "Epoch 204/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1092 - acc: 0.9624 - val_loss: 1.5905 - val_acc: 0.7553\n",
            "Epoch 205/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1051 - acc: 0.9655 - val_loss: 1.7831 - val_acc: 0.7389\n",
            "Epoch 206/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1141 - acc: 0.9611 - val_loss: 2.0585 - val_acc: 0.7216\n",
            "Epoch 207/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1224 - acc: 0.9584 - val_loss: 1.8534 - val_acc: 0.7298\n",
            "Epoch 208/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1321 - acc: 0.9555 - val_loss: 1.8894 - val_acc: 0.7253\n",
            "Epoch 209/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1173 - acc: 0.9609 - val_loss: 1.9965 - val_acc: 0.7321\n",
            "Epoch 210/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1135 - acc: 0.9619 - val_loss: 1.7374 - val_acc: 0.7487\n",
            "Epoch 211/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1148 - acc: 0.9602 - val_loss: 1.8296 - val_acc: 0.7361\n",
            "Epoch 212/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1231 - acc: 0.9581 - val_loss: 1.8646 - val_acc: 0.7398\n",
            "Epoch 213/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1177 - acc: 0.9591 - val_loss: 1.5848 - val_acc: 0.7620\n",
            "Epoch 214/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1287 - acc: 0.9557 - val_loss: 1.7658 - val_acc: 0.7407\n",
            "Epoch 215/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1411 - acc: 0.9519 - val_loss: 1.6681 - val_acc: 0.7598\n",
            "Epoch 216/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1196 - acc: 0.9585 - val_loss: 1.7134 - val_acc: 0.7432\n",
            "Epoch 217/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1113 - acc: 0.9619 - val_loss: 1.8144 - val_acc: 0.7404\n",
            "Epoch 218/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1257 - acc: 0.9560 - val_loss: 1.8446 - val_acc: 0.7188\n",
            "Epoch 219/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1125 - acc: 0.9621 - val_loss: 1.5893 - val_acc: 0.7592\n",
            "Epoch 220/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1031 - acc: 0.9650 - val_loss: 1.5783 - val_acc: 0.7586\n",
            "Epoch 221/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0993 - acc: 0.9649 - val_loss: 1.7092 - val_acc: 0.7499\n",
            "Epoch 222/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1103 - acc: 0.9629 - val_loss: 1.6565 - val_acc: 0.7604\n",
            "Epoch 223/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1153 - acc: 0.9619 - val_loss: 1.6405 - val_acc: 0.7462\n",
            "Epoch 224/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1121 - acc: 0.9617 - val_loss: 2.0060 - val_acc: 0.7279\n",
            "Epoch 225/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1234 - acc: 0.9574 - val_loss: 1.7941 - val_acc: 0.7440\n",
            "Epoch 226/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1101 - acc: 0.9618 - val_loss: 1.7500 - val_acc: 0.7496\n",
            "Epoch 227/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1078 - acc: 0.9640 - val_loss: 1.9292 - val_acc: 0.7338\n",
            "Epoch 228/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1217 - acc: 0.9596 - val_loss: 2.0622 - val_acc: 0.7190\n",
            "Epoch 229/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1281 - acc: 0.9561 - val_loss: 1.9503 - val_acc: 0.7226\n",
            "Epoch 230/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1166 - acc: 0.9605 - val_loss: 1.8856 - val_acc: 0.7298\n",
            "Epoch 231/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1117 - acc: 0.9618 - val_loss: 1.7097 - val_acc: 0.7533\n",
            "Epoch 232/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1014 - acc: 0.9657 - val_loss: 1.8477 - val_acc: 0.7373\n",
            "Epoch 233/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0946 - acc: 0.9678 - val_loss: 1.6880 - val_acc: 0.7513\n",
            "Epoch 234/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0914 - acc: 0.9685 - val_loss: 1.8120 - val_acc: 0.7505\n",
            "Epoch 235/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0846 - acc: 0.9718 - val_loss: 1.6362 - val_acc: 0.7609\n",
            "Epoch 236/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0856 - acc: 0.9715 - val_loss: 1.6938 - val_acc: 0.7597\n",
            "Epoch 237/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0932 - acc: 0.9697 - val_loss: 1.9200 - val_acc: 0.7354\n",
            "Epoch 238/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1115 - acc: 0.9622 - val_loss: 1.9376 - val_acc: 0.7312\n",
            "Epoch 239/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1099 - acc: 0.9619 - val_loss: 1.9590 - val_acc: 0.7317\n",
            "Epoch 240/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1126 - acc: 0.9621 - val_loss: 1.7965 - val_acc: 0.7495\n",
            "Epoch 241/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1308 - acc: 0.9560 - val_loss: 1.8567 - val_acc: 0.7421\n",
            "Epoch 242/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1137 - acc: 0.9592 - val_loss: 1.6895 - val_acc: 0.7645\n",
            "Epoch 243/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0995 - acc: 0.9658 - val_loss: 1.7863 - val_acc: 0.7654\n",
            "Epoch 244/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0911 - acc: 0.9697 - val_loss: 1.7913 - val_acc: 0.7636\n",
            "Epoch 245/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1031 - acc: 0.9649 - val_loss: 1.7327 - val_acc: 0.7509\n",
            "Epoch 246/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0967 - acc: 0.9665 - val_loss: 1.6781 - val_acc: 0.7651\n",
            "Epoch 247/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0936 - acc: 0.9677 - val_loss: 1.8858 - val_acc: 0.7429\n",
            "Epoch 248/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1038 - acc: 0.9649 - val_loss: 1.9107 - val_acc: 0.7350\n",
            "Epoch 249/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1482 - acc: 0.9520 - val_loss: 2.1569 - val_acc: 0.6932\n",
            "Epoch 250/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1556 - acc: 0.9484 - val_loss: 1.8448 - val_acc: 0.7567\n",
            "Epoch 251/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1315 - acc: 0.9536 - val_loss: 1.9599 - val_acc: 0.7437\n",
            "Epoch 252/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1133 - acc: 0.9620 - val_loss: 1.7301 - val_acc: 0.7524\n",
            "Epoch 253/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1025 - acc: 0.9645 - val_loss: 1.7673 - val_acc: 0.7553\n",
            "Epoch 254/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0896 - acc: 0.9698 - val_loss: 1.6475 - val_acc: 0.7647\n",
            "Epoch 255/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0773 - acc: 0.9740 - val_loss: 1.9110 - val_acc: 0.7430\n",
            "Epoch 256/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0824 - acc: 0.9718 - val_loss: 1.6193 - val_acc: 0.7672\n",
            "Epoch 257/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0786 - acc: 0.9750 - val_loss: 1.8015 - val_acc: 0.7516\n",
            "Epoch 258/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0874 - acc: 0.9702 - val_loss: 1.7173 - val_acc: 0.7586\n",
            "Epoch 259/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0831 - acc: 0.9724 - val_loss: 1.8438 - val_acc: 0.7527\n",
            "Epoch 260/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0936 - acc: 0.9685 - val_loss: 1.7700 - val_acc: 0.7454\n",
            "Epoch 261/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1161 - acc: 0.9612 - val_loss: 1.8059 - val_acc: 0.7507\n",
            "Epoch 262/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1102 - acc: 0.9620 - val_loss: 1.7105 - val_acc: 0.7576\n",
            "Epoch 263/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1059 - acc: 0.9628 - val_loss: 2.3957 - val_acc: 0.6973\n",
            "Epoch 264/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1139 - acc: 0.9619 - val_loss: 1.8247 - val_acc: 0.7351\n",
            "Epoch 265/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1044 - acc: 0.9637 - val_loss: 1.7947 - val_acc: 0.7584\n",
            "Epoch 266/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0957 - acc: 0.9676 - val_loss: 1.7779 - val_acc: 0.7516\n",
            "Epoch 267/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1010 - acc: 0.9663 - val_loss: 1.8544 - val_acc: 0.7420\n",
            "Epoch 268/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1059 - acc: 0.9651 - val_loss: 1.7283 - val_acc: 0.7568\n",
            "Epoch 269/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0867 - acc: 0.9709 - val_loss: 1.7987 - val_acc: 0.7503\n",
            "Epoch 270/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0893 - acc: 0.9699 - val_loss: 1.7176 - val_acc: 0.7603\n",
            "Epoch 271/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0800 - acc: 0.9727 - val_loss: 1.9750 - val_acc: 0.7439\n",
            "Epoch 272/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0727 - acc: 0.9754 - val_loss: 1.8559 - val_acc: 0.7504\n",
            "Epoch 273/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0860 - acc: 0.9707 - val_loss: 1.7066 - val_acc: 0.7684\n",
            "Epoch 274/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0927 - acc: 0.9686 - val_loss: 1.7981 - val_acc: 0.7572\n",
            "Epoch 275/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0942 - acc: 0.9673 - val_loss: 1.8000 - val_acc: 0.7554\n",
            "Epoch 276/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0908 - acc: 0.9682 - val_loss: 1.7410 - val_acc: 0.7701\n",
            "Epoch 277/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0899 - acc: 0.9693 - val_loss: 2.0614 - val_acc: 0.7395\n",
            "Epoch 278/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0842 - acc: 0.9709 - val_loss: 1.6540 - val_acc: 0.7698\n",
            "Epoch 279/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0772 - acc: 0.9747 - val_loss: 1.7372 - val_acc: 0.7610\n",
            "Epoch 280/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0765 - acc: 0.9741 - val_loss: 1.6956 - val_acc: 0.7683\n",
            "Epoch 281/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0718 - acc: 0.9763 - val_loss: 1.6947 - val_acc: 0.7676\n",
            "Epoch 282/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0777 - acc: 0.9744 - val_loss: 2.0723 - val_acc: 0.7438\n",
            "Epoch 283/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0946 - acc: 0.9672 - val_loss: 1.8005 - val_acc: 0.7594\n",
            "Epoch 284/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1054 - acc: 0.9644 - val_loss: 2.3381 - val_acc: 0.6973\n",
            "Epoch 285/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1179 - acc: 0.9594 - val_loss: 2.1092 - val_acc: 0.7322\n",
            "Epoch 286/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0985 - acc: 0.9654 - val_loss: 1.9063 - val_acc: 0.7412\n",
            "Epoch 287/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0932 - acc: 0.9669 - val_loss: 1.8585 - val_acc: 0.7463\n",
            "Epoch 288/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1165 - acc: 0.9614 - val_loss: 2.2106 - val_acc: 0.7287\n",
            "Epoch 289/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1090 - acc: 0.9632 - val_loss: 2.2970 - val_acc: 0.7139\n",
            "Epoch 290/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1312 - acc: 0.9555 - val_loss: 1.8015 - val_acc: 0.7496\n",
            "Epoch 291/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1213 - acc: 0.9589 - val_loss: 2.3161 - val_acc: 0.7149\n",
            "Epoch 292/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1268 - acc: 0.9567 - val_loss: 1.7256 - val_acc: 0.7585\n",
            "Epoch 293/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0953 - acc: 0.9658 - val_loss: 1.6724 - val_acc: 0.7663\n",
            "Epoch 294/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0835 - acc: 0.9707 - val_loss: 1.8007 - val_acc: 0.7548\n",
            "Epoch 295/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0781 - acc: 0.9732 - val_loss: 1.9730 - val_acc: 0.7422\n",
            "Epoch 296/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0754 - acc: 0.9756 - val_loss: 1.9337 - val_acc: 0.7497\n",
            "Epoch 297/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0803 - acc: 0.9725 - val_loss: 1.8398 - val_acc: 0.7567\n",
            "Epoch 298/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0775 - acc: 0.9743 - val_loss: 1.7595 - val_acc: 0.7610\n",
            "Epoch 299/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0723 - acc: 0.9757 - val_loss: 1.8358 - val_acc: 0.7559\n",
            "Epoch 300/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0673 - acc: 0.9782 - val_loss: 1.8477 - val_acc: 0.7672\n",
            "Epoch 301/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0688 - acc: 0.9771 - val_loss: 1.9384 - val_acc: 0.7539\n",
            "Epoch 302/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0722 - acc: 0.9760 - val_loss: 1.8957 - val_acc: 0.7489\n",
            "Epoch 303/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0779 - acc: 0.9726 - val_loss: 1.9225 - val_acc: 0.7497\n",
            "Epoch 304/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0784 - acc: 0.9729 - val_loss: 1.8482 - val_acc: 0.7564\n",
            "Epoch 305/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0813 - acc: 0.9731 - val_loss: 1.7979 - val_acc: 0.7551\n",
            "Epoch 306/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0864 - acc: 0.9701 - val_loss: 2.0972 - val_acc: 0.7226\n",
            "Epoch 307/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0864 - acc: 0.9696 - val_loss: 1.7834 - val_acc: 0.7688\n",
            "Epoch 308/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0778 - acc: 0.9737 - val_loss: 1.9598 - val_acc: 0.7514\n",
            "Epoch 309/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0777 - acc: 0.9731 - val_loss: 2.3231 - val_acc: 0.7229\n",
            "Epoch 310/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0769 - acc: 0.9739 - val_loss: 2.0613 - val_acc: 0.7447\n",
            "Epoch 311/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0824 - acc: 0.9720 - val_loss: 1.9112 - val_acc: 0.7464\n",
            "Epoch 312/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0795 - acc: 0.9735 - val_loss: 1.9332 - val_acc: 0.7531\n",
            "Epoch 313/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0862 - acc: 0.9708 - val_loss: 1.9135 - val_acc: 0.7600\n",
            "Epoch 314/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0802 - acc: 0.9721 - val_loss: 1.8560 - val_acc: 0.7516\n",
            "Epoch 315/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0797 - acc: 0.9718 - val_loss: 1.9844 - val_acc: 0.7429\n",
            "Epoch 316/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0876 - acc: 0.9702 - val_loss: 1.7725 - val_acc: 0.7640\n",
            "Epoch 317/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1024 - acc: 0.9665 - val_loss: 2.0873 - val_acc: 0.7325\n",
            "Epoch 318/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1079 - acc: 0.9632 - val_loss: 1.8130 - val_acc: 0.7580\n",
            "Epoch 319/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1039 - acc: 0.9643 - val_loss: 2.0772 - val_acc: 0.7429\n",
            "Epoch 320/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0902 - acc: 0.9692 - val_loss: 1.8747 - val_acc: 0.7503\n",
            "Epoch 321/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0826 - acc: 0.9724 - val_loss: 1.9449 - val_acc: 0.7466\n",
            "Epoch 322/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0799 - acc: 0.9724 - val_loss: 1.8340 - val_acc: 0.7573\n",
            "Epoch 323/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0754 - acc: 0.9751 - val_loss: 1.9369 - val_acc: 0.7471\n",
            "Epoch 324/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0857 - acc: 0.9705 - val_loss: 1.9839 - val_acc: 0.7431\n",
            "Epoch 325/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0817 - acc: 0.9724 - val_loss: 1.8012 - val_acc: 0.7613\n",
            "Epoch 326/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0803 - acc: 0.9732 - val_loss: 1.9722 - val_acc: 0.7286\n",
            "Epoch 327/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0803 - acc: 0.9729 - val_loss: 1.8842 - val_acc: 0.7658\n",
            "Epoch 328/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0702 - acc: 0.9763 - val_loss: 1.8365 - val_acc: 0.7669\n",
            "Epoch 329/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0668 - acc: 0.9775 - val_loss: 1.8155 - val_acc: 0.7661\n",
            "Epoch 330/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0622 - acc: 0.9783 - val_loss: 1.7372 - val_acc: 0.7701\n",
            "Epoch 331/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0673 - acc: 0.9770 - val_loss: 2.0344 - val_acc: 0.7399\n",
            "Epoch 332/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0801 - acc: 0.9725 - val_loss: 2.1729 - val_acc: 0.7262\n",
            "Epoch 333/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0806 - acc: 0.9740 - val_loss: 1.8864 - val_acc: 0.7572\n",
            "Epoch 334/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0891 - acc: 0.9692 - val_loss: 2.3598 - val_acc: 0.7100\n",
            "Epoch 335/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0952 - acc: 0.9681 - val_loss: 1.8753 - val_acc: 0.7585\n",
            "Epoch 336/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0805 - acc: 0.9732 - val_loss: 1.9543 - val_acc: 0.7517\n",
            "Epoch 337/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0848 - acc: 0.9709 - val_loss: 1.8364 - val_acc: 0.7671\n",
            "Epoch 338/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0880 - acc: 0.9701 - val_loss: 1.9062 - val_acc: 0.7597\n",
            "Epoch 339/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0830 - acc: 0.9710 - val_loss: 2.1465 - val_acc: 0.7267\n",
            "Epoch 340/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0838 - acc: 0.9710 - val_loss: 2.0083 - val_acc: 0.7554\n",
            "Epoch 341/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0724 - acc: 0.9751 - val_loss: 2.0911 - val_acc: 0.7404\n",
            "Epoch 342/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0828 - acc: 0.9714 - val_loss: 2.2869 - val_acc: 0.7321\n",
            "Epoch 343/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0805 - acc: 0.9724 - val_loss: 1.9154 - val_acc: 0.7567\n",
            "Epoch 344/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0744 - acc: 0.9748 - val_loss: 1.9231 - val_acc: 0.7571\n",
            "Epoch 345/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0727 - acc: 0.9759 - val_loss: 2.1923 - val_acc: 0.7413\n",
            "Epoch 346/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0652 - acc: 0.9780 - val_loss: 2.0821 - val_acc: 0.7466\n",
            "Epoch 347/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0705 - acc: 0.9765 - val_loss: 1.9031 - val_acc: 0.7609\n",
            "Epoch 348/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0656 - acc: 0.9776 - val_loss: 1.8245 - val_acc: 0.7669\n",
            "Epoch 349/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0724 - acc: 0.9768 - val_loss: 2.0444 - val_acc: 0.7450\n",
            "Epoch 350/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0761 - acc: 0.9741 - val_loss: 1.9718 - val_acc: 0.7466\n",
            "Epoch 351/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0700 - acc: 0.9764 - val_loss: 1.9795 - val_acc: 0.7472\n",
            "Epoch 352/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0742 - acc: 0.9754 - val_loss: 1.9709 - val_acc: 0.7514\n",
            "Epoch 353/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0651 - acc: 0.9780 - val_loss: 1.8062 - val_acc: 0.7742\n",
            "Epoch 354/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0639 - acc: 0.9789 - val_loss: 1.9357 - val_acc: 0.7591\n",
            "Epoch 355/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0696 - acc: 0.9757 - val_loss: 1.9198 - val_acc: 0.7527\n",
            "Epoch 356/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0700 - acc: 0.9766 - val_loss: 1.8149 - val_acc: 0.7624\n",
            "Epoch 357/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0705 - acc: 0.9767 - val_loss: 2.1879 - val_acc: 0.7430\n",
            "Epoch 358/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0892 - acc: 0.9696 - val_loss: 2.1882 - val_acc: 0.7409\n",
            "Epoch 359/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1048 - acc: 0.9652 - val_loss: 2.1723 - val_acc: 0.7402\n",
            "Epoch 360/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0862 - acc: 0.9705 - val_loss: 1.9583 - val_acc: 0.7474\n",
            "Epoch 361/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0797 - acc: 0.9722 - val_loss: 2.1386 - val_acc: 0.7419\n",
            "Epoch 362/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0830 - acc: 0.9717 - val_loss: 2.2021 - val_acc: 0.7348\n",
            "Epoch 363/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0807 - acc: 0.9725 - val_loss: 2.0260 - val_acc: 0.7549\n",
            "Epoch 364/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0765 - acc: 0.9741 - val_loss: 2.0649 - val_acc: 0.7507\n",
            "Epoch 365/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0899 - acc: 0.9685 - val_loss: 2.4633 - val_acc: 0.7172\n",
            "Epoch 366/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0968 - acc: 0.9665 - val_loss: 2.3999 - val_acc: 0.7197\n",
            "Epoch 367/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1034 - acc: 0.9655 - val_loss: 2.0673 - val_acc: 0.7531\n",
            "Epoch 368/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0927 - acc: 0.9678 - val_loss: 2.0211 - val_acc: 0.7425\n",
            "Epoch 369/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0780 - acc: 0.9739 - val_loss: 1.9641 - val_acc: 0.7504\n",
            "Epoch 370/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0689 - acc: 0.9768 - val_loss: 1.9649 - val_acc: 0.7570\n",
            "Epoch 371/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0810 - acc: 0.9729 - val_loss: 1.9563 - val_acc: 0.7492\n",
            "Epoch 372/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0711 - acc: 0.9754 - val_loss: 2.1992 - val_acc: 0.7355\n",
            "Epoch 373/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0681 - acc: 0.9769 - val_loss: 2.0838 - val_acc: 0.7409\n",
            "Epoch 374/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0815 - acc: 0.9727 - val_loss: 2.1639 - val_acc: 0.7413\n",
            "Epoch 375/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0792 - acc: 0.9725 - val_loss: 1.9712 - val_acc: 0.7519\n",
            "Epoch 376/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0713 - acc: 0.9756 - val_loss: 2.0401 - val_acc: 0.7527\n",
            "Epoch 377/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0686 - acc: 0.9765 - val_loss: 1.9494 - val_acc: 0.7562\n",
            "Epoch 378/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0622 - acc: 0.9795 - val_loss: 1.9695 - val_acc: 0.7593\n",
            "Epoch 379/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0559 - acc: 0.9810 - val_loss: 1.9427 - val_acc: 0.7549\n",
            "Epoch 380/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0575 - acc: 0.9810 - val_loss: 1.9314 - val_acc: 0.7554\n",
            "Epoch 381/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0592 - acc: 0.9805 - val_loss: 1.9056 - val_acc: 0.7520\n",
            "Epoch 382/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0754 - acc: 0.9743 - val_loss: 1.9145 - val_acc: 0.7511\n",
            "Epoch 383/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0835 - acc: 0.9718 - val_loss: 2.2579 - val_acc: 0.7414\n",
            "Epoch 384/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0825 - acc: 0.9720 - val_loss: 1.9871 - val_acc: 0.7593\n",
            "Epoch 385/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0901 - acc: 0.9692 - val_loss: 2.0732 - val_acc: 0.7477\n",
            "Epoch 386/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0927 - acc: 0.9690 - val_loss: 2.2753 - val_acc: 0.7291\n",
            "Epoch 387/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0979 - acc: 0.9677 - val_loss: 2.1946 - val_acc: 0.7430\n",
            "Epoch 388/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0970 - acc: 0.9673 - val_loss: 2.4262 - val_acc: 0.7038\n",
            "Epoch 389/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0853 - acc: 0.9714 - val_loss: 2.2131 - val_acc: 0.7332\n",
            "Epoch 390/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0886 - acc: 0.9707 - val_loss: 2.1080 - val_acc: 0.7483\n",
            "Epoch 391/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0769 - acc: 0.9740 - val_loss: 1.9527 - val_acc: 0.7604\n",
            "Epoch 392/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0650 - acc: 0.9777 - val_loss: 1.8939 - val_acc: 0.7582\n",
            "Epoch 393/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0664 - acc: 0.9779 - val_loss: 1.9241 - val_acc: 0.7588\n",
            "Epoch 394/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0677 - acc: 0.9776 - val_loss: 2.0250 - val_acc: 0.7567\n",
            "Epoch 395/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0727 - acc: 0.9757 - val_loss: 2.0695 - val_acc: 0.7433\n",
            "Epoch 396/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0682 - acc: 0.9769 - val_loss: 2.0000 - val_acc: 0.7468\n",
            "Epoch 397/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0732 - acc: 0.9751 - val_loss: 2.0737 - val_acc: 0.7344\n",
            "Epoch 398/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0706 - acc: 0.9767 - val_loss: 1.9438 - val_acc: 0.7574\n",
            "Epoch 399/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0563 - acc: 0.9813 - val_loss: 1.9057 - val_acc: 0.7640\n",
            "Epoch 400/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0543 - acc: 0.9818 - val_loss: 1.9066 - val_acc: 0.7683\n",
            "Epoch 401/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0535 - acc: 0.9822 - val_loss: 2.1503 - val_acc: 0.7415\n",
            "Epoch 402/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0673 - acc: 0.9767 - val_loss: 1.9224 - val_acc: 0.7579\n",
            "Epoch 403/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0497 - acc: 0.9835 - val_loss: 1.9003 - val_acc: 0.7718\n",
            "Epoch 404/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0407 - acc: 0.9871 - val_loss: 1.8450 - val_acc: 0.7712\n",
            "Epoch 405/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0437 - acc: 0.9862 - val_loss: 2.0954 - val_acc: 0.7499\n",
            "Epoch 406/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0467 - acc: 0.9854 - val_loss: 1.9428 - val_acc: 0.7648\n",
            "Epoch 407/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0526 - acc: 0.9823 - val_loss: 1.9720 - val_acc: 0.7529\n",
            "Epoch 408/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0484 - acc: 0.9846 - val_loss: 2.1427 - val_acc: 0.7326\n",
            "Epoch 409/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0691 - acc: 0.9768 - val_loss: 2.0170 - val_acc: 0.7483\n",
            "Epoch 410/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0678 - acc: 0.9776 - val_loss: 2.0665 - val_acc: 0.7462\n",
            "Epoch 411/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0801 - acc: 0.9726 - val_loss: 2.3357 - val_acc: 0.7423\n",
            "Epoch 412/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1002 - acc: 0.9678 - val_loss: 2.4337 - val_acc: 0.7207\n",
            "Epoch 413/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1376 - acc: 0.9552 - val_loss: 2.2895 - val_acc: 0.7367\n",
            "Epoch 414/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1179 - acc: 0.9601 - val_loss: 2.1518 - val_acc: 0.7463\n",
            "Epoch 415/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0931 - acc: 0.9676 - val_loss: 2.0619 - val_acc: 0.7421\n",
            "Epoch 416/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0767 - acc: 0.9734 - val_loss: 2.0864 - val_acc: 0.7491\n",
            "Epoch 417/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0734 - acc: 0.9760 - val_loss: 1.9869 - val_acc: 0.7616\n",
            "Epoch 418/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0843 - acc: 0.9715 - val_loss: 2.0946 - val_acc: 0.7407\n",
            "Epoch 419/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0750 - acc: 0.9749 - val_loss: 2.4996 - val_acc: 0.7156\n",
            "Epoch 420/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0802 - acc: 0.9732 - val_loss: 1.9594 - val_acc: 0.7495\n",
            "Epoch 421/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0737 - acc: 0.9750 - val_loss: 1.8831 - val_acc: 0.7691\n",
            "Epoch 422/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0630 - acc: 0.9799 - val_loss: 2.0388 - val_acc: 0.7599\n",
            "Epoch 423/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0553 - acc: 0.9813 - val_loss: 2.0791 - val_acc: 0.7496\n",
            "Epoch 424/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0574 - acc: 0.9808 - val_loss: 1.8988 - val_acc: 0.7678\n",
            "Epoch 425/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0470 - acc: 0.9852 - val_loss: 1.9113 - val_acc: 0.7678\n",
            "Epoch 426/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0517 - acc: 0.9822 - val_loss: 1.8720 - val_acc: 0.7672\n",
            "Epoch 427/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0579 - acc: 0.9810 - val_loss: 1.9865 - val_acc: 0.7583\n",
            "Epoch 428/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0525 - acc: 0.9827 - val_loss: 1.8977 - val_acc: 0.7640\n",
            "Epoch 429/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0518 - acc: 0.9825 - val_loss: 1.8570 - val_acc: 0.7656\n",
            "Epoch 430/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0550 - acc: 0.9816 - val_loss: 2.0373 - val_acc: 0.7571\n",
            "Epoch 431/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0574 - acc: 0.9808 - val_loss: 1.9402 - val_acc: 0.7592\n",
            "Epoch 432/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0538 - acc: 0.9829 - val_loss: 1.9450 - val_acc: 0.7651\n",
            "Epoch 433/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0584 - acc: 0.9807 - val_loss: 1.9708 - val_acc: 0.7616\n",
            "Epoch 434/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0587 - acc: 0.9802 - val_loss: 1.9365 - val_acc: 0.7594\n",
            "Epoch 435/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0679 - acc: 0.9778 - val_loss: 2.2422 - val_acc: 0.7294\n",
            "Epoch 436/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0723 - acc: 0.9758 - val_loss: 2.3144 - val_acc: 0.7333\n",
            "Epoch 437/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0754 - acc: 0.9746 - val_loss: 2.2055 - val_acc: 0.7439\n",
            "Epoch 438/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0842 - acc: 0.9720 - val_loss: 1.9639 - val_acc: 0.7636\n",
            "Epoch 439/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0860 - acc: 0.9715 - val_loss: 2.1374 - val_acc: 0.7534\n",
            "Epoch 440/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0809 - acc: 0.9711 - val_loss: 2.1609 - val_acc: 0.7512\n",
            "Epoch 441/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0857 - acc: 0.9699 - val_loss: 2.1440 - val_acc: 0.7479\n",
            "Epoch 442/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0688 - acc: 0.9766 - val_loss: 2.0716 - val_acc: 0.7431\n",
            "Epoch 443/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0620 - acc: 0.9796 - val_loss: 1.9306 - val_acc: 0.7668\n",
            "Epoch 444/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0634 - acc: 0.9785 - val_loss: 2.0105 - val_acc: 0.7589\n",
            "Epoch 445/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0719 - acc: 0.9752 - val_loss: 2.1233 - val_acc: 0.7499\n",
            "Epoch 446/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0657 - acc: 0.9776 - val_loss: 1.9980 - val_acc: 0.7618\n",
            "Epoch 447/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0658 - acc: 0.9777 - val_loss: 2.0634 - val_acc: 0.7463\n",
            "Epoch 448/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0620 - acc: 0.9795 - val_loss: 2.0101 - val_acc: 0.7561\n",
            "Epoch 449/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0544 - acc: 0.9808 - val_loss: 2.0114 - val_acc: 0.7540\n",
            "Epoch 450/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0648 - acc: 0.9781 - val_loss: 2.0989 - val_acc: 0.7442\n",
            "Epoch 451/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0652 - acc: 0.9770 - val_loss: 2.0400 - val_acc: 0.7512\n",
            "Epoch 452/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0658 - acc: 0.9774 - val_loss: 1.9917 - val_acc: 0.7661\n",
            "Epoch 453/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0719 - acc: 0.9749 - val_loss: 2.4075 - val_acc: 0.7309\n",
            "Epoch 454/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0714 - acc: 0.9766 - val_loss: 2.0087 - val_acc: 0.7572\n",
            "Epoch 455/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0539 - acc: 0.9816 - val_loss: 2.0044 - val_acc: 0.7549\n",
            "Epoch 456/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0545 - acc: 0.9814 - val_loss: 2.0732 - val_acc: 0.7550\n",
            "Epoch 457/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0512 - acc: 0.9828 - val_loss: 2.1335 - val_acc: 0.7423\n",
            "Epoch 458/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0607 - acc: 0.9801 - val_loss: 1.9325 - val_acc: 0.7558\n",
            "Epoch 459/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0572 - acc: 0.9802 - val_loss: 1.9332 - val_acc: 0.7657\n",
            "Epoch 460/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0491 - acc: 0.9837 - val_loss: 1.8955 - val_acc: 0.7754\n",
            "Epoch 461/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0535 - acc: 0.9824 - val_loss: 2.4570 - val_acc: 0.7219\n",
            "Epoch 462/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0636 - acc: 0.9788 - val_loss: 1.9833 - val_acc: 0.7678\n",
            "Epoch 463/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0764 - acc: 0.9748 - val_loss: 2.2037 - val_acc: 0.7457\n",
            "Epoch 464/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0701 - acc: 0.9756 - val_loss: 2.1116 - val_acc: 0.7439\n",
            "Epoch 465/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0735 - acc: 0.9743 - val_loss: 2.0796 - val_acc: 0.7591\n",
            "Epoch 466/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0683 - acc: 0.9773 - val_loss: 2.0402 - val_acc: 0.7580\n",
            "Epoch 467/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0635 - acc: 0.9791 - val_loss: 2.1040 - val_acc: 0.7494\n",
            "Epoch 468/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0681 - acc: 0.9770 - val_loss: 2.1500 - val_acc: 0.7496\n",
            "Epoch 469/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0566 - acc: 0.9809 - val_loss: 1.9631 - val_acc: 0.7622\n",
            "Epoch 470/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0492 - acc: 0.9835 - val_loss: 2.0019 - val_acc: 0.7612\n",
            "Epoch 471/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0472 - acc: 0.9846 - val_loss: 2.0215 - val_acc: 0.7635\n",
            "Epoch 472/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0600 - acc: 0.9796 - val_loss: 2.1185 - val_acc: 0.7534\n",
            "Epoch 473/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0586 - acc: 0.9793 - val_loss: 1.9937 - val_acc: 0.7601\n",
            "Epoch 474/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0737 - acc: 0.9753 - val_loss: 2.1766 - val_acc: 0.7531\n",
            "Epoch 475/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0954 - acc: 0.9688 - val_loss: 2.3207 - val_acc: 0.7412\n",
            "Epoch 476/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1067 - acc: 0.9648 - val_loss: 2.3137 - val_acc: 0.7387\n",
            "Epoch 477/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0796 - acc: 0.9736 - val_loss: 2.0479 - val_acc: 0.7597\n",
            "Epoch 478/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0685 - acc: 0.9772 - val_loss: 2.1563 - val_acc: 0.7523\n",
            "Epoch 479/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0634 - acc: 0.9787 - val_loss: 1.9909 - val_acc: 0.7594\n",
            "Epoch 480/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0496 - acc: 0.9838 - val_loss: 1.9685 - val_acc: 0.7609\n",
            "Epoch 481/500\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.0567 - acc: 0.9813 - val_loss: 1.9748 - val_acc: 0.7649\n",
            "Epoch 482/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0477 - acc: 0.9840 - val_loss: 1.9380 - val_acc: 0.7626\n",
            "Epoch 483/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0447 - acc: 0.9853 - val_loss: 1.9501 - val_acc: 0.7666\n",
            "Epoch 484/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0451 - acc: 0.9849 - val_loss: 2.0056 - val_acc: 0.7649\n",
            "Epoch 485/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0362 - acc: 0.9883 - val_loss: 1.9656 - val_acc: 0.7656\n",
            "Epoch 486/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0353 - acc: 0.9888 - val_loss: 2.0016 - val_acc: 0.7596\n",
            "Epoch 487/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0402 - acc: 0.9868 - val_loss: 1.9635 - val_acc: 0.7618\n",
            "Epoch 488/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0495 - acc: 0.9835 - val_loss: 2.0424 - val_acc: 0.7604\n",
            "Epoch 489/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0471 - acc: 0.9849 - val_loss: 2.0687 - val_acc: 0.7618\n",
            "Epoch 490/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0557 - acc: 0.9816 - val_loss: 2.1345 - val_acc: 0.7536\n",
            "Epoch 491/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0586 - acc: 0.9806 - val_loss: 2.0665 - val_acc: 0.7584\n",
            "Epoch 492/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0642 - acc: 0.9789 - val_loss: 2.0508 - val_acc: 0.7578\n",
            "Epoch 493/500\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0513 - acc: 0.9820 - val_loss: 2.0998 - val_acc: 0.7551\n",
            "Epoch 494/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0472 - acc: 0.9845 - val_loss: 2.1199 - val_acc: 0.7558\n",
            "Epoch 495/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0701 - acc: 0.9766 - val_loss: 2.1539 - val_acc: 0.7525\n",
            "Epoch 496/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0757 - acc: 0.9740 - val_loss: 2.2135 - val_acc: 0.7476\n",
            "Epoch 497/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0935 - acc: 0.9696 - val_loss: 2.1207 - val_acc: 0.7573\n",
            "Epoch 498/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0801 - acc: 0.9725 - val_loss: 2.0378 - val_acc: 0.7659\n",
            "Epoch 499/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0678 - acc: 0.9768 - val_loss: 2.1946 - val_acc: 0.7580\n",
            "Epoch 500/500\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0632 - acc: 0.9784 - val_loss: 2.0190 - val_acc: 0.7608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkGqxXbSxo-k",
        "colab_type": "code",
        "outputId": "7fb340e5-0586-4a07-d7b8-4183fd64f409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 0s 59us/step\n",
            "Test accuracy:  0.7567857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "861vjsvytGrV",
        "colab_type": "code",
        "outputId": "6d4b5637-fe28-4125-ef8e-49415e93fb33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "NN4_val_loss = history.history['val_loss']\n",
        "NN4_train_loss = history.history['loss']\n",
        "NN4_val_acc = history.history['val_acc']\n",
        "NN4_train_acc = history.history['acc']\n",
        "\n",
        "\n",
        "epochs = range(1,501)\n",
        "\n",
        "\n",
        "plt.plot(epochs, NN4_val_loss, 'b--', label='Validation Loss')\n",
        "plt.plot(epochs, NN4_train_loss, 'bo', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe2f967bef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcJMUsZpxLZJ",
        "colab_type": "text"
      },
      "source": [
        "Overfitting is seen from around 60th epoch as can be seen from both the acc and loss comparisons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QvEWqOKwo1I",
        "colab_type": "code",
        "outputId": "4ccdcc12-d4e1-4cc8-8fe5-b61acf5bd6ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "\n",
        "plt.plot(epochs, NN4_val_acc, 'b+', label='Validation acc')\n",
        "plt.plot(epochs, NN4_train_acc, 'bo', label='Training acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe2f9710400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_IHSmyJxyqM",
        "colab_type": "text"
      },
      "source": [
        "Add Drop out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-GTmhl0xwGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.adam(lr = 0.01)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKuAiCqWyEq6",
        "colab_type": "code",
        "outputId": "1c743b9d-8e69-430b-a5f9-274aad9cfbb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=2048 ,epochs = 300, verbose = 1,  validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/300\n",
            "33600/33600 [==============================] - 2s 46us/step - loss: 2.4303 - acc: 0.1153 - val_loss: 2.3520 - val_acc: 0.1224\n",
            "Epoch 2/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 2.1989 - acc: 0.1809 - val_loss: 2.1269 - val_acc: 0.2316\n",
            "Epoch 3/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.9677 - acc: 0.2745 - val_loss: 2.1835 - val_acc: 0.2514\n",
            "Epoch 4/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.7996 - acc: 0.3439 - val_loss: 2.0222 - val_acc: 0.2803\n",
            "Epoch 5/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.6676 - acc: 0.3995 - val_loss: 1.7606 - val_acc: 0.3867\n",
            "Epoch 6/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.5750 - acc: 0.4393 - val_loss: 1.5608 - val_acc: 0.4584\n",
            "Epoch 7/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.5040 - acc: 0.4737 - val_loss: 1.7311 - val_acc: 0.4272\n",
            "Epoch 8/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.4593 - acc: 0.4981 - val_loss: 1.7471 - val_acc: 0.3881\n",
            "Epoch 9/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.3946 - acc: 0.5284 - val_loss: 1.3932 - val_acc: 0.5437\n",
            "Epoch 10/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.3537 - acc: 0.5477 - val_loss: 1.7665 - val_acc: 0.3879\n",
            "Epoch 11/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.3190 - acc: 0.5583 - val_loss: 1.3916 - val_acc: 0.5418\n",
            "Epoch 12/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.2895 - acc: 0.5717 - val_loss: 1.5040 - val_acc: 0.4798\n",
            "Epoch 13/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.2471 - acc: 0.5891 - val_loss: 1.3787 - val_acc: 0.5340\n",
            "Epoch 14/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.2302 - acc: 0.5963 - val_loss: 1.3150 - val_acc: 0.5753\n",
            "Epoch 15/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.2039 - acc: 0.6062 - val_loss: 1.3349 - val_acc: 0.5551\n",
            "Epoch 16/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.1763 - acc: 0.6176 - val_loss: 1.4190 - val_acc: 0.5353\n",
            "Epoch 17/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.1675 - acc: 0.6224 - val_loss: 1.2218 - val_acc: 0.6211\n",
            "Epoch 18/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.1555 - acc: 0.6276 - val_loss: 1.3509 - val_acc: 0.5549\n",
            "Epoch 19/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.1334 - acc: 0.6339 - val_loss: 1.2490 - val_acc: 0.5898\n",
            "Epoch 20/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.1154 - acc: 0.6410 - val_loss: 1.1384 - val_acc: 0.6425\n",
            "Epoch 21/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.1030 - acc: 0.6465 - val_loss: 1.1611 - val_acc: 0.6304\n",
            "Epoch 22/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.0858 - acc: 0.6544 - val_loss: 1.1265 - val_acc: 0.6301\n",
            "Epoch 23/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.0788 - acc: 0.6602 - val_loss: 1.0051 - val_acc: 0.6721\n",
            "Epoch 24/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.0575 - acc: 0.6644 - val_loss: 1.0387 - val_acc: 0.6690\n",
            "Epoch 25/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.0459 - acc: 0.6686 - val_loss: 1.2037 - val_acc: 0.6051\n",
            "Epoch 26/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.0444 - acc: 0.6702 - val_loss: 1.1984 - val_acc: 0.6144\n",
            "Epoch 27/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.0342 - acc: 0.6732 - val_loss: 1.0183 - val_acc: 0.6801\n",
            "Epoch 28/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.0268 - acc: 0.6753 - val_loss: 1.1534 - val_acc: 0.6240\n",
            "Epoch 29/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.0401 - acc: 0.6738 - val_loss: 1.2756 - val_acc: 0.5766\n",
            "Epoch 30/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.0229 - acc: 0.6771 - val_loss: 0.9565 - val_acc: 0.6984\n",
            "Epoch 31/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.0121 - acc: 0.6832 - val_loss: 1.0505 - val_acc: 0.6646\n",
            "Epoch 32/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9907 - acc: 0.6907 - val_loss: 1.6485 - val_acc: 0.4972\n",
            "Epoch 33/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.0079 - acc: 0.6843 - val_loss: 1.1055 - val_acc: 0.6433\n",
            "Epoch 34/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9823 - acc: 0.6929 - val_loss: 0.9306 - val_acc: 0.7041\n",
            "Epoch 35/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9790 - acc: 0.6931 - val_loss: 1.0051 - val_acc: 0.6828\n",
            "Epoch 36/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9841 - acc: 0.6906 - val_loss: 0.9844 - val_acc: 0.6811\n",
            "Epoch 37/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9787 - acc: 0.6932 - val_loss: 0.9030 - val_acc: 0.7159\n",
            "Epoch 38/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9649 - acc: 0.7003 - val_loss: 0.9264 - val_acc: 0.7096\n",
            "Epoch 39/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9716 - acc: 0.6973 - val_loss: 1.0563 - val_acc: 0.6548\n",
            "Epoch 40/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9665 - acc: 0.6994 - val_loss: 0.8754 - val_acc: 0.7252\n",
            "Epoch 41/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9534 - acc: 0.7026 - val_loss: 1.2391 - val_acc: 0.5868\n",
            "Epoch 42/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9516 - acc: 0.7041 - val_loss: 1.2070 - val_acc: 0.6290\n",
            "Epoch 43/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.9678 - acc: 0.6975 - val_loss: 1.0117 - val_acc: 0.6758\n",
            "Epoch 44/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9514 - acc: 0.7030 - val_loss: 0.9181 - val_acc: 0.7067\n",
            "Epoch 45/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9384 - acc: 0.7088 - val_loss: 0.9235 - val_acc: 0.7052\n",
            "Epoch 46/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.9423 - acc: 0.7064 - val_loss: 0.8553 - val_acc: 0.7345\n",
            "Epoch 47/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9376 - acc: 0.7076 - val_loss: 0.8369 - val_acc: 0.7393\n",
            "Epoch 48/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9228 - acc: 0.7132 - val_loss: 1.1320 - val_acc: 0.6478\n",
            "Epoch 49/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9342 - acc: 0.7100 - val_loss: 0.9302 - val_acc: 0.7060\n",
            "Epoch 50/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.9338 - acc: 0.7088 - val_loss: 1.2303 - val_acc: 0.6156\n",
            "Epoch 51/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.9354 - acc: 0.7093 - val_loss: 0.9614 - val_acc: 0.6951\n",
            "Epoch 52/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.9198 - acc: 0.7161 - val_loss: 0.8903 - val_acc: 0.7177\n",
            "Epoch 53/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9160 - acc: 0.7148 - val_loss: 0.8744 - val_acc: 0.7211\n",
            "Epoch 54/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9137 - acc: 0.7176 - val_loss: 0.9860 - val_acc: 0.6932\n",
            "Epoch 55/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.9048 - acc: 0.7199 - val_loss: 0.8626 - val_acc: 0.7283\n",
            "Epoch 56/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8984 - acc: 0.7216 - val_loss: 0.8715 - val_acc: 0.7227\n",
            "Epoch 57/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9073 - acc: 0.7182 - val_loss: 0.8640 - val_acc: 0.7241\n",
            "Epoch 58/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8961 - acc: 0.7213 - val_loss: 0.8747 - val_acc: 0.7208\n",
            "Epoch 59/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9000 - acc: 0.7192 - val_loss: 0.9527 - val_acc: 0.6955\n",
            "Epoch 60/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9047 - acc: 0.7183 - val_loss: 0.8803 - val_acc: 0.7208\n",
            "Epoch 61/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8961 - acc: 0.7220 - val_loss: 0.8641 - val_acc: 0.7247\n",
            "Epoch 62/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8992 - acc: 0.7205 - val_loss: 0.8382 - val_acc: 0.7383\n",
            "Epoch 63/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8898 - acc: 0.7230 - val_loss: 0.8720 - val_acc: 0.7250\n",
            "Epoch 64/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8966 - acc: 0.7218 - val_loss: 0.9481 - val_acc: 0.7026\n",
            "Epoch 65/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9001 - acc: 0.7215 - val_loss: 0.8543 - val_acc: 0.7366\n",
            "Epoch 66/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8720 - acc: 0.7269 - val_loss: 0.9386 - val_acc: 0.6978\n",
            "Epoch 67/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8765 - acc: 0.7263 - val_loss: 0.8551 - val_acc: 0.7312\n",
            "Epoch 68/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8796 - acc: 0.7282 - val_loss: 0.8937 - val_acc: 0.7183\n",
            "Epoch 69/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8799 - acc: 0.7273 - val_loss: 0.9062 - val_acc: 0.7159\n",
            "Epoch 70/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8788 - acc: 0.7238 - val_loss: 0.8323 - val_acc: 0.7388\n",
            "Epoch 71/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8810 - acc: 0.7258 - val_loss: 1.0496 - val_acc: 0.6738\n",
            "Epoch 72/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8691 - acc: 0.7303 - val_loss: 0.8132 - val_acc: 0.7488\n",
            "Epoch 73/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8743 - acc: 0.7307 - val_loss: 0.9140 - val_acc: 0.7080\n",
            "Epoch 74/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8686 - acc: 0.7296 - val_loss: 0.8309 - val_acc: 0.7393\n",
            "Epoch 75/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8748 - acc: 0.7276 - val_loss: 0.9157 - val_acc: 0.7071\n",
            "Epoch 76/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8721 - acc: 0.7291 - val_loss: 0.8688 - val_acc: 0.7267\n",
            "Epoch 77/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8669 - acc: 0.7318 - val_loss: 1.0027 - val_acc: 0.6851\n",
            "Epoch 78/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8625 - acc: 0.7284 - val_loss: 0.8532 - val_acc: 0.7250\n",
            "Epoch 79/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8708 - acc: 0.7310 - val_loss: 0.8373 - val_acc: 0.7392\n",
            "Epoch 80/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8615 - acc: 0.7339 - val_loss: 0.7423 - val_acc: 0.7720\n",
            "Epoch 81/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8510 - acc: 0.7364 - val_loss: 0.7896 - val_acc: 0.7544\n",
            "Epoch 82/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8552 - acc: 0.7354 - val_loss: 0.8656 - val_acc: 0.7314\n",
            "Epoch 83/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8598 - acc: 0.7343 - val_loss: 0.8575 - val_acc: 0.7230\n",
            "Epoch 84/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8617 - acc: 0.7332 - val_loss: 0.8903 - val_acc: 0.7168\n",
            "Epoch 85/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8634 - acc: 0.7334 - val_loss: 1.0557 - val_acc: 0.6726\n",
            "Epoch 86/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8569 - acc: 0.7332 - val_loss: 0.9520 - val_acc: 0.6978\n",
            "Epoch 87/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8530 - acc: 0.7343 - val_loss: 0.9372 - val_acc: 0.7113\n",
            "Epoch 88/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8499 - acc: 0.7380 - val_loss: 0.7920 - val_acc: 0.7509\n",
            "Epoch 89/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8427 - acc: 0.7374 - val_loss: 0.8545 - val_acc: 0.7249\n",
            "Epoch 90/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8498 - acc: 0.7388 - val_loss: 0.8021 - val_acc: 0.7490\n",
            "Epoch 91/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8550 - acc: 0.7358 - val_loss: 0.8172 - val_acc: 0.7482\n",
            "Epoch 92/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8587 - acc: 0.7330 - val_loss: 0.8010 - val_acc: 0.7539\n",
            "Epoch 93/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8413 - acc: 0.7400 - val_loss: 0.8993 - val_acc: 0.7133\n",
            "Epoch 94/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8452 - acc: 0.7365 - val_loss: 0.7746 - val_acc: 0.7629\n",
            "Epoch 95/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8391 - acc: 0.7379 - val_loss: 0.8141 - val_acc: 0.7433\n",
            "Epoch 96/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8458 - acc: 0.7383 - val_loss: 0.8514 - val_acc: 0.7375\n",
            "Epoch 97/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8342 - acc: 0.7389 - val_loss: 0.8264 - val_acc: 0.7342\n",
            "Epoch 98/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8340 - acc: 0.7396 - val_loss: 0.8834 - val_acc: 0.7271\n",
            "Epoch 99/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8550 - acc: 0.7353 - val_loss: 0.8184 - val_acc: 0.7418\n",
            "Epoch 100/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8335 - acc: 0.7439 - val_loss: 0.7602 - val_acc: 0.7628\n",
            "Epoch 101/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8271 - acc: 0.7423 - val_loss: 0.8099 - val_acc: 0.7463\n",
            "Epoch 102/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8387 - acc: 0.7423 - val_loss: 0.8269 - val_acc: 0.7424\n",
            "Epoch 103/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8275 - acc: 0.7428 - val_loss: 0.7633 - val_acc: 0.7596\n",
            "Epoch 104/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8269 - acc: 0.7404 - val_loss: 0.8819 - val_acc: 0.7189\n",
            "Epoch 105/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8355 - acc: 0.7412 - val_loss: 0.7799 - val_acc: 0.7604\n",
            "Epoch 106/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8294 - acc: 0.7428 - val_loss: 0.9064 - val_acc: 0.7196\n",
            "Epoch 107/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8475 - acc: 0.7371 - val_loss: 0.8633 - val_acc: 0.7320\n",
            "Epoch 108/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8343 - acc: 0.7403 - val_loss: 0.7966 - val_acc: 0.7493\n",
            "Epoch 109/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8255 - acc: 0.7438 - val_loss: 0.9240 - val_acc: 0.7011\n",
            "Epoch 110/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8320 - acc: 0.7447 - val_loss: 0.8491 - val_acc: 0.7299\n",
            "Epoch 111/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8240 - acc: 0.7487 - val_loss: 0.8344 - val_acc: 0.7441\n",
            "Epoch 112/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8246 - acc: 0.7463 - val_loss: 0.8388 - val_acc: 0.7383\n",
            "Epoch 113/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8228 - acc: 0.7466 - val_loss: 0.9910 - val_acc: 0.6858\n",
            "Epoch 114/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8239 - acc: 0.7432 - val_loss: 0.8648 - val_acc: 0.7339\n",
            "Epoch 115/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8296 - acc: 0.7438 - val_loss: 0.8292 - val_acc: 0.7451\n",
            "Epoch 116/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8212 - acc: 0.7468 - val_loss: 0.7688 - val_acc: 0.7587\n",
            "Epoch 117/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8237 - acc: 0.7463 - val_loss: 0.9187 - val_acc: 0.7142\n",
            "Epoch 118/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8213 - acc: 0.7457 - val_loss: 0.8016 - val_acc: 0.7488\n",
            "Epoch 119/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8315 - acc: 0.7398 - val_loss: 0.8552 - val_acc: 0.7371\n",
            "Epoch 120/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8220 - acc: 0.7451 - val_loss: 0.8313 - val_acc: 0.7378\n",
            "Epoch 121/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8183 - acc: 0.7474 - val_loss: 0.8194 - val_acc: 0.7394\n",
            "Epoch 122/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8156 - acc: 0.7475 - val_loss: 0.9332 - val_acc: 0.7072\n",
            "Epoch 123/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8121 - acc: 0.7484 - val_loss: 0.7724 - val_acc: 0.7568\n",
            "Epoch 124/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8166 - acc: 0.7512 - val_loss: 0.9444 - val_acc: 0.6963\n",
            "Epoch 125/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8215 - acc: 0.7446 - val_loss: 0.7599 - val_acc: 0.7644\n",
            "Epoch 126/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8115 - acc: 0.7503 - val_loss: 0.8188 - val_acc: 0.7511\n",
            "Epoch 127/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8284 - acc: 0.7427 - val_loss: 0.7250 - val_acc: 0.7752\n",
            "Epoch 128/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8098 - acc: 0.7508 - val_loss: 0.8075 - val_acc: 0.7472\n",
            "Epoch 129/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8213 - acc: 0.7480 - val_loss: 0.7712 - val_acc: 0.7598\n",
            "Epoch 130/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8029 - acc: 0.7526 - val_loss: 0.7834 - val_acc: 0.7545\n",
            "Epoch 131/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8095 - acc: 0.7479 - val_loss: 0.7577 - val_acc: 0.7633\n",
            "Epoch 132/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8005 - acc: 0.7521 - val_loss: 0.8612 - val_acc: 0.7332\n",
            "Epoch 133/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8169 - acc: 0.7465 - val_loss: 0.8867 - val_acc: 0.7157\n",
            "Epoch 134/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8032 - acc: 0.7543 - val_loss: 0.7883 - val_acc: 0.7538\n",
            "Epoch 135/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7985 - acc: 0.7523 - val_loss: 0.8977 - val_acc: 0.7191\n",
            "Epoch 136/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8032 - acc: 0.7525 - val_loss: 0.8091 - val_acc: 0.7421\n",
            "Epoch 137/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7971 - acc: 0.7553 - val_loss: 0.8200 - val_acc: 0.7417\n",
            "Epoch 138/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7975 - acc: 0.7555 - val_loss: 0.8309 - val_acc: 0.7329\n",
            "Epoch 139/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8014 - acc: 0.7554 - val_loss: 0.7975 - val_acc: 0.7481\n",
            "Epoch 140/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8004 - acc: 0.7540 - val_loss: 0.7722 - val_acc: 0.7601\n",
            "Epoch 141/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8015 - acc: 0.7535 - val_loss: 0.8374 - val_acc: 0.7382\n",
            "Epoch 142/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8029 - acc: 0.7540 - val_loss: 0.9152 - val_acc: 0.7157\n",
            "Epoch 143/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8085 - acc: 0.7511 - val_loss: 0.8142 - val_acc: 0.7419\n",
            "Epoch 144/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8069 - acc: 0.7532 - val_loss: 0.7758 - val_acc: 0.7602\n",
            "Epoch 145/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8011 - acc: 0.7535 - val_loss: 0.7712 - val_acc: 0.7599\n",
            "Epoch 146/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8035 - acc: 0.7502 - val_loss: 0.8933 - val_acc: 0.7243\n",
            "Epoch 147/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7955 - acc: 0.7542 - val_loss: 0.7852 - val_acc: 0.7536\n",
            "Epoch 148/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7936 - acc: 0.7542 - val_loss: 0.8401 - val_acc: 0.7333\n",
            "Epoch 149/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7925 - acc: 0.7565 - val_loss: 0.8401 - val_acc: 0.7371\n",
            "Epoch 150/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8001 - acc: 0.7541 - val_loss: 0.8064 - val_acc: 0.7454\n",
            "Epoch 151/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7949 - acc: 0.7522 - val_loss: 0.7755 - val_acc: 0.7550\n",
            "Epoch 152/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7923 - acc: 0.7566 - val_loss: 0.8903 - val_acc: 0.7301\n",
            "Epoch 153/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7990 - acc: 0.7560 - val_loss: 0.7552 - val_acc: 0.7625\n",
            "Epoch 154/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7963 - acc: 0.7528 - val_loss: 0.7411 - val_acc: 0.7693\n",
            "Epoch 155/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7941 - acc: 0.7546 - val_loss: 0.7665 - val_acc: 0.7624\n",
            "Epoch 156/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7867 - acc: 0.7572 - val_loss: 0.8561 - val_acc: 0.7392\n",
            "Epoch 157/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7902 - acc: 0.7568 - val_loss: 0.7767 - val_acc: 0.7555\n",
            "Epoch 158/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7844 - acc: 0.7583 - val_loss: 0.7692 - val_acc: 0.7607\n",
            "Epoch 159/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7896 - acc: 0.7578 - val_loss: 0.7981 - val_acc: 0.7542\n",
            "Epoch 160/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7923 - acc: 0.7545 - val_loss: 0.8488 - val_acc: 0.7267\n",
            "Epoch 161/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7970 - acc: 0.7542 - val_loss: 0.8314 - val_acc: 0.7378\n",
            "Epoch 162/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7919 - acc: 0.7579 - val_loss: 1.0025 - val_acc: 0.6929\n",
            "Epoch 163/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7965 - acc: 0.7547 - val_loss: 0.8300 - val_acc: 0.7394\n",
            "Epoch 164/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7825 - acc: 0.7582 - val_loss: 0.7943 - val_acc: 0.7485\n",
            "Epoch 165/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7766 - acc: 0.7607 - val_loss: 0.8390 - val_acc: 0.7469\n",
            "Epoch 166/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7782 - acc: 0.7624 - val_loss: 0.7831 - val_acc: 0.7541\n",
            "Epoch 167/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7826 - acc: 0.7590 - val_loss: 0.7941 - val_acc: 0.7533\n",
            "Epoch 168/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7819 - acc: 0.7582 - val_loss: 0.7887 - val_acc: 0.7492\n",
            "Epoch 169/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7933 - acc: 0.7534 - val_loss: 0.7774 - val_acc: 0.7603\n",
            "Epoch 170/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7854 - acc: 0.7594 - val_loss: 0.7398 - val_acc: 0.7713\n",
            "Epoch 171/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7860 - acc: 0.7600 - val_loss: 0.7334 - val_acc: 0.7707\n",
            "Epoch 172/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7822 - acc: 0.7593 - val_loss: 0.7991 - val_acc: 0.7495\n",
            "Epoch 173/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7771 - acc: 0.7603 - val_loss: 0.8057 - val_acc: 0.7519\n",
            "Epoch 174/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7783 - acc: 0.7626 - val_loss: 0.8303 - val_acc: 0.7411\n",
            "Epoch 175/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7863 - acc: 0.7571 - val_loss: 0.7582 - val_acc: 0.7611\n",
            "Epoch 176/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7837 - acc: 0.7582 - val_loss: 0.8049 - val_acc: 0.7451\n",
            "Epoch 177/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7872 - acc: 0.7568 - val_loss: 0.7224 - val_acc: 0.7748\n",
            "Epoch 178/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7834 - acc: 0.7569 - val_loss: 0.8129 - val_acc: 0.7473\n",
            "Epoch 179/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7811 - acc: 0.7614 - val_loss: 0.7906 - val_acc: 0.7566\n",
            "Epoch 180/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7861 - acc: 0.7579 - val_loss: 0.8255 - val_acc: 0.7454\n",
            "Epoch 181/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7739 - acc: 0.7626 - val_loss: 0.7628 - val_acc: 0.7638\n",
            "Epoch 182/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7681 - acc: 0.7633 - val_loss: 0.8044 - val_acc: 0.7450\n",
            "Epoch 183/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7711 - acc: 0.7644 - val_loss: 0.9178 - val_acc: 0.7158\n",
            "Epoch 184/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7800 - acc: 0.7593 - val_loss: 0.7262 - val_acc: 0.7744\n",
            "Epoch 185/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7722 - acc: 0.7618 - val_loss: 0.7190 - val_acc: 0.7756\n",
            "Epoch 186/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7785 - acc: 0.7610 - val_loss: 0.7300 - val_acc: 0.7734\n",
            "Epoch 187/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7719 - acc: 0.7651 - val_loss: 0.8177 - val_acc: 0.7501\n",
            "Epoch 188/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7730 - acc: 0.7611 - val_loss: 0.7592 - val_acc: 0.7589\n",
            "Epoch 189/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7669 - acc: 0.7625 - val_loss: 0.7848 - val_acc: 0.7543\n",
            "Epoch 190/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7715 - acc: 0.7610 - val_loss: 0.7996 - val_acc: 0.7502\n",
            "Epoch 191/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7755 - acc: 0.7613 - val_loss: 0.8283 - val_acc: 0.7377\n",
            "Epoch 192/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7760 - acc: 0.7604 - val_loss: 0.7478 - val_acc: 0.7704\n",
            "Epoch 193/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7751 - acc: 0.7617 - val_loss: 0.7417 - val_acc: 0.7708\n",
            "Epoch 194/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7674 - acc: 0.7635 - val_loss: 0.7293 - val_acc: 0.7782\n",
            "Epoch 195/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7648 - acc: 0.7650 - val_loss: 0.7677 - val_acc: 0.7659\n",
            "Epoch 196/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7668 - acc: 0.7656 - val_loss: 0.7342 - val_acc: 0.7741\n",
            "Epoch 197/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7703 - acc: 0.7625 - val_loss: 0.7780 - val_acc: 0.7548\n",
            "Epoch 198/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7813 - acc: 0.7614 - val_loss: 0.7277 - val_acc: 0.7780\n",
            "Epoch 199/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7677 - acc: 0.7638 - val_loss: 0.7844 - val_acc: 0.7562\n",
            "Epoch 200/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7740 - acc: 0.7638 - val_loss: 0.8072 - val_acc: 0.7508\n",
            "Epoch 201/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7701 - acc: 0.7627 - val_loss: 0.8137 - val_acc: 0.7493\n",
            "Epoch 202/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7830 - acc: 0.7593 - val_loss: 0.7673 - val_acc: 0.7624\n",
            "Epoch 203/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7711 - acc: 0.7626 - val_loss: 0.7300 - val_acc: 0.7757\n",
            "Epoch 204/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7750 - acc: 0.7627 - val_loss: 0.8265 - val_acc: 0.7433\n",
            "Epoch 205/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7714 - acc: 0.7655 - val_loss: 0.7737 - val_acc: 0.7559\n",
            "Epoch 206/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7692 - acc: 0.7637 - val_loss: 0.7605 - val_acc: 0.7622\n",
            "Epoch 207/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7710 - acc: 0.7639 - val_loss: 0.7132 - val_acc: 0.7804\n",
            "Epoch 208/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7734 - acc: 0.7628 - val_loss: 0.7552 - val_acc: 0.7671\n",
            "Epoch 209/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7787 - acc: 0.7605 - val_loss: 0.8258 - val_acc: 0.7433\n",
            "Epoch 210/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7769 - acc: 0.7626 - val_loss: 0.7750 - val_acc: 0.7617\n",
            "Epoch 211/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7662 - acc: 0.7644 - val_loss: 0.8063 - val_acc: 0.7502\n",
            "Epoch 212/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7627 - acc: 0.7690 - val_loss: 0.9120 - val_acc: 0.7175\n",
            "Epoch 213/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7615 - acc: 0.7657 - val_loss: 0.7466 - val_acc: 0.7691\n",
            "Epoch 214/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7714 - acc: 0.7623 - val_loss: 0.7803 - val_acc: 0.7591\n",
            "Epoch 215/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7651 - acc: 0.7641 - val_loss: 0.7866 - val_acc: 0.7563\n",
            "Epoch 216/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7617 - acc: 0.7664 - val_loss: 0.6988 - val_acc: 0.7860\n",
            "Epoch 217/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7682 - acc: 0.7646 - val_loss: 0.7002 - val_acc: 0.7859\n",
            "Epoch 218/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7667 - acc: 0.7643 - val_loss: 0.7170 - val_acc: 0.7807\n",
            "Epoch 219/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7591 - acc: 0.7657 - val_loss: 0.7044 - val_acc: 0.7843\n",
            "Epoch 220/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7572 - acc: 0.7665 - val_loss: 0.7213 - val_acc: 0.7773\n",
            "Epoch 221/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7603 - acc: 0.7657 - val_loss: 0.7555 - val_acc: 0.7628\n",
            "Epoch 222/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7587 - acc: 0.7656 - val_loss: 0.7248 - val_acc: 0.7779\n",
            "Epoch 223/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7539 - acc: 0.7679 - val_loss: 0.7029 - val_acc: 0.7828\n",
            "Epoch 224/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7556 - acc: 0.7663 - val_loss: 0.7842 - val_acc: 0.7624\n",
            "Epoch 225/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7634 - acc: 0.7619 - val_loss: 0.7810 - val_acc: 0.7578\n",
            "Epoch 226/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7567 - acc: 0.7663 - val_loss: 0.7483 - val_acc: 0.7667\n",
            "Epoch 227/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7621 - acc: 0.7657 - val_loss: 0.8281 - val_acc: 0.7457\n",
            "Epoch 228/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7608 - acc: 0.7638 - val_loss: 0.8309 - val_acc: 0.7432\n",
            "Epoch 229/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7591 - acc: 0.7666 - val_loss: 0.7834 - val_acc: 0.7562\n",
            "Epoch 230/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7679 - acc: 0.7648 - val_loss: 0.7031 - val_acc: 0.7820\n",
            "Epoch 231/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7649 - acc: 0.7639 - val_loss: 0.7878 - val_acc: 0.7537\n",
            "Epoch 232/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7589 - acc: 0.7688 - val_loss: 0.7748 - val_acc: 0.7595\n",
            "Epoch 233/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7577 - acc: 0.7660 - val_loss: 0.6991 - val_acc: 0.7846\n",
            "Epoch 234/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7618 - acc: 0.7658 - val_loss: 0.7997 - val_acc: 0.7509\n",
            "Epoch 235/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7610 - acc: 0.7658 - val_loss: 0.7108 - val_acc: 0.7802\n",
            "Epoch 236/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7557 - acc: 0.7651 - val_loss: 0.7687 - val_acc: 0.7611\n",
            "Epoch 237/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7526 - acc: 0.7672 - val_loss: 0.7835 - val_acc: 0.7536\n",
            "Epoch 238/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7618 - acc: 0.7647 - val_loss: 0.7047 - val_acc: 0.7813\n",
            "Epoch 239/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7586 - acc: 0.7664 - val_loss: 0.7603 - val_acc: 0.7655\n",
            "Epoch 240/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7603 - acc: 0.7666 - val_loss: 0.7844 - val_acc: 0.7612\n",
            "Epoch 241/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7639 - acc: 0.7643 - val_loss: 0.9268 - val_acc: 0.7119\n",
            "Epoch 242/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7636 - acc: 0.7643 - val_loss: 0.9402 - val_acc: 0.7090\n",
            "Epoch 243/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7630 - acc: 0.7654 - val_loss: 0.7628 - val_acc: 0.7637\n",
            "Epoch 244/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7618 - acc: 0.7670 - val_loss: 0.7214 - val_acc: 0.7762\n",
            "Epoch 245/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7561 - acc: 0.7689 - val_loss: 0.7993 - val_acc: 0.7529\n",
            "Epoch 246/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7603 - acc: 0.7643 - val_loss: 0.7690 - val_acc: 0.7584\n",
            "Epoch 247/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7585 - acc: 0.7668 - val_loss: 0.7178 - val_acc: 0.7779\n",
            "Epoch 248/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7456 - acc: 0.7687 - val_loss: 0.6844 - val_acc: 0.7893\n",
            "Epoch 249/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7570 - acc: 0.7658 - val_loss: 0.7580 - val_acc: 0.7659\n",
            "Epoch 250/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7501 - acc: 0.7703 - val_loss: 0.7650 - val_acc: 0.7613\n",
            "Epoch 251/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7561 - acc: 0.7672 - val_loss: 0.7631 - val_acc: 0.7644\n",
            "Epoch 252/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7582 - acc: 0.7665 - val_loss: 0.7681 - val_acc: 0.7621\n",
            "Epoch 253/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7440 - acc: 0.7729 - val_loss: 0.6926 - val_acc: 0.7839\n",
            "Epoch 254/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7541 - acc: 0.7687 - val_loss: 0.7459 - val_acc: 0.7710\n",
            "Epoch 255/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7463 - acc: 0.7715 - val_loss: 0.7265 - val_acc: 0.7737\n",
            "Epoch 256/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7382 - acc: 0.7768 - val_loss: 0.8500 - val_acc: 0.7444\n",
            "Epoch 257/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7588 - acc: 0.7653 - val_loss: 0.9331 - val_acc: 0.7141\n",
            "Epoch 258/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7482 - acc: 0.7696 - val_loss: 0.7292 - val_acc: 0.7764\n",
            "Epoch 259/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7512 - acc: 0.7698 - val_loss: 0.7518 - val_acc: 0.7614\n",
            "Epoch 260/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7634 - acc: 0.7635 - val_loss: 0.8148 - val_acc: 0.7503\n",
            "Epoch 261/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7557 - acc: 0.7676 - val_loss: 0.7121 - val_acc: 0.7794\n",
            "Epoch 262/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7443 - acc: 0.7704 - val_loss: 0.6659 - val_acc: 0.7964\n",
            "Epoch 263/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7448 - acc: 0.7707 - val_loss: 0.6886 - val_acc: 0.7902\n",
            "Epoch 264/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7559 - acc: 0.7655 - val_loss: 0.8249 - val_acc: 0.7414\n",
            "Epoch 265/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7567 - acc: 0.7683 - val_loss: 0.7382 - val_acc: 0.7671\n",
            "Epoch 266/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7508 - acc: 0.7692 - val_loss: 0.6981 - val_acc: 0.7836\n",
            "Epoch 267/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7507 - acc: 0.7649 - val_loss: 0.7425 - val_acc: 0.7669\n",
            "Epoch 268/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7531 - acc: 0.7704 - val_loss: 0.7081 - val_acc: 0.7826\n",
            "Epoch 269/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7403 - acc: 0.7726 - val_loss: 0.7379 - val_acc: 0.7703\n",
            "Epoch 270/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7421 - acc: 0.7722 - val_loss: 0.8750 - val_acc: 0.7263\n",
            "Epoch 271/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7436 - acc: 0.7691 - val_loss: 0.7195 - val_acc: 0.7833\n",
            "Epoch 272/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7531 - acc: 0.7687 - val_loss: 0.7249 - val_acc: 0.7726\n",
            "Epoch 273/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7458 - acc: 0.7718 - val_loss: 0.7152 - val_acc: 0.7776\n",
            "Epoch 274/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7376 - acc: 0.7752 - val_loss: 0.7633 - val_acc: 0.7690\n",
            "Epoch 275/300\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7478 - acc: 0.7716 - val_loss: 0.6863 - val_acc: 0.7913\n",
            "Epoch 276/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7518 - acc: 0.7677 - val_loss: 0.7848 - val_acc: 0.7615\n",
            "Epoch 277/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7502 - acc: 0.7685 - val_loss: 0.6892 - val_acc: 0.7892\n",
            "Epoch 278/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7464 - acc: 0.7726 - val_loss: 0.7523 - val_acc: 0.7675\n",
            "Epoch 279/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7483 - acc: 0.7700 - val_loss: 0.7897 - val_acc: 0.7558\n",
            "Epoch 280/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7368 - acc: 0.7732 - val_loss: 0.7096 - val_acc: 0.7791\n",
            "Epoch 281/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7405 - acc: 0.7714 - val_loss: 0.7538 - val_acc: 0.7693\n",
            "Epoch 282/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7437 - acc: 0.7717 - val_loss: 0.7546 - val_acc: 0.7646\n",
            "Epoch 283/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7505 - acc: 0.7684 - val_loss: 0.6990 - val_acc: 0.7842\n",
            "Epoch 284/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7445 - acc: 0.7719 - val_loss: 0.6797 - val_acc: 0.7918\n",
            "Epoch 285/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7425 - acc: 0.7713 - val_loss: 0.7283 - val_acc: 0.7751\n",
            "Epoch 286/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7430 - acc: 0.7735 - val_loss: 0.7162 - val_acc: 0.7785\n",
            "Epoch 287/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7520 - acc: 0.7684 - val_loss: 0.7737 - val_acc: 0.7650\n",
            "Epoch 288/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7452 - acc: 0.7701 - val_loss: 0.6829 - val_acc: 0.7918\n",
            "Epoch 289/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7393 - acc: 0.7740 - val_loss: 0.7358 - val_acc: 0.7720\n",
            "Epoch 290/300\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7551 - acc: 0.7666 - val_loss: 0.7621 - val_acc: 0.7593\n",
            "Epoch 291/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7490 - acc: 0.7690 - val_loss: 0.7990 - val_acc: 0.7484\n",
            "Epoch 292/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7455 - acc: 0.7713 - val_loss: 0.7055 - val_acc: 0.7826\n",
            "Epoch 293/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7435 - acc: 0.7730 - val_loss: 0.7473 - val_acc: 0.7756\n",
            "Epoch 294/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7439 - acc: 0.7724 - val_loss: 0.7176 - val_acc: 0.7804\n",
            "Epoch 295/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7420 - acc: 0.7702 - val_loss: 0.7510 - val_acc: 0.7678\n",
            "Epoch 296/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7359 - acc: 0.7736 - val_loss: 0.7462 - val_acc: 0.7632\n",
            "Epoch 297/300\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7455 - acc: 0.7679 - val_loss: 0.7019 - val_acc: 0.7856\n",
            "Epoch 298/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7361 - acc: 0.7755 - val_loss: 0.7675 - val_acc: 0.7617\n",
            "Epoch 299/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7307 - acc: 0.7762 - val_loss: 0.6957 - val_acc: 0.7860\n",
            "Epoch 300/300\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7454 - acc: 0.7712 - val_loss: 0.8627 - val_acc: 0.7342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laf6ZFoVy6yp",
        "colab_type": "code",
        "outputId": "c118abb3-6934-4024-9f08-b5ebbb439fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 0s 58us/step\n",
            "Test accuracy:  0.7908333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur4ScqUHye7q",
        "colab_type": "code",
        "outputId": "8f6ce0d4-93aa-4690-cc0a-f8f4182eab98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "NN5_val_loss = history.history['val_loss']\n",
        "NN5_train_loss = history.history['loss']\n",
        "NN5_val_acc = history.history['val_acc']\n",
        "NN5_train_acc = history.history['acc']\n",
        "\n",
        "\n",
        "epochs = range(1,301)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, NN5_val_loss, 'b', label='Validation Loss')\n",
        "plt.plot(epochs, NN5_train_loss, 'r', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, NN5_val_acc, 'b', label='Validation Acc')\n",
        "plt.plot(epochs, NN5_train_acc, 'r', label='Training Acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe2f716add8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAE+CAYAAADI27ezAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dJVJrbF0C0J",
        "colab_type": "text"
      },
      "source": [
        "Overfitting is reduced to a great extent. However the validation graphs are a little fluctuating and there is probably opportunity to improve the test (unseen) accuracy score from 79% to higher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--zRjOgv0c99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN5_test_acc= 0.7908"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtLjL69ggXA9",
        "colab_type": "text"
      },
      "source": [
        "We need to try hyperparameter optimization to arrive at the best values and then use it to ge the best results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bExetwGPmewz",
        "colab_type": "text"
      },
      "source": [
        "# Let's start with two hidden layers and increased neurons in the hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8do1ukYxgL9W",
        "colab": {}
      },
      "source": [
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(128, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.adam(lr = 0.0001)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reMvIDcgd9ZU",
        "colab_type": "code",
        "outputId": "2b738d28-044d-4fcb-8032-448b72749059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=2048 ,epochs = 100, verbose = 1,  validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/100\n",
            "33600/33600 [==============================] - 1s 40us/step - loss: 2.6808 - acc: 0.1090 - val_loss: 2.5072 - val_acc: 0.1206\n",
            "Epoch 2/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 2.5197 - acc: 0.1298 - val_loss: 2.3907 - val_acc: 0.1419\n",
            "Epoch 3/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 2.4094 - acc: 0.1512 - val_loss: 2.2887 - val_acc: 0.1609\n",
            "Epoch 4/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 2.3005 - acc: 0.1810 - val_loss: 2.1828 - val_acc: 0.2101\n",
            "Epoch 5/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 2.2134 - acc: 0.2153 - val_loss: 2.1261 - val_acc: 0.2526\n",
            "Epoch 6/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 2.1287 - acc: 0.2473 - val_loss: 2.0989 - val_acc: 0.2534\n",
            "Epoch 7/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 2.0387 - acc: 0.2884 - val_loss: 2.0203 - val_acc: 0.2967\n",
            "Epoch 8/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.9633 - acc: 0.3195 - val_loss: 1.9863 - val_acc: 0.3254\n",
            "Epoch 9/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.8907 - acc: 0.3518 - val_loss: 1.8848 - val_acc: 0.3699\n",
            "Epoch 10/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.8277 - acc: 0.3779 - val_loss: 1.8296 - val_acc: 0.3918\n",
            "Epoch 11/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.7691 - acc: 0.4061 - val_loss: 1.7538 - val_acc: 0.4308\n",
            "Epoch 12/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.7160 - acc: 0.4292 - val_loss: 1.7252 - val_acc: 0.4430\n",
            "Epoch 13/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.6709 - acc: 0.4480 - val_loss: 1.6423 - val_acc: 0.4800\n",
            "Epoch 14/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.6237 - acc: 0.4701 - val_loss: 1.5944 - val_acc: 0.5170\n",
            "Epoch 15/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.5882 - acc: 0.4884 - val_loss: 1.5421 - val_acc: 0.5369\n",
            "Epoch 16/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.5467 - acc: 0.5041 - val_loss: 1.4790 - val_acc: 0.5677\n",
            "Epoch 17/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.5202 - acc: 0.5179 - val_loss: 1.4521 - val_acc: 0.5813\n",
            "Epoch 18/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.4822 - acc: 0.5321 - val_loss: 1.3975 - val_acc: 0.6085\n",
            "Epoch 19/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.4467 - acc: 0.5476 - val_loss: 1.3617 - val_acc: 0.6191\n",
            "Epoch 20/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.4266 - acc: 0.5543 - val_loss: 1.3795 - val_acc: 0.5998\n",
            "Epoch 21/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.4065 - acc: 0.5603 - val_loss: 1.3154 - val_acc: 0.6333\n",
            "Epoch 22/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.3694 - acc: 0.5799 - val_loss: 1.2683 - val_acc: 0.6579\n",
            "Epoch 23/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.3529 - acc: 0.5846 - val_loss: 1.2729 - val_acc: 0.6420\n",
            "Epoch 24/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.3243 - acc: 0.5957 - val_loss: 1.2275 - val_acc: 0.6658\n",
            "Epoch 25/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.3095 - acc: 0.6019 - val_loss: 1.2113 - val_acc: 0.6744\n",
            "Epoch 26/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.2879 - acc: 0.6088 - val_loss: 1.1953 - val_acc: 0.6754\n",
            "Epoch 27/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.2679 - acc: 0.6156 - val_loss: 1.1759 - val_acc: 0.6821\n",
            "Epoch 28/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.2544 - acc: 0.6209 - val_loss: 1.1629 - val_acc: 0.6873\n",
            "Epoch 29/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.2366 - acc: 0.6250 - val_loss: 1.1281 - val_acc: 0.6951\n",
            "Epoch 30/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.2203 - acc: 0.6337 - val_loss: 1.1156 - val_acc: 0.7014\n",
            "Epoch 31/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.2052 - acc: 0.6372 - val_loss: 1.0995 - val_acc: 0.7007\n",
            "Epoch 32/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.1919 - acc: 0.6424 - val_loss: 1.0885 - val_acc: 0.7082\n",
            "Epoch 33/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.1762 - acc: 0.6487 - val_loss: 1.0805 - val_acc: 0.6991\n",
            "Epoch 34/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.1663 - acc: 0.6507 - val_loss: 1.0644 - val_acc: 0.7099\n",
            "Epoch 35/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.1536 - acc: 0.6533 - val_loss: 1.0642 - val_acc: 0.7041\n",
            "Epoch 36/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.1354 - acc: 0.6600 - val_loss: 1.0303 - val_acc: 0.7222\n",
            "Epoch 37/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.1264 - acc: 0.6610 - val_loss: 1.0274 - val_acc: 0.7214\n",
            "Epoch 38/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.1172 - acc: 0.6676 - val_loss: 1.0145 - val_acc: 0.7217\n",
            "Epoch 39/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.1072 - acc: 0.6686 - val_loss: 1.0106 - val_acc: 0.7204\n",
            "Epoch 40/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.0947 - acc: 0.6691 - val_loss: 1.0168 - val_acc: 0.7168\n",
            "Epoch 41/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.0903 - acc: 0.6729 - val_loss: 0.9924 - val_acc: 0.7257\n",
            "Epoch 42/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.0740 - acc: 0.6740 - val_loss: 0.9827 - val_acc: 0.7259\n",
            "Epoch 43/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.0651 - acc: 0.6804 - val_loss: 0.9701 - val_acc: 0.7311\n",
            "Epoch 44/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.0545 - acc: 0.6832 - val_loss: 0.9637 - val_acc: 0.7334\n",
            "Epoch 45/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.0444 - acc: 0.6887 - val_loss: 0.9548 - val_acc: 0.7355\n",
            "Epoch 46/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.0394 - acc: 0.6880 - val_loss: 0.9449 - val_acc: 0.7358\n",
            "Epoch 47/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.0310 - acc: 0.6900 - val_loss: 0.9461 - val_acc: 0.7363\n",
            "Epoch 48/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.0224 - acc: 0.6922 - val_loss: 0.9329 - val_acc: 0.7374\n",
            "Epoch 49/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.0136 - acc: 0.6957 - val_loss: 0.9142 - val_acc: 0.7429\n",
            "Epoch 50/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.0012 - acc: 0.7001 - val_loss: 0.9197 - val_acc: 0.7416\n",
            "Epoch 51/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9982 - acc: 0.6987 - val_loss: 0.9261 - val_acc: 0.7404\n",
            "Epoch 52/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9927 - acc: 0.7007 - val_loss: 0.9172 - val_acc: 0.7373\n",
            "Epoch 53/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9854 - acc: 0.7040 - val_loss: 0.8859 - val_acc: 0.7487\n",
            "Epoch 54/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.9762 - acc: 0.7049 - val_loss: 0.8890 - val_acc: 0.7493\n",
            "Epoch 55/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9691 - acc: 0.7076 - val_loss: 0.8829 - val_acc: 0.7517\n",
            "Epoch 56/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9599 - acc: 0.7126 - val_loss: 0.8766 - val_acc: 0.7525\n",
            "Epoch 57/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9527 - acc: 0.7111 - val_loss: 0.8775 - val_acc: 0.7524\n",
            "Epoch 58/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9487 - acc: 0.7128 - val_loss: 0.8782 - val_acc: 0.7524\n",
            "Epoch 59/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9388 - acc: 0.7166 - val_loss: 0.8685 - val_acc: 0.7537\n",
            "Epoch 60/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.9381 - acc: 0.7182 - val_loss: 0.8488 - val_acc: 0.7584\n",
            "Epoch 61/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9296 - acc: 0.7191 - val_loss: 0.8457 - val_acc: 0.7578\n",
            "Epoch 62/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9271 - acc: 0.7229 - val_loss: 0.8421 - val_acc: 0.7606\n",
            "Epoch 63/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9219 - acc: 0.7196 - val_loss: 0.8422 - val_acc: 0.7563\n",
            "Epoch 64/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.9107 - acc: 0.7225 - val_loss: 0.8381 - val_acc: 0.7594\n",
            "Epoch 65/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.9031 - acc: 0.7273 - val_loss: 0.8322 - val_acc: 0.7604\n",
            "Epoch 66/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8977 - acc: 0.7290 - val_loss: 0.8318 - val_acc: 0.7564\n",
            "Epoch 67/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8953 - acc: 0.7290 - val_loss: 0.8139 - val_acc: 0.7664\n",
            "Epoch 68/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8873 - acc: 0.7326 - val_loss: 0.8255 - val_acc: 0.7657\n",
            "Epoch 69/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8836 - acc: 0.7329 - val_loss: 0.8176 - val_acc: 0.7635\n",
            "Epoch 70/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8794 - acc: 0.7335 - val_loss: 0.8011 - val_acc: 0.7709\n",
            "Epoch 71/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8697 - acc: 0.7362 - val_loss: 0.7911 - val_acc: 0.7737\n",
            "Epoch 72/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8673 - acc: 0.7367 - val_loss: 0.7834 - val_acc: 0.7752\n",
            "Epoch 73/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8595 - acc: 0.7398 - val_loss: 0.7857 - val_acc: 0.7746\n",
            "Epoch 74/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8554 - acc: 0.7410 - val_loss: 0.7888 - val_acc: 0.7738\n",
            "Epoch 75/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8516 - acc: 0.7426 - val_loss: 0.7782 - val_acc: 0.7746\n",
            "Epoch 76/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8502 - acc: 0.7413 - val_loss: 0.7739 - val_acc: 0.7779\n",
            "Epoch 77/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8428 - acc: 0.7423 - val_loss: 0.7653 - val_acc: 0.7797\n",
            "Epoch 78/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8411 - acc: 0.7418 - val_loss: 0.7624 - val_acc: 0.7786\n",
            "Epoch 79/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8339 - acc: 0.7459 - val_loss: 0.7640 - val_acc: 0.7779\n",
            "Epoch 80/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8285 - acc: 0.7479 - val_loss: 0.7565 - val_acc: 0.7848\n",
            "Epoch 81/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8261 - acc: 0.7521 - val_loss: 0.7597 - val_acc: 0.7816\n",
            "Epoch 82/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8180 - acc: 0.7507 - val_loss: 0.7567 - val_acc: 0.7760\n",
            "Epoch 83/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8206 - acc: 0.7513 - val_loss: 0.7491 - val_acc: 0.7809\n",
            "Epoch 84/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8078 - acc: 0.7543 - val_loss: 0.7328 - val_acc: 0.7873\n",
            "Epoch 85/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8044 - acc: 0.7576 - val_loss: 0.7393 - val_acc: 0.7895\n",
            "Epoch 86/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8069 - acc: 0.7557 - val_loss: 0.7308 - val_acc: 0.7864\n",
            "Epoch 87/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8009 - acc: 0.7552 - val_loss: 0.7339 - val_acc: 0.7864\n",
            "Epoch 88/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7973 - acc: 0.7566 - val_loss: 0.7214 - val_acc: 0.7913\n",
            "Epoch 89/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7959 - acc: 0.7572 - val_loss: 0.7340 - val_acc: 0.7876\n",
            "Epoch 90/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7882 - acc: 0.7595 - val_loss: 0.7191 - val_acc: 0.7924\n",
            "Epoch 91/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7857 - acc: 0.7611 - val_loss: 0.7390 - val_acc: 0.7839\n",
            "Epoch 92/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7827 - acc: 0.7624 - val_loss: 0.7160 - val_acc: 0.7905\n",
            "Epoch 93/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7775 - acc: 0.7608 - val_loss: 0.7093 - val_acc: 0.7919\n",
            "Epoch 94/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7784 - acc: 0.7611 - val_loss: 0.7064 - val_acc: 0.7966\n",
            "Epoch 95/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7680 - acc: 0.7648 - val_loss: 0.7016 - val_acc: 0.7968\n",
            "Epoch 96/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7696 - acc: 0.7671 - val_loss: 0.7036 - val_acc: 0.7958\n",
            "Epoch 97/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7666 - acc: 0.7650 - val_loss: 0.7037 - val_acc: 0.7951\n",
            "Epoch 98/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7626 - acc: 0.7665 - val_loss: 0.7016 - val_acc: 0.7964\n",
            "Epoch 99/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7574 - acc: 0.7680 - val_loss: 0.7043 - val_acc: 0.7949\n",
            "Epoch 100/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7570 - acc: 0.7682 - val_loss: 0.7069 - val_acc: 0.7893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEnOXiISd9gF",
        "colab_type": "code",
        "outputId": "293256c4-501a-4e58-93f2-9b004e42e4f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 0s 50us/step\n",
            "Test accuracy:  0.7975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5JMwVs40zdq",
        "colab_type": "code",
        "outputId": "0b416435-d11f-4540-e040-67c6d3500320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "NN6_test_acc=0.7975\n",
        "NN6_val_loss = history.history['val_loss']\n",
        "NN6_train_loss = history.history['loss']\n",
        "NN6_val_acc = history.history['val_acc']\n",
        "NN6_train_acc = history.history['acc']\n",
        "\n",
        "\n",
        "epochs = range(1,101)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, NN6_val_loss, 'b', label='Validation Loss')\n",
        "plt.plot(epochs, NN6_train_loss, 'r', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, NN6_val_acc, 'b', label='Validation Acc')\n",
        "plt.plot(epochs, NN6_train_acc, 'r', label='Training Acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe2f6c95f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAE9CAYAAABOT8UdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZowxxhgTQP4fw3GluAfDHnMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCu_xpyE1MyL",
        "colab_type": "text"
      },
      "source": [
        "Not much overfitting but not much improvement in the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n51DrlVd9BZ",
        "colab_type": "code",
        "outputId": "44f93855-de8e-4028-9dc5-4a34ba89a8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's reduce the drop out to 0.1\n",
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(128, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.adam(lr = 0.001)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=2048 ,epochs = 100, verbose = 1, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/100\n",
            "33600/33600 [==============================] - 1s 44us/step - loss: 2.3131 - acc: 0.1964 - val_loss: 2.3298 - val_acc: 0.1952\n",
            "Epoch 2/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.7488 - acc: 0.4233 - val_loss: 2.0509 - val_acc: 0.2911\n",
            "Epoch 3/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.4178 - acc: 0.5682 - val_loss: 1.6302 - val_acc: 0.4433\n",
            "Epoch 4/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.2146 - acc: 0.6398 - val_loss: 1.5582 - val_acc: 0.4898\n",
            "Epoch 5/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.0960 - acc: 0.6751 - val_loss: 1.5217 - val_acc: 0.4915\n",
            "Epoch 6/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.0138 - acc: 0.6971 - val_loss: 1.3505 - val_acc: 0.5610\n",
            "Epoch 7/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.9558 - acc: 0.7142 - val_loss: 1.2464 - val_acc: 0.6129\n",
            "Epoch 8/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9006 - acc: 0.7321 - val_loss: 1.1081 - val_acc: 0.6514\n",
            "Epoch 9/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8467 - acc: 0.7451 - val_loss: 1.0774 - val_acc: 0.6732\n",
            "Epoch 10/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8067 - acc: 0.7568 - val_loss: 1.1020 - val_acc: 0.6548\n",
            "Epoch 11/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7722 - acc: 0.7649 - val_loss: 1.1009 - val_acc: 0.6622\n",
            "Epoch 12/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7457 - acc: 0.7717 - val_loss: 1.0102 - val_acc: 0.6854\n",
            "Epoch 13/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7264 - acc: 0.7782 - val_loss: 1.3561 - val_acc: 0.5912\n",
            "Epoch 14/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7040 - acc: 0.7861 - val_loss: 1.1050 - val_acc: 0.6492\n",
            "Epoch 15/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.6774 - acc: 0.7915 - val_loss: 1.1518 - val_acc: 0.6456\n",
            "Epoch 16/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.6676 - acc: 0.7904 - val_loss: 1.0270 - val_acc: 0.6804\n",
            "Epoch 17/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.6460 - acc: 0.8008 - val_loss: 0.9583 - val_acc: 0.6896\n",
            "Epoch 18/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.6343 - acc: 0.8027 - val_loss: 0.8160 - val_acc: 0.7495\n",
            "Epoch 19/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.6181 - acc: 0.8076 - val_loss: 0.9332 - val_acc: 0.7041\n",
            "Epoch 20/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.6014 - acc: 0.8121 - val_loss: 0.7902 - val_acc: 0.7571\n",
            "Epoch 21/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5776 - acc: 0.8232 - val_loss: 0.7909 - val_acc: 0.7576\n",
            "Epoch 22/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5748 - acc: 0.8235 - val_loss: 0.8556 - val_acc: 0.7309\n",
            "Epoch 23/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5669 - acc: 0.8236 - val_loss: 0.9751 - val_acc: 0.7028\n",
            "Epoch 24/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5529 - acc: 0.8272 - val_loss: 0.8602 - val_acc: 0.7304\n",
            "Epoch 25/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5504 - acc: 0.8302 - val_loss: 0.8404 - val_acc: 0.7361\n",
            "Epoch 26/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5388 - acc: 0.8325 - val_loss: 0.7323 - val_acc: 0.7803\n",
            "Epoch 27/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5314 - acc: 0.8341 - val_loss: 0.7941 - val_acc: 0.7572\n",
            "Epoch 28/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5172 - acc: 0.8371 - val_loss: 0.8261 - val_acc: 0.7466\n",
            "Epoch 29/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5087 - acc: 0.8396 - val_loss: 0.7835 - val_acc: 0.7689\n",
            "Epoch 30/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5112 - acc: 0.8401 - val_loss: 0.7536 - val_acc: 0.7591\n",
            "Epoch 31/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4985 - acc: 0.8430 - val_loss: 0.8119 - val_acc: 0.7561\n",
            "Epoch 32/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4892 - acc: 0.8468 - val_loss: 0.7791 - val_acc: 0.7652\n",
            "Epoch 33/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4820 - acc: 0.8477 - val_loss: 0.8373 - val_acc: 0.7543\n",
            "Epoch 34/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4824 - acc: 0.8476 - val_loss: 0.7677 - val_acc: 0.7666\n",
            "Epoch 35/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4790 - acc: 0.8497 - val_loss: 0.7933 - val_acc: 0.7600\n",
            "Epoch 36/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4773 - acc: 0.8500 - val_loss: 0.9715 - val_acc: 0.7126\n",
            "Epoch 37/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4730 - acc: 0.8525 - val_loss: 0.8411 - val_acc: 0.7492\n",
            "Epoch 38/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4653 - acc: 0.8536 - val_loss: 0.9405 - val_acc: 0.7159\n",
            "Epoch 39/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4588 - acc: 0.8569 - val_loss: 0.7014 - val_acc: 0.7888\n",
            "Epoch 40/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4579 - acc: 0.8560 - val_loss: 0.6793 - val_acc: 0.7923\n",
            "Epoch 41/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4498 - acc: 0.8589 - val_loss: 0.7481 - val_acc: 0.7732\n",
            "Epoch 42/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4427 - acc: 0.8603 - val_loss: 0.9558 - val_acc: 0.7236\n",
            "Epoch 43/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4485 - acc: 0.8586 - val_loss: 1.1285 - val_acc: 0.6938\n",
            "Epoch 44/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4399 - acc: 0.8612 - val_loss: 0.6659 - val_acc: 0.7999\n",
            "Epoch 45/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4355 - acc: 0.8637 - val_loss: 0.8461 - val_acc: 0.7558\n",
            "Epoch 46/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4301 - acc: 0.8629 - val_loss: 0.8045 - val_acc: 0.7578\n",
            "Epoch 47/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4222 - acc: 0.8659 - val_loss: 0.9395 - val_acc: 0.7319\n",
            "Epoch 48/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4268 - acc: 0.8650 - val_loss: 0.7221 - val_acc: 0.7808\n",
            "Epoch 49/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4210 - acc: 0.8686 - val_loss: 0.7875 - val_acc: 0.7631\n",
            "Epoch 50/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4111 - acc: 0.8697 - val_loss: 0.6577 - val_acc: 0.8017\n",
            "Epoch 51/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4138 - acc: 0.8696 - val_loss: 0.7327 - val_acc: 0.7808\n",
            "Epoch 52/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4085 - acc: 0.8717 - val_loss: 0.7938 - val_acc: 0.7615\n",
            "Epoch 53/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4076 - acc: 0.8701 - val_loss: 0.7433 - val_acc: 0.7749\n",
            "Epoch 54/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4049 - acc: 0.8732 - val_loss: 0.7187 - val_acc: 0.7851\n",
            "Epoch 55/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3952 - acc: 0.8757 - val_loss: 0.8012 - val_acc: 0.7678\n",
            "Epoch 56/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3976 - acc: 0.8722 - val_loss: 0.9942 - val_acc: 0.7284\n",
            "Epoch 57/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3985 - acc: 0.8722 - val_loss: 0.8696 - val_acc: 0.7359\n",
            "Epoch 58/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3896 - acc: 0.8777 - val_loss: 0.6961 - val_acc: 0.7906\n",
            "Epoch 59/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3904 - acc: 0.8749 - val_loss: 0.7237 - val_acc: 0.7831\n",
            "Epoch 60/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3781 - acc: 0.8815 - val_loss: 0.6738 - val_acc: 0.7987\n",
            "Epoch 61/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3784 - acc: 0.8810 - val_loss: 0.8192 - val_acc: 0.7581\n",
            "Epoch 62/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3897 - acc: 0.8753 - val_loss: 0.9244 - val_acc: 0.7262\n",
            "Epoch 63/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3736 - acc: 0.8808 - val_loss: 0.7948 - val_acc: 0.7717\n",
            "Epoch 64/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3720 - acc: 0.8809 - val_loss: 0.7710 - val_acc: 0.7737\n",
            "Epoch 65/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3768 - acc: 0.8799 - val_loss: 0.7818 - val_acc: 0.7698\n",
            "Epoch 66/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3783 - acc: 0.8793 - val_loss: 0.6333 - val_acc: 0.8102\n",
            "Epoch 67/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3660 - acc: 0.8829 - val_loss: 0.8886 - val_acc: 0.7496\n",
            "Epoch 68/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3649 - acc: 0.8825 - val_loss: 0.8050 - val_acc: 0.7566\n",
            "Epoch 69/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3667 - acc: 0.8806 - val_loss: 0.7753 - val_acc: 0.7746\n",
            "Epoch 70/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3574 - acc: 0.8875 - val_loss: 0.6733 - val_acc: 0.7995\n",
            "Epoch 71/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3589 - acc: 0.8856 - val_loss: 0.7559 - val_acc: 0.7816\n",
            "Epoch 72/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3553 - acc: 0.8868 - val_loss: 0.7592 - val_acc: 0.7789\n",
            "Epoch 73/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3583 - acc: 0.8847 - val_loss: 0.6505 - val_acc: 0.8036\n",
            "Epoch 74/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3557 - acc: 0.8868 - val_loss: 0.8259 - val_acc: 0.7746\n",
            "Epoch 75/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3553 - acc: 0.8863 - val_loss: 0.7646 - val_acc: 0.7885\n",
            "Epoch 76/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3446 - acc: 0.8907 - val_loss: 0.6935 - val_acc: 0.7905\n",
            "Epoch 77/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3424 - acc: 0.8919 - val_loss: 0.6763 - val_acc: 0.7964\n",
            "Epoch 78/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3377 - acc: 0.8921 - val_loss: 0.8059 - val_acc: 0.7685\n",
            "Epoch 79/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3438 - acc: 0.8890 - val_loss: 0.8027 - val_acc: 0.7720\n",
            "Epoch 80/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3388 - acc: 0.8929 - val_loss: 0.7426 - val_acc: 0.7910\n",
            "Epoch 81/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3404 - acc: 0.8923 - val_loss: 0.6389 - val_acc: 0.8082\n",
            "Epoch 82/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3336 - acc: 0.8929 - val_loss: 0.6855 - val_acc: 0.8033\n",
            "Epoch 83/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3358 - acc: 0.8919 - val_loss: 0.7334 - val_acc: 0.7914\n",
            "Epoch 84/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3326 - acc: 0.8923 - val_loss: 0.7364 - val_acc: 0.7899\n",
            "Epoch 85/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3311 - acc: 0.8946 - val_loss: 0.7538 - val_acc: 0.7909\n",
            "Epoch 86/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3310 - acc: 0.8942 - val_loss: 0.6887 - val_acc: 0.8047\n",
            "Epoch 87/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3284 - acc: 0.8932 - val_loss: 0.8698 - val_acc: 0.7591\n",
            "Epoch 88/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3332 - acc: 0.8929 - val_loss: 0.7970 - val_acc: 0.7764\n",
            "Epoch 89/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3259 - acc: 0.8946 - val_loss: 0.7216 - val_acc: 0.7881\n",
            "Epoch 90/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3314 - acc: 0.8946 - val_loss: 0.6985 - val_acc: 0.8001\n",
            "Epoch 91/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3265 - acc: 0.8949 - val_loss: 0.8002 - val_acc: 0.7778\n",
            "Epoch 92/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3294 - acc: 0.8935 - val_loss: 0.9554 - val_acc: 0.7435\n",
            "Epoch 93/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3235 - acc: 0.8966 - val_loss: 0.7429 - val_acc: 0.7849\n",
            "Epoch 94/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3183 - acc: 0.8982 - val_loss: 0.7136 - val_acc: 0.7946\n",
            "Epoch 95/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3161 - acc: 0.8983 - val_loss: 0.6248 - val_acc: 0.8237\n",
            "Epoch 96/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3158 - acc: 0.8996 - val_loss: 0.7576 - val_acc: 0.7919\n",
            "Epoch 97/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3085 - acc: 0.9001 - val_loss: 0.6806 - val_acc: 0.8051\n",
            "Epoch 98/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3134 - acc: 0.8978 - val_loss: 0.8456 - val_acc: 0.7621\n",
            "Epoch 99/100\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3147 - acc: 0.8980 - val_loss: 0.6683 - val_acc: 0.8092\n",
            "Epoch 100/100\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3116 - acc: 0.8996 - val_loss: 0.6543 - val_acc: 0.8123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6QgcGSLpmsy",
        "colab_type": "code",
        "outputId": "7741a535-dfe8-4761-d651-f57e062dd550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "results = model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 0s 56us/step\n",
            "Test accuracy:  0.8073809523809524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp4PJ-My1XBO",
        "colab_type": "code",
        "outputId": "119a8c62-eabc-4142-8476-06dd54e55107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "NN6_test_acc=0.8073\n",
        "NN6_val_loss = history.history['val_loss']\n",
        "NN6_train_loss = history.history['loss']\n",
        "NN6_val_acc = history.history['val_acc']\n",
        "NN6_train_acc = history.history['acc']\n",
        "\n",
        "\n",
        "epochs = range(1,101)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, NN6_val_loss, 'b', label='Validation Loss')\n",
        "plt.plot(epochs, NN6_train_loss, 'r', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, NN6_val_acc, 'b', label='Validation Acc')\n",
        "plt.plot(epochs, NN6_train_acc, 'r', label='Training Acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe2f6834b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAE9CAYAAABOT8UdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydeXiU5fW/74csJGFJ2AQB2beEfRGV\nTRCVRYWqVKHgUmwpVqvWpdLWKnX51vbnvtYFVCyiVkRQQdRKizv7jpAACYSwBgiEPcnz++PMy0zC\nzGSSzJJkzn1dc70z73pmCO985nPOcx5jrUVRFEVRFEWpHNSIdACKoiiKoiiKGxVniqIoiqIolQgV\nZ4qiKIqiKJUIFWeKoiiKoiiVCBVniqIoiqIolQgVZ4qiKIqiKJWI2EgHEEwaNmxoW7VqFekwFEUJ\nE8uXL99vrW0U6TiCgd6/FCX68HUPq1birFWrVixbtizSYSiKEiaMMVmRjiFY6P1LUaIPX/cwTWsq\niqIoiqJUIlScKYqiKIqiVCJUnCmKoiiKolQiqlXNmaKUxunTp8nOzubEiRORDkUpAwkJCTRv3py4\nuLhIh6IoihJyVJwpUUV2djZ16tShVatWGGMiHY4SANZacnNzyc7OpnXr1pEOR1EUJeRoWlOJKk6c\nOEGDBg1UmFUhjDE0aNBA3U5FUaIGFWdK1KHCrOqh/2aKokQTKs4UJYwMGTKEhQsXFlv3zDPPcOut\nt/o9rnbt2gDk5OQwZswYr/sMHjy41D5ZzzzzDMeOHTvzeuTIkRw6dCiQ0P0ydepUnnjiiQqfp6pg\njBlujNlkjMkwxkzxsr2lMeY/xpg1xpj/GmOaRyJORVGqJirOFCWMjBs3jnfffbfYunfffZdx48YF\ndHzTpk354IMPyn39kuJs/vz5pKSklPt80YgxJgZ4ERgBpAHjjDFpJXZ7Aphhre0GPAz8LbxRKopS\nlYlKcbZoEZT4flSUsDBmzBg+/fRTTp06BUBmZiY5OTkMHDiQ/Px8hg4dSq9evejatStz58496/jM\nzEy6dOkCwPHjxxk7diypqalcffXVHD9+/Mx+t956K3369KFz58489NBDADz33HPk5OQwZMgQhgwZ\nAkhX+v379wPw1FNP0aVLF7p06cIzzzxz5nqpqan8+te/pnPnzlx++eXFrlMa3s559OhRrrjiCrp3\n706XLl147733AJgyZQppaWl069aNe++9t0yfa5jpC2RYa7daa08B7wKjS+yTBnzler7Iy3ZFUaoK\np07B2rWwcCHMnAmffSbrSpKTAytXBuWSUTlac9o0+P57GDs20pEo0Ub9+vXp27cvCxYsYPTo0bz7\n7rtcd911GGNISEhgzpw51K1bl/3793PhhRcyatQon/VWL7/8MklJSWzcuJE1a9bQq1evM9see+wx\n6tevT2FhIUOHDmXNmjXccccdPPXUUyxatIiGDRsWO9fy5ct54403+PHHH7HWcsEFF3DxxRdTr149\n0tPTmTVrFq+99hrXXXcds2fPZsKECaW+V1/n3Lp1K02bNuXTTz8FIC8vj9zcXObMmcNPP/2EMSYo\nqdYQ0gzY4fE6G7igxD6rgWuAZ4GrgTrGmAbW2lzPnYwxk4BJAC1atAhZwIqiBMC2beLefPcdZGaK\nADt0CH76CU6fLr5vvXowbBjEx8OxY7BiBWzdCj17yvMKEpXirNeJbynakwuMinQoSgS56y5YtSq4\n5+zRA1wGkU+c1KYjzqZNmwZIy4g//elPLF68mBo1arBz50727NlDkyZNvJ5n8eLF3HHHHQB069aN\nbt26ndn2/vvv8+qrr1JQUMCuXbvYsGFDse0l+eabb7j66qupVasWANdccw1ff/01o0aNonXr1vTo\n0QOA3r17k5mZGdBn4eucw4cP55577uH+++/nyiuvZODAgRQUFJCQkMAtt9zClVdeyZVXXhnQNSox\n9wIvGGNuBhYDO4HCkjtZa18FXgXo06ePDWeAilLlOXEC5s+HjAyoXx8aNoQ2baBdO0hKkn2OHoVv\nvhHBZC3ExMhxR45A7dpwwQVQty489RR8+KEcU68edOwIiYnQqhWMHAndu0OLFnKN9HR47z1YvBhq\n1BCB1r073H47DBoUlLcWleLs0k0vctXRpRQWjiImJtLRKNHG6NGj+f3vf8+KFSs4duwYvXv3BmDm\nzJns27eP5cuXExcXR6tWrcrVPmLbtm088cQTLF26lHr16nHzzTdXqA1FzZo1zzyPiYkpU1rTGx06\ndGDFihXMnz+fBx54gKFDh/Lggw+yZMkS/vOf//DBBx/wwgsv8NVXX5V+ssiwEzjP43Vz17ozWGtz\nEOcMY0xt4FprbaW2AxUl5BQUSHpw3ToROMeOiSPVvz9cc43s89JL8OqrUKsWnHMOnDwJe/fKtrZt\nRSAdPw7798OXX0JenvdrJSbKOfLyzna9nO0nTohgA0hOhj//GcaPF2FWw0/VV8eOEOIfkFEpzmrU\nSiSR4+TlidhWopPSHK5QUbt2bYYMGcLEiROLDQTIy8vjnHPOIS4ujkWLFpGVleX3PIMGDeKdd97h\nkksuYd26daxZswaAw4cPU6tWLZKTk9mzZw8LFixg8ODBANSpU4cjR46cldYcOHAgN998M1OmTMFa\ny5w5c3j77bcr9D59nTMnJ4f69eszYcIEUlJSeP3118nPz+fYsWOMHDmS/v3706ZNmwpdO8QsBdob\nY1ojomws8AvPHYwxDYED1toi4I/A9LBHqSiViZUr4eabwXWfokYNSEgAY+C558TxSkyE9evhoovE\n1dqxQ/Zp0QKKimTbwoXiiiUnw+jRIqYuvFBE2J49sGWLOGkHD4r4S06GIUPknPHxIhBr1oTYWHHP\nli+X64weLQ5aJSEqxVlsbRFnBw+qOFMiw7hx47j66quLjdwcP348V111FV27dqVPnz506tTJ7zlu\nvfVWfvnLX5KamkpqauoZB6579+707NmTTp06cd5559G/f/8zx0yaNInhw4fTtGlTFi1adGZ9r169\nuPnmm+nbty8Av/rVr+jZs2fAKUyARx999EzRP8hsDN7OuXDhQu677z5q1KhBXFwcL7/8MkeOHGH0\n6NGcOHECay1PPfVUwNcNN9baAmPM7cBCIAaYbq1db4x5GFhmrZ0HDAb+ZoyxSFrztogFrCjBpKhI\n0ncffQSXXQYDB8L27eKInXce/OxncO658Omn8PXXIqRiY2UUXsOGMH26iKl27SAuDgoLYd48eOIJ\nqe/68EM5R1l7G9atK9fv08f/fh6ZAOrUAdcP18qGsbb6lDn06dPHltbnCWDL1ffS5KOX2bDkKOef\nH4bAlErDxo0bSU1NjXQYSjnw9m9njFlurS3lblw1CPT+pShhYft2+OEHSQ3Wqwe9eomwufNOeP55\nuPhiEWQHDogL1r49ZGVJqtChY0cRc4cOwYgR8PTT6oiUwNc9LGTOmTHmPGAG0BiwwKvW2mdL7DMe\nuB8wwBHgVmvtate2TNe6QqAgmDfg+GSXc3bAui6tKIqiKNWEw4dhwgTYtAkaNRJh9fjj4mIVFopL\ntXq11GIlJIiI6tgRmjeHlBRpafD888XbRSQlQZcusGQJ/P738OSTIrwyMiTtmJgoxfeffy71YMOG\nyXqlXIQyrVkA3GOtXWGMqQMsN8Z8Ya3d4LHPNuBia+1BY8wIZNSS55D0Idba/cEOrGZKIjWwHNp7\nCqhZ6v6KoiiKUmlZuhT+8x8RZHXqwPDhsGyZ1FHl5sKLL4oL9q9/yTD1BQukxqtmTcjPl/WeGCP1\nYbfdJmJu1y4RXQsXwh13iDAzRkY+duzoPq5WLbj66rC+9epKyMSZtXYXsMv1/IgxZiPSH2iDxz7f\neRzyAzLqKeQkpCQAcGTfCVScKYqiKJWWoiJp2TB9OmRnw733SorQqcl6+2349a9lVOMDD0DTpiKm\n/v1vqd0C+PhjaezZsaPUf73yCkya5L7G0aPigO3aJUX1ffpA587F4xitfZTDSVgGBBhjWgE9gR/9\n7HYLsMDjtQU+dxXUvuLqBxQUEhskAnBk73EgOVinVRRFUZTyUVgoBfQZGdCvn7SNmDkT/vEPSU/W\nrSu1X1dcISMP27eXEYpz50pR+xNPSO+tuXPhgw+Ki6mrrhKB98AD8Ic/yOhFT2rVkj5d3buH9S0r\nvgm5OHP1+JkN3GWtPexjnyGIOBvgsXqAtXanMeYc4AtjzE/W2sVeji1zh+24OiLOjuVWrF+ToiiK\nopSJnBwZkbh+vfT6AmnxsGwZ7Nvn3i8uTmrCevaEGTPg2mvF9Zo2DV54Af73PynEv+suEXBxcdC7\ntzz3Ru/eks5UqgQhFWfGmDhEmM201n7oY59uwOvACM+pTay1O13LvcaYOch8dmeJs3J12E6QtObR\n3PI35lQURVEUvxw5Ap98InVgrVrB++9LvZbTf8tJMx46BJdcIgKsa1eZPmjVKml0etllxdtK3Hqr\nPJRqTShHaxpgGrDRWuu1aZExpgXwIXCDtXazx/paQA1XrVot4HLg4aAFlyjO2YmD6pwp4SU3N5eh\nQ4cCsHv3bmJiYmjUqBEAS5YsIT4+vtRz/PKXv2TKlCl09CzELcGLL75ISkoK48ePr3DMAwYM4IUX\nXjgzhZOiKF7YuFEK6w8ckFGK+/aJy3W4RMLo+uth6lQRZr56eZXS41Cp/oTSOesP3ACsNcY4Mxj+\nCWgBYK39J/Ag0AB4yTW5s9MyozEwx7UuFnjHWvtZ0CJzOWcqzpRw06BBA1a5JvScOnUqtWvX5t57\n7y22j7UWay01fEwf8sYbb5R6ndtu056nihJUioqk9mvZMhnlePHF0l7i3XdFhP34o4xeTEmREZIx\nMXDddfDb38rzrVtFdLmaRSuKP0I5WvMbSmkiZq39FfArL+u3AqGrTHQ5Z6cOa1pTqRxkZGQwatQo\nevbsycqVK/niiy/461//yooVKzh+/DjXX389Dz74IOB2srp06ULDhg2ZPHkyCxYsICkpiblz53LO\nOefwwAMP0LBhQ+666y4GDBjAgAED+Oqrr8jLy+ONN96gX79+HD16lBtvvJGNGzeSlpZGZmYmr7/+\nekAO2fHjx5k8eTIrVqwgLi6OZ555hkGDBrF27VomTpzI6dOnKSoq4qOPPqJRo0Zcd9115OTkUFhY\nyNSpUxkzZkyoP1JFKT87d8p8jq1aSdH9Sy/JaMnc3OL7xcdLL7C0NCnInzABGjd2zxmZ7DHg7KKL\nwvoWlKpNVE7fdEac5alzplQefvrpJ2bMmEEf1/Qjjz/+OPXr16egoIAhQ4YwZswY0tLSih2Tl5fH\nxRdfzOOPP87dd9/N9OnTmTJlylnnttayZMkS5s2bx8MPP8xnn33G888/T5MmTZg9ezarV6+mV69e\nAcf63HPPUbNmTdauXcv69esZOXIk6enpvPTSS9x7771cf/31nDx5Emstc+fOpVWrVixwFSPn+Zqo\nWFEizZYt8Le/wVtvyRyMDjEx0r/riivg/PNFfC1aJK0nrr8eLrigeIoyKSn8sSvViugUZ660ZmG+\nirOo5q67pOg2mPToUe4Z1du2bXtGmAHMmjWLadOmUVBQQE5ODhs2bDhLnCUmJjJixAgAevfuzddf\nf+313Ndcc82ZfZz5Mr/55hvuv/9+QObj7Fyyr5EfvvnmG+677z4AOnfuTNOmTcnIyKBfv348+uij\nZGVlcc0119CuXTu6devGlClTmDJlCldddVWxuT4VJaLs3Cl1YvPnw4YN0tm+Zk2YPFnSlllZ4oCN\nHy/zNnqic/8pISQ6xZnLOTOnTnDyZPF5UBUlUtSqVevM8/T0dJ599lmWLFlCSkoKEyZM4MSJs9Pw\nngMIYmJiKPD8te9BTdcfub99gsENN9zARRddxKeffsrw4cOZPn06gwYNYtmyZcyfP58pU6YwYsQI\n/vSnP4UsBkU5Q2GhzPUYGytpyo8/li73e/ZIof5PP4G1Ugf2s59JevK666BZs0hHrkQ5US3OEjnO\nwYPQpEmE41EiQzkdrnBw+PBh6tSpQ926ddm1axcLFy5k+PDhQb1G//79ef/99xk4cCBr165lw4YN\npR/kYuDAgcycOZNBgwaxceNGdu3aRbt27di6dSvt2rXjzjvvZNu2baxZs4a2bdvSsGFDbrjhBurU\nqcO/Sk4VoyjBZM0aePRRmTty2zZxvjxp21amLmrWDMaMgRtukIauilKJiE5x5kprJnCCAwdUnCmV\nj169epGWlkanTp1o2bJlSFKBv/vd77jxxhtJS0s780hO9j5jxrBhw4iLiwNEmE2fPp3f/OY3dO3a\nlbi4OGbMmEF8fDzvvPMOs2bNIi4ujqZNmzJ16lS+++47pkyZQo0aNYiPj+ef//xn0N+LorBzJzzy\nCLz2moyYvOQSqROrX18ctMREmYy7UyffLSwUpZJgrA2sb2tVoE+fPnbZsmWl73jsGNSqxf08zqhv\n7kdLYKKHjRs3kpqaGukwKgUFBQUUFBSQkJBAeno6l19+Oenp6cTGVs7fbN7+7Ywxy13td6o8Ad+/\nFGHLFtixQ+rE5syRBq8At98ODz4oUx0pSiXH1z2sct6FQ43LOUvkOAcORDgWRYkQ+fn5DB06lIKC\nAqy1vPLKK5VWmCkKIJ30Z8yQ0ZQrVrjX16kjouyOO6B168jFpyhBIjrvxDVqYOPiSTh9goMHIx2M\nokSGlJQUli9fHukwFMU/1krh/uuvw6uvQn4+9OoFTz8N3bpBgwZSR1a7dqQjVZSgEZ3iDLCJiSSe\nVudMURSl0nHsmIysfO89WLzY3XH/+uvh3ntlMnBFqcZErTgzSYkkHT5OtjpnUYe1FqMFwVWK6lQb\nq/hh82Z49llJXebnQ9OmMGoUDBgAl18OzZtHOkJFCQvRK84SEqgTd0KdsygjISGB3NxcGjRooAKt\nimCtJTc3lwRXrahSDUlPhz/9CWbPhrg4GDsWbr4ZBg0Sx0xRooyoFWckJlI3/rjWnEUZzZs3Jzs7\nm3379kU6FKUMJCQk0Fxdk6rPpk0wd664YseOyRRJubkyeXjNmvDHP8Lvfqf9jZRKwd698OKL8MAD\n8pshnES1OKsTqzVn0UZcXBytdTSXooSfI0ek91hOjrxOSpLO/fHxMHEi/PWvKsqUSsVbb8HDD8uf\n7cUXh/fa0SvOEhKoFaOjNRVFUcLCI4+IMPv6a+jXD2rUiHREiuKXb7+V5ZIlKs7CR2IitYw6Z4qi\nKCFnwwZpfTFxohT3K0olx1r47jt5vnRp+K8fvT9dEhJINFpzpiiKElKOH4ff/lb6kD3+eKSjUZSA\n2LIF9u2TrPuSJaXvf/gw/OMf8NBDwbl+9IqzxEQSrIzW1FH6iqIoIWDJEulJ9r//wZNPQqNGkY5I\nKYXf/AbefjvSUQh5eTJmJBI4Kc1x4yArSwYH+OLpp6FlS7j/fli5MjiaIqrFWXzRcQoLZeCQoihK\noBhjhhtjNhljMowxU7xsb2GMWWSMWWmMWWOMGRmJOCNGYaFUUl90kYzK/OILSWkqlZqTJ2HaNPjo\no0hHAkePQvv2cP75MkFEuPnuO6hbVzq6gDu1WVAgf94O+/bB3XdD9+7yW2TePAhGl6boFWcJCcQX\nHgfQujNFUQLGGBMDvAiMANKAccaYtBK7PQC8b63tCYwFXgpvlBFk0yYZ3vbQQ2I7rF0Ll14a6aiU\nAEhPF+GRnV3+c6xdC//+d8Vj+eADET4ZGdC7N8yc6X//776Ddu1g9+6KX9s530UXQZ8+MnbFEWfX\nXw8jRrj3y8iQ5X33iZAMFtErzhITiS04AaB1Z4qilIW+QIa1dqu19hTwLjC6xD4WqOt6ngzkhDG+\n8GMtTJ0KHTpAp06wfLl0+f/XvyA5OdLRhZzNm6GoqGzHHDwoX/4rV4YmpvKwYYMsd+zwv19hoRii\n3vjjH2HCBDhxomKxTJsmf06bNok4++UvYetW3/v/+c9SJ1Za8f6yZZIu9cehQ7B+PfTvL6WSaWni\niv3wA3z4IXz/vTt16Yizdu0Cf2+BEN3i7LQ4Z0ePRjgWRVGqEs0Az6+vbNc6T6YCE4wx2cB84Hfe\nTmSMmWSMWWaMWValGyNPny59ylq0gOeekzzUDTdEOqqwkJMDqalld4t++EEer70WmrgmTZK54suC\nI85274bTp33v98ADIlhK7nPqFPz3v7JcscL/tfzVZW3eLB1XJk6UGbzefVeawE45q4BA+O47uS6I\n++eLtWvhggvkd4Q/fvhB4uvXT16ff76IPue4/Hx3u76MDHHWgt0+M3rFWUICNU6fwlCkNWeKogSb\nccCb1trmwEjgbWPMWfdba+2r1to+1to+japqsfzu3TIZ+cUXw+efS4f/KJrNYdcucc1WrSrbcevW\nyfKjj8ruupXG8uUi+t55p2zHrV8vS2vlffnio4+kSP6zz4qv/+EHt9nhFNR745lnoHFjEWHemDZN\nZu266SZ53bQp/OEPIoC///7s/R97DBo0EJPWnzj7wx/ks5471784/O47EVx9+8rrvn1h/35YuBCG\nDZN1mzbJMiNDBgPEx/s+X3kImTgzxpznKojdYIxZb4y508s+xhjznKuodo0xppfHtpuMMemux01B\nDzAxEYAETqg4UxSlLOwEzvN43dy1zpNbgPcBrLXfAwlAw7BEFw62b4cFCyAzU8TY8ePw6qvVqrGs\ntfIWn3jC/xf5oUOydL6sA8URZ7t2wY8/li9GX7zwgiwdJyxQNmyAWrXkua+6s+xsd4H+W28V3/bF\nFyKqmjXzLs6KiuCee+D3v5d6svnzz97n9Gk57xVXFJ8w4t574dxzpfjec+Tk0qVynrvvloy6L3H2\nxRciJnv0gG3bfA8yOHQI3nhDBFmdOrLOqSVr1EiMYXAfn5ER/JQmhNY5KwDusdamARcCt3kpmh0B\ntHc9JgEvAxhj6gMPARcg9R0PGWPqBTU6lzhL5LimNRVFKQtLgfbGmNbGmHik4H9eiX22A0MBjDGp\niDirwnlLD+bPh65dYeRIyeV88AE8+KAUCFUTVq2CgQPlLd53n3/xVBFx1revpOs+/LD8sYI4TR07\nimbetw9mzRJhsWePTF0aCKdPi5M1ZIi89iXOvvxSlkOHwscfFz//F1/Ie7r0UnGfHFG7YYPMa9+1\nKzz1lOj5Vq0kdVmShQsl7ltuKb6+Vi3429/EnWvcWI5v0kSul5wMt90mozu9uXGFhSLuWrWSP1eQ\n2L1xxx0imB0RBhJ3hw7i0LVvL3Vons5ZlRJn1tpd1toVrudHgI2cXZcxGphhhR+AFGPMucAw4Atr\n7QFr7UHgC2B4UANMSJCFOmeKopQBa20BcDuwELmvvW+tXW+MedgYM8q12z3Ar40xq4FZwM3WVoOO\nik89BVdeCW3byrfoK69I58377ot0ZEHlt78VZ+SZZ2Q+9lmzfO/riLOMjOItFvxRWCiCZcAAETlz\n5vh2506fLj3t+fXXIkqGD4f/9/+kJYbTDHXjxsBiysiQNhGXXy6v/Ymzc84RR/HUKakHAxngsHQp\nXHaZFNI7Iy3375c6r3/8Q0TVm2/Cs8/CoEESd8n3/c47kqL0HBHpcNNNIpSfeAIuvFD+FP/+d3Hp\nkpNFQO3YIUauJzNmwJo1Iu7athX37JNPzj7/7NnS4+2BB4qPvIyPFzH2619Lm4yOHeXv48ABeYRC\nnIVl+iZjTCugJ1Dy94evwtpACm4rhjpniqKUE2vtfKTQ33Pdgx7PNwD9wx1XSHn9dclJjRkj37BO\n/qsasmMHXHUV3HknLF4M770nujQm5ux9HXF26pTUYbVpU/r5t26V0YxdukgqbtIkKVbv1q34fseP\ni0i44AJxx3yRlSV1WVu2iDi79FK49lpxixwRWBpOCrRfP/mn9SbOrBVxNnSoCJzu3SUFedttsGiR\niMjLLoN6rjzXt9/Ke83PF3HUtav7XAMHimjavFnEDsh+c+fCjTeKo+iNvn3dtWAlad9ellu2yGcL\ncOSIuHYXXihtMEBE3f/9nwir+vVlXWGhiPLevWXkpz86dpT3FqqRmhCGAQHGmNrAbOAua+3hEJy/\nfKOdPMSZOmeKoih+WLQIbr1VqqFnzQqbMPviC5kWJ5xYK2m1xo3l9S9+Ia8XLfK+vyPOwHeBO4hT\n46TxnHqzLl1g9GhxY7ylNqdOlSL9mTP99+PMyoLBg6VzSVKSGJktWsjzQOvONmyQOFJTZTyHN3G2\nfr18Fk7buptuErfs/vvF8apTR4RkaiqkpEiN1/PPw9VXFxdmIOIMiqc2582TFh3jxgUWc0kcceZZ\nd/b3v8uYlaefdjeHvfJKEZKeAxq2bZNatttu8y0MHTp1ks98zRp5XeXEmTEmDhFmM6213rLqvgpr\nAym4BSow2smV1kypqWlNRVEUn2zYIDZMhw5iIcWGJeHC3r2SYvv738NyuTMcPCipRKcYfeRIER2+\nUpuHDrk/En91Z3fcIa0hrHWLs7Q0SREOHOiuhXJYtkzSdxdfLGlKX9cvLBSnr2VL+PnPJZ7LL5ex\nGampgac1N2yQmqykJN/izKk3c8TZLbfANddInLNni0CMi5Nr9+snfy6HDknvs5J06CDv3VOczZol\n1w7E6fOGI84ckZyVJbGNHy/OmcP558u1PevOnM8pNbX06zhO34IFIvgCcUvLSihHaxpgGrDRWvuU\nj93mATe6Rm1eCORZa3chtRyXG2PquQYCXO5aFzxczln9RE1rKoqieGXNGvnGrVlTrJ8wNpR1RsN5\nG9EXSvbskaXjnCUmivMze7aIpJIcOgTnnSdOkS/nzEl5ZmRIofy6dfKF7hiQY8eKK+WIttOnRcg1\naSJtK3r0kFZy3sjJkVqxli3ltafrk5rq3zkrKnLXyW3YIGIRfIuzL74QUdWihbyuW1c+F0cEPfKI\ne1+nR9hll3nvnG+MiDBHnOXmipM1dmz5B/3WrSv/bo5zNnWqnOtvfyu+X40aImCd3mjg/pwCEWed\nOsny88/ls3J5PUEllM5Zf+AG4BJjzCrXY6QxZrIxZrJrn/nAViADeA34LYC19gDwCDIqainwsGtd\n8DjjnGlaU1EU5SxWrpShe/HxMnF5sLtsloIjdFatcjf8DAclxRlIajMvT76MS3LokNRYOd3svbF9\nu7uo/403pL7MM8137bVSz6bQP30AACAASURBVOYU17/1luzzwgsi+m65RZq6euullpUlS0eceZKW\nJiKrZGp48WLpuN+0qdRcPfWUxO4pznJyig9wOHxYUrvOgAFPmjeXcsTu3d3rhg8XofiXv3j/TEAc\nw23bYOdOEZ8FBfJZV4T27UWc5eWJc3fjjSKeS9Knj6Q7nemeNm6UVh2B/P5o317EZX6+260LNqEc\nrfmNtdZYa7tZa3u4HvOttf+01v7TtY+11t5mrW1rre1qrV3mcfx0a2071+ONoAfocs6SE06oc6Yo\niuLJ6tWSu6pTR77JI9Amw1PolGx2CjJYtLRpeMqD82XtKc6c+qi1a8/e/9AhEVAdO/p2zrZskWXH\njiIYNm92F6yDpNiGDhVxdvq0tGw4/3z42c9k+y9+IRr5DS/fhKWJMzg7tXn99VLjNmSIpPvuuUdc\nQU9xVljoFqogadfjxyVFGAi9e4ugcz47bzjbLr5YGsReeKG4hBXBEWfvvSfxlmzJ4dCzpyyd6bM2\nbgzMNQORD87nHYp6M4jmGQJc4iwlXp0zRVGUM2zYILmopCSxSkJRUBMAmzeLWGjWTGp7PNm0SZyZ\nG27w3oKi5LyO27dLKwdvacmSOILEswFqUpK89ja3o6c427HD+3SAjjibOlXclsLC4uIMJJ23ZYuM\nEM3MlFYYTgF7/fqSWp058+y2Go44c1KNnjhiyzO1mZcnAvTPf5Yar88+k0L8a66RzxTcEzx4pjbf\neks0+gUXnH0dX5SW7uveXRq75uVJwf6iRe73XF46dJA+Zc8/D507i0PmDUcErlghf0NlEWfgrjtT\ncRZsXH81deO05kxRFAWQb+NLL5Uc21dfhT2V6cmmTVLbM2KEpBM953F0elR9/LH0pfJkxQr5wn/6\nafe6yZNlROHVV5c+IfeePVLgX69E2/M2bdwiyxNHnDnmotNewZMtW+Qr5+c/d2vdkuLs6qslDfjy\nyyIoRo4svn3oUKnL2r69+PqsLGjY0PsA2tatpVzQU5w59VhOvMZI25DZs92CtKQ427ZNDNQbb6y4\nePIkNlbKGjMz4a67glO75aQZ162Tuj1f8datK8JqxQpJ4R454hazgeDUnak4CzYu56x2nI7WVBRF\n4fhxyaMdOSKV36EqpgmAggIRNB06iDg7fLj4nIqffCKuyIABMgrSERH5+dKGIT9fGolmZYkbs2CB\nmIELFkjripJNSj3ZvVvSjCWL0tu0Kd05A+91Z1u3yvExMdKqoVGjszPFKSnuxqsPPni2qOjcWZbO\noAGHrCzvKU0Q8dOxY/G0ppN69ffP64izHa5uozNmSDyhmMu+SZPgdmZx3ldsLEyY4H/fXr0krVmW\nkZoOzr5Vruas0uMSZ3ViNK2pKEqUYy386ldiI8ycebatU0HWrJF6qhdf9D5xdUm2bROB1rGjGHmx\nse7U5qFDMsJv1CjphXv6tAivt9+G228XZ+itt+Qt3Xmn1DKdd56k7l5/XVy4km6bJ3v2FE9pOrRp\nI2Ll1Cn3uoICEYIpKW4HZcUK6fg/c6Z7vy1bpDM9yLyS27d7nyj7L3+RhqlXXnn2NsfVcSYnd8jK\nkhYYvkhLK35MeroILScebzRoIC5WdrZ8jjNmSH2at9RpZaNdO7cbeM45/vft2VP+1r77Tl6XRZxN\nmCD/xo5oDjbhaVhTGXH5p7VjNK2pKEqU8+ab0kX0scdE9QSRo0clTeekJVu1ki9EfzjuTocOkn4a\nNEgEwp/+JAMBCgvds0h98IF0wr/xRjnmgQfkeU6Ou7/Wm2/KLX/iRHHa/PX+8mxA60nbtiJUsrLc\nbokzICElRerSWrQo3pdtwABZt3WrpCVBhIOv9F2fPr5rpFJSpP7OU2g58ZRMgXrStasIY8fh27xZ\nYvKXQjTG3U7jtdckfmc6qMpOUpI04w2kNq5XL1m+8458Nt7+3X1Rq1bFR5b6I3qds/h4MIakGprW\nVBQlivGc38Zbt9AKkpkpwuzpp+HRR+W1U8TuCyc16KQKH3tMirz/8hepM2vY0P3lO2KEpPoWLJDz\nOyLi7rvF1ejZ053eMkacFW91YQ67d3v/knZqxTzrzpzZAVJSZPnQQ3Ld116T14sWidg7etS/UxUo\nXboUT2vu2ycpWl9pTXD3GFvm6oWQnh5YKq55c8luT54sE0OMHVv+uMPNL34R2OftjNjctElcs2DW\n01WU6BVnxkBiIknmOMeOlT6xrKIoSrXk8cdFkTzzTFC+nf73P0nrOWRmyvLCCyXV5Ozjj82bZYRi\ngwbuY2+9VUbgffSROEWe81waIyMN//xnd7f++HhYskRSoJ77lhRnubnw/vvy3FqZmcBXWhOK152V\nFGcTJ8KTT8qyYUMZU+GIuWAMeu3cWVw/p/+YvzYaDo4Tt3SpvL+yiLPcXJnE/MMPvadhqzqNGrnr\n68qS0gwH0SvOABISSDJSGXrsWIRjURRFCTeZmaImxo8vW48EPzzyiNRVeV4CJJ3ZpYuILs/O7N7w\nnAzb4f/+Txyto0e912R5Iynp7GLzdu1EYDkC56WXpO/Xtm3uqZu8OWdNmkgq0J84c6hRQ2q0Fi1y\ni7NgOGedO8toUyctHIg4q1dP3vPSpbB/v8QcSNu6K6+UxyefyOdYXXFSmyrOKhOJiSQg46o1tako\nStTx5z97n9+mAuzYIQXvTsuKzEwRNY0by6UGDSpdnG3adLaASE6WdGGvXpJmKy9t20pR/07XbM1O\n1/3ly703oHWoUUNaU3iKs4MHZVlSnAFcconUbC1cKM6ev6L9QHGKz526s0DEGUDfvuIiOm00AnHO\nrr9eUshhnLErIjipzbK00QgHKs6sOGc6KEBRlKhi3TrpQnrnnd7ntykH1rpH+DkiJjNTxIOTMR08\nWJyfkv26HPLzpZjfm7tzxRUiourWLX+MzqhKJ7W5Zo0sly/33oDWk7Zt/deceTJkiCxnz5aPt2bN\n8sfs4AgIp+4sK0smcfB2fU/OP1/EqJNOjsCED5WWkSNFdHub/zOSRLc4S0ggvkidM0VRopC//hVq\n15ahjkHi0CF3iYgz4jIzs7hrNHiwLH3VnTnHlUxrBgtPcZaf7xZbnuLM16g9p9eZMyuBP3HWoYPM\nXXnyZHBSmiBCrGXL4s6Zp/D1Rd++snznHam/C4aLV13o21f+TRs1inQkxYlucZaYSHyROmeKokQZ\nq1ZJD4q77nJX3QcBp2kpuFNoJcVZ165SB+VLnH38sSxD5e40by4uVkaGzJVprUx4XVpaE0Sc5edL\n7RaIOKtRQzRuSYyR1KZzXLDo3FnE2alTsiwtpQkyVVFMjDhurVvLTARK5UbFWYGIM3XOFEWJGh56\nSOyeu+8O6mk952JMT3cLGU9x5qvurLBQBhJMnSqF6KFq7lmjhoiljAx3SvPGG+HAAfjxRxEuJadu\ncig5YtPpHebLuXLEWbCcM5DP5aefZEToli2ld8EHKeh3+gprSrNqEN3iLCGB2EJJa6pzpihKVLBt\nm7TLv/PO0ouVyojjnLVoIelJz5GangweLMLCU8zdfbd087jzTmnFUXL6pGDitNNYvVoK3q++WtYv\nXOh96iaHkr3OHHHmi2HDZHu/fsGLvUsXcc1mzpSRsYH2H3NSmxGclUspA9EtzhITiT2tzpmiKFGE\nM1HixIlBP3V2ttsZS0/3Lc4GDpTl11/L0lr4979hzBgRaLEhnrvGU5x16wbdu8s1Dx703yXemQe+\npHPmi6ZNxZG7+OLgxe60fpg8WQbbBopT8K7irGoQ3eIsIYGYUyrOFEWJEoqKZOLJSy4JyUSJ2dki\nSFJTZcSlM6rQETUO3btLndY338jrLVtkBoBLLw16SF5p10466y9ZIuIsIcGd9vM1UhMkPXjuuYGL\nMwh+13lnloAXXyzbuYcOlVj79w9uPEpoiG5xlphIjVOa1lQUJUr45htJa950U0hOv2OHFNw7dU1f\nfinCp+QE1LGxcNFFbuds8WJZDhoUkrDOwhmxWVAgQhGgd29Zlja/YseObtEZiDgLBZ07lz3t26aN\nOIM9eoQmJiW4RL04MyfUOVMUJUp46y2xrK65JiSnz86Wnl5O6uzrryWl6c3hGThQRM7Bg7Jfw4bQ\nqVNIwjoLzwL9bt1kGag4u+ACWLlSnLdIiTOl+hPd4iwhAXP8OImJ6pwpilLNOXbMXdhVck6jIGCt\n2zlznKlTp3z31Bo4UI759ltxzgYODN/E0y1bintnjDudGag4u+gicdyWL1dxpoSO6BZniYlw4gS1\na6tzpihKNefDD+HIkZClNJ0GtM2bi/Zr1kzW+xJnfftK24r33pMaLmeQQDiIjZW42rd369ReveCP\nfyzdVLzoIlkuXiw/6lWcKaEgxGNiKjmJiVBYSHLSafLztSufoijVmGnTJJ8XosIupy2GMxNUhw4y\nZZAvcZaUJG7VrFnyOlz1Zg6TJxcfFRobK5Orl8Y558jHuGCBvFZxpoSCkDlnxpjpxpi9xph1Prbf\nZ4xZ5XqsM8YUGmPqu7ZlGmPWurYtC1WMJCQA0CDpuKY1FUUJGGPMcGPMJmNMhjFmipftT3vc3zYb\nYw5FIs4zZGRI19eJE0PWQMzpcda8uSydujN/UwUNHCjNZ+vUcRfmh4t77pGeauXhoovg++/luYoz\nJRSEMq35JjDc10Zr7f+z1vaw1vYA/gj8z1p7wGOXIa7tfUIWYWIiAPWTTmhaU1GUgDDGxAAvAiOA\nNGCcMSbNcx9r7e897m/PAx+GP1IPpk8XURailCa4nTNHnDkjNv2JswEDZNmvX+h7mwWTfv1EVIKK\nMyU0hEycWWsXAwdK3VEYB8wKVSw+cYmzlJrqnCmKEjB9gQxr7VZr7SngXWC0n/0jc39zKCiAN9+E\nkSPdhWAhYMcO0X/nniuvR42Cn//cPRrSGwMGSALjsstCFlZIcOrOQMWZEhoiPiDAGJOEOGyzPVZb\n4HNjzHJjzKSQXdyV1qyXcFydM0VRAqUZ4DHFN9mudWdhjGkJtAa+8rF9kjFmmTFm2b59+4IeKCDF\nUbt2wS23hOb8LrKzRZg5Dlj79vD++2d+A3ulfn2ZJ/KOO0IaWtDp0sU9kEDFmRIKIi7OgKuAb0uk\nNAdYa3shaYPbjDE+S0UrdHNLSgIgJf6YOmeKooSCscAH1tpCbxutta9aa/tYa/s0atQoNBHMmCFV\n7FdcEZrzu3B6nJWVli1l1GZVIjZW+p2BijMlNFQGcTaWEpa/tXana7kXmIOkEbxSoZtb3boA1I89\nrM6ZoiiBshPwlCHNXeu8cdb9LaycPAmffQY/+1nIFZDT4yxaGDAAYmKgXr1IR6JURyIqzowxycDF\nwFyPdbWMMXWc58DlgNcRnxUmORmAlBp5Ks4URQmUpUB7Y0xrY0w8IsDmldzJGNMJqAd8H+b43Pzv\nf9LE8aqrQnqZoiJxzqJJnN17L3z1lUy4oCjBJmTjY4wxs4DBQENjTDbwEBAHYK39p2u3q4HPrbWe\nScXGwBwjraJjgXestZ+FJEhHnJnDHDsmN5gQjTJXFKWaYK0tMMbcDiwEYoDp1tr1xpiHgWXWWkeo\njQXetdbaSMXKvHlS9DV0aJkPPXVKRiT6qxlzePppach64YXliLGKUqdO+HuzKdFDyMSZtXZcAPu8\nibTc8Fy3FQhPxxtXWrMueVgrc6WFYFYTRVGqGdba+cD8EuseLPF6ajhjOgtr4eOPZShkIAqrBL/5\nDWzZ4p6U3BfLl7s76193XTljVRSlGNHtE7mcszqFeYDOr6koSjVi7VrYvr1cKc3Tp2HOHEhP979f\nfj6MGyfzUb72WvjmxlSU6k4VavsXAmrWhJo1qe0SZ/n5MqhJURSlyjPPlV298soyH/r995CXJ9kE\na32LrvffFwH35ZfSFkNRlOAQ3c4ZQHIyiQWHAZ38XFGUasTHH8vs4k2alPlQZ97IU6dkMnNfrFgh\ntVdDhpQzRkVRvKLirG5dkk5pWlNRlGpEfj4sXQrDfc6gB8Dzz8sE4CVxxBnAAT/zvKxcCT166EAq\nRQk2+l8qOZmaJ91pTUVRlCrP2rWSj+zd2+cu1sKTT8Irr8C2be71O3fC6tXuJqu+xFlhoezXo0cQ\n41YUBVBxBsnJxB9X50xRlGrEqlWy9KOc1q6FrCx5PnOme/1nrsZF48fL0pc4y8iQe2bPnhWMVVGU\ns1BxlpxM3HGtOVMUpRqxapVU6PuZT2nePCn079IF/vUvcdJAUprNmrl7ePkSZytXylLFmaIEHxVn\ndesSk69pTUVRqhFOMZif3hZz50rq8s47YdMmWLZM+potXAgjRkCDBrKfL3G2apXMCJWWFoL4FSXK\nUXGWnHxGnGlaU1GUKk9BgeQs/aQ0d+4UMTZqFIwZI12Fnn4ahg2T53/4g7s1hj/nrEsXiI8PwXtQ\nlCgnuvucgTSiPXIEQxGHD6tWVRSlirN5M5w44VecffKJLEeNgpQU6VM7a5ZMJPDVV9C+vaQ5a9b0\nLs6sFXFWjhZqiqIEgKqR5GSMtXRvk8/y5ZEORlEUpYIEMBhg3jxo08adkvztb0Wkvf++e35MY8Q9\n8ybOcnJg3z6tN1OUUKHOmWt+zRH98nj+o7qcPi11FIqiKFWSVavE8urUyevm06fFHZs0yV2SNmQI\n5Oae3a+spDj76isZLOBM66TiTFFCgzpnrvk1h/TKIz8fliyJcDyKoigVwSkG8/ErMz1dsp59+hRf\n762RrKc4O34cLr8cOneWmjRjoHv3IMeuKAqg4uyMOLsg9TDGyBxxiqIoVRJrxTnzk9Jct06WXbuW\nfjpPcbZjhzSePf98EXipqTJ1k6IowUfFmUuc1bV59O4N//lPhONRFEUpLzk5sH9/qeKsRg2fWc9i\neIqz7dtl+fjj0oD200+DEK+iKF5RceaqOSMvj0svhe+/135niqJUUdaulWW3bn53ad8eEhJKP11J\n5wykr23LltCqVcVCVRTFNyrOXM6ZI84KCmDx4siGpCiKUi527pRly5Y+d1m3TkrSAqF+fTh2TGrU\nHHHWrFkFY1QUpVRUnDni7PBh+veXX5Nad6YoSpVk925ZNm7sdfOxYzILQCD1ZuBuRHvwoKQ1mzSR\ngaCKooQWFWe1akkBRl4eCQkwYAB8/LEUviqKolQp9uyRH5w+cpYbN8qYgbI4ZyCpzR07/E7VqShK\nEFFxZozUneXJFE6/+Y0Uu77zToTjUhRFKSu7d4u95QNnpKaKM0Wp3Kg4A/ml6RJn11wDvXrBQw/B\nqVMRjktRFKUsBCDOataEtm0DO50jznJzJa3ZokUQYlQUpVRUnIGIs8OHAclwPvYYbNsG06ZFOC5F\nUZSyUIo4W7tW+pPFBjg3jCPOtm6Fo0fVOVOUcBEycWaMmW6M2WuMWedj+2BjTJ4xZpXr8aDHtuHG\nmE3GmAxjzJRQxXgGD+cMYNgwqT175BHpiq0oilIl2LPH52AAEOcs0MEA4BZnznSdKs4UJTyE0jl7\nExheyj5fW2t7uB4PAxhjYoAXgRFAGjDOGJMWwjiL1ZxJDHDXXbBrF6xZE9IrK4qiBIdjxyQD4MM5\nO3hQOm0EWm8GMgNAbKxbnGlaU1HCQ8jEmbV2MXCg1B3Ppi+QYa3daq09BbwLjA5qcCUp4ZwBtGsn\ny+zskF5ZURQlOOzZI0sf4mz9elmWRZwZI+7Zxo3yWp0zRQkPka45u8gYs9oYs8AY09m1rhmww2Of\nbNe60OFRc+bQvLnr4irOFEWpCjg9znyIs82bZdmxY9lOW7++NOeOjfWbMVUUJYhEUpytAFpaa7sD\nzwMfleckxphJxphlxphl+/btK18kjnNm7ZlV9etDYqK7K7aiKIpDIHWxxpjrjDEbjDHrjTGhb87j\nOGc+FFRmpgx4Kmtq0qk7a94cYmLKH56iKIETMXFmrT1src13PZ8PxBljGgI7AU/zvLlrna/zvGqt\n7WOt7dOoUaPyBVO3Lpw+LXOUuDBGbkYqzhRF8SSQulhjTHvgj0B/a21n4K6QB1aKc7Ztm9zT4uLK\ndlpHnGlKU1HCR8TEmTGmiTHGuJ73dcWSCywF2htjWhtj4oGxwLyQBuMxv6Yn552naU1FUc4ikLrY\nXwMvWmsPAlhr94Y8qt275Veljx+p27ZB69ZlP62KM0UJP6FspTEL+B7oaIzJNsbcYoyZbIyZ7Npl\nDLDOGLMaeA4Ya4UC4HZgIbAReN9auz5UcQLF5tf0RJ0zRVG8EEhdbAeggzHmW2PMD8aY0kauV5zd\nu6FhQ5/WWEXFmY7UVJTwEWArwrJjrR1XyvYXgBd8bJsPzA9FXF7x45zl5Mg8m1proShKGYgF2gOD\nkdKMxcaYrtbaQ547GWMmAZMAWlRU/fjpcXbypNzLWrUq+2nVOVOU8BPp0ZqVg7p1ZVlCnDVvLsLM\nKeVQFEUhsLrYbGCetfa0tXYbsBkRa8UISs2sg5/ZAbKyZKlpTUWpGqg4A7/OGWjdmaIoxQikLvYj\nxDXDNdCpA7A1pFH5EWfbtsmyPOLs3HNlGeh8nIqiVBwVZ+C35gy07kxRFDe+6mKNMQ8bY0a5dlsI\n5BpjNgCLgPustbkhDCogcVaetOaoUfDll5AW2nlaFEXxIGQ1Z1UKdc4URSkD3upirbUPejy3wN2u\nR+g5ckRaAfnpcRYXB02blv3UsbEwdGjFwlMUpWyocwYygRycJc7q1dNGtIqiVAEC6HHWsqUObFKU\nqoKKM5CfhvXqQYkZBozRXmeKolQBAhBn5ak3UxQlMqg4c2jSxOuwTO11pihKpScAcVaeejNFUSKD\nijMHH+JMnTNFUSo9fubVzM+H/fvVOVOUqkRA4swY09YYU9P1fLAx5g5jTEpoQwszfpwzpxGtoijV\ni2pzb9u9WwrKGjQ4a1NmpixVnClK1SFQ52w2UGiMaQe8ijRgfCdkUUUCP86ZNqJVlGpL9bi37dkD\n55wDNc6+pVekx5miKJEhUHFW5OrtczXwvLX2PuDc0IUVAZo0gaNHJQfggfY6U5RqTfW4tx0+7G4J\nVIKK9DhTFCUyBCrOThtjxgE3AZ+41nmfXbeq4hTSlrDIgtHrLDsbMjLKf7yiKCGjetzb8vOhdm2v\nmzIzISlJjDVFUaoGgYqzXwIXAY9Za7cZY1oDb4curAjgQ5wFwzm76y4YP778xyuKEjKqx73t6FGf\n4mz7dvmRaUyYY1IUpdwENEOAtXYDcAeAMaYeUMda+/dQBhZ2fIizevUgJQU2bSr/qffsgZ0lp0VW\nFCXiVJt7W36+z/b/u3eXb2YARVEiR6CjNf9rjKlrjKkPrABeM8Y8FdrQwowPcWYMdOsGa9aU/9SH\nD0Nu6GbVUxSlnFSbe1t+PtSq5XXTrl0+258pilJJCTStmWytPQxcA8yw1l4AXBq6sCJAgwYyFN3L\nsMzu3UWcFRWV79SHD8u0d8eOVTBGRVGCTfW4t/moOXPmQz+36g1xUJSoJlBxFmuMORe4DnfRbPUi\nJkYqZn2Is6NHYevW8p368GFZ7t9fgfgURQkF1ePe5qPm7MgR+VGozpmiVC0CFWcPAwuBLdbapcaY\nNkB66MKKED56nXXvLsvVq8t+Smvd4kxTm4pS6aj69zZrfTpnzu1MnTNFqVoEJM6stf+21naz1t7q\ner3VWnttaEOLAD7EWefO0tuxPOLsxAkoKJDn6pwpSuWiWtzbTp6UTtleas527ZKlOmeKUrUIdEBA\nc2PMHGPMXtdjtjGmeaiDCzs+xFliInToUD5x5rhmoM6ZolQ2qsW9zWmcrc6ZolQbAk1rvgHMA5q6\nHh+71lUvmjSRvhdeKv+7dy+fOMvLcz9XcaYolY6qf287elSWXsSZOmeKUjUJVJw1sta+Ya0tcD3e\nBBqFMK7I0KSJ5CAPHDhrU/fukJUFhw6V7ZSezpmmNRWl0lH1722lOGdxcVC/fphjUhSlQgQqznKN\nMROMMTGuxwTArw9kjJnuShOs87F9vDFmjTFmrTHmO2NMd49tma71q4wxywJ/OxXER68zcA8KKGu/\nM01rKkqlpsz3tkqHI8581Jw1aaKzAyhKVSNQcTYRGWq+G9gFjAFuLuWYN4HhfrZvAy621nYFHgFe\nLbF9iLW2h7W2T4AxVpwQizN1zhSl0lGee1vlohTnTFOailL1CHS0Zpa1dpS1tpG19hxr7c8AvyOa\nrLWLgbPzg+7t31lrD7pe/gBEvgjXjzhr2lRSA2WtO3PEWcOG6pwpSmWjPPe2SkcpNWc6GEBRqh6B\nOmfeuDtoUcAtwAKP1xb43Biz3Bgzyd+BxphJxphlxphl+/btq1gUfsSZMeUbFOCIszZtVJwpShUh\nmPe20OMnranOmaJUTSoizoJSxWCMGYKIs/s9Vg+w1vYCRgC3GWMG+TreWvuqtbaPtbZPo0YVrOOt\nU0f6ZngRZwCdOsHmzdLzMVAccda6taY1FaWKULUqtHykNU+fhn371DlTlKpIRcRZGSSKd4wx3YDX\ngdHW2jO+krV2p2u5F5gD9K3otQIMyGevM4C2baU1hpfBnD45fBhq1pS0qDpnilIlqPC9Laz4EGd7\n98pSnTNFqXrE+ttojDmC9xuVARIrcmFjTAvgQ+AGa+1mj/W1gBrW2iOu55cjU6yEBz/irF07WW7Z\nIvOkB8Lhw1C3ruyfny/NvGvWDFKsiqKUi1De28KOU3NWIq3p9DhT50xRqh5+xZm1tk55T2yMmQUM\nBhoaY7KBh4A413n/CTwINABeMjLOu8A1MrMxMMe1LhZ4x1r7WXnjKDNNmsBPP3nd1LatLLdsgb4B\nenmOOGvYUF7n5oqLpihK5KjIva3SkZ8vv/hii9/Ond+Y6pwpStXDrzirCNbacaVs/xXwKy/rtwLd\nzz4iTLRrB59+Ks1oS9zs2rSR5ZYtgZ8uL8/tnIGKM0WpDhhjhgPPAjHA69bax0tsvxn4f8BO16oX\nrLWvhyQYH5Oeq3OmKFWXitScVU9SU+HUKdi27axNSUkirDIyAj+dZ1oTdFCAolR1jDExwIvIgKU0\nYJwxJs3Lru+5ejX2Jw8ROgAAIABJREFUCJkwA5/izHHOGjcO2ZUVRQkRKs5Kkpoqy40bvW5u27Zs\nzpm3tKaiKFWavkCGtXartfYU8C4wOmLRHD3q0zmrX19rXBWlKqLirCQhEmfqnClKtaEZsMPjdbZr\nXUmudU1R94Ex5ryQRZOfrz3OFKWaoeKsJMnJUqThR5zt2gXHjgV2upLiTJ0zRYkKPgZaWWu7AV8A\nb3nbKShNtP3UnGm9maJUTVSceSMtzac4c9ppbN0a2KkOHxa9V7Om3D9VnClKlWcn4OmENcdd+A+A\ntTbXWnvS9fJ1oLe3EwWlibYXcWate9JzRVGqHirOvJGaKuLMy1QATjuNQAYFnDwpYwvq1pXXDRq4\n05r/+5+M5FQUpcqxFGhvjGltjIkHxgLzPHcwxnh6VqMA77/2goFHzdnBgzBxIrRqBVlZ0KJFyK6q\nKEoIUXHmjdRUOHIEdu48a5Nnr7PScKZucsSZM/n5hg0weDC88EJwwlUUJXxYawuA24GFiOh631q7\n3hjzsDFmlGu3O4wx640xq4E7gJtDFpBHzdn8+fDGG9Cjh9xf/vCHkF1VUZQQErI+Z1Uaz0EBzZsX\n21S/PqSklE+cOc7ZP/8przdsCFK8iqKEFWvtfGB+iXUPejz/I/DHsATjkdZMT5dZ6N57DxISwnJ1\nRVFCgDpn3ihlxGa7duV3zrKzYcYMeb1pUwXjrOJ8+SWsWBHpKBSlCmNtsbRmRoakMlWYKUrVRsWZ\nNxo3FnvMz4jNQGrOnJoyT+csJ0fW9+wp4sxLWVvUMHkyPPpopKNQlCrM8eNyE3GlNdPT3YOWFEWp\nuqg484Yx7kEBXmjbVoptT5/2fxpvzhnIYNCJEyUb4UyxEm0UFcGOHVLapyhKOcnPl6WHc9a+fQTj\nURQlKKg484Ufcda+PRQWFk/J5ebCZ5/Bt9+KcAPvNWcgjlGnTvI8WlOb+/fLSNZA+8UpiuIFD3F2\n4AAcOKDOmaJUB1Sc+SI1FfbulbtdCUaPhkaN4M47RaQdPAjnnw8jRsCAATJBembm2eJs8GC46iq4\n8Ubo2FHWRas4y86W5dGjkY1DUao0zn+g2rVJT5en6pwpStVHxZkvunWTpZeK9Xr14Mkn4ccf4ZVX\nRGxlZ8sIqenTJWX37bducZacLMvOnWHePHndrJlMpB7t4kydM0WpAI5zVqvWmTpYdc4Upeqj4swX\nF1wgtWfffut184QJcMkl8LvfwSefwFNPwXXXwQ03QGIiLFki4iwuzvvEwzVqQIcO8NNPIX4flRR1\nzhQlCHikNZ02Gm3aRDYkRVEqjoozXyQni3vmQ5wZAy+/LMJr/Hi47TZZHxsLvXvD0qXueTWN8X6J\njh2j1znb4Zo2Wp0zRakAHuIsIwPOO0/baChKdUDFmT/694fvv4eCAq+bO3QQkfH228UFWN++kg3N\nzXXXm3mjY0epTTtxIrhhVwXUOVOUIFCi5kzrzRSleqDizB/9+8sv07Vrfe7SoMHZzljfvjKv5rff\nli7OrA2sZ1p1wxFnp0+X3pJEURQflKg5U3GmKNUDFWf+GDBAlj5Sm77o21eWO3aULs4gOlObjjgD\nTW0qSrlxibMDp2prGw1FqUaoOPNHixYyt+Y335TpsFat3D3N/ImzDh1kGW3izFoRZ3XqyGtNbSpK\nOXGJs/SdSYA6Z4pSXVBxVhr9+5fZOTPG7Z75E2d16kDTptEnznJzpc7OcQ7VOVOUcnL0KCQlkbEt\nBlDnTFGqCyEVZ8aY6caYvcaYdT62G2PMc8aYDGPMGmNML49tNxlj0l2Pm0IZp18GDBCbZ/v2Mh0W\niDgD6XW7zuunU31xUpqOc6jOmaKUk/x8qFVL22goSjUj1M7Zm8BwP9tHAO1dj0nAywDGmPrAQ8AF\nQF/gIWNMvZBG6ov+/WVZxtRmoOJs4EBYuVKmM4oWHHGmzpmiVJD8fKhdm8xMceG1jYaiVA9CKs6s\ntYuBs+c/cjMamGGFH4AUY8y5wDDgC2vtAWvtQeAL/Iu80NGtG6SkwMKFZTrs/POl0awz2bkvhg2T\nGqwvv6xAjFUMdc4UJUi4xNmRI+6ZSBRFqfpEuuasGbDD43W2a52v9eEnJkYm05w3T2bqDpBGjWDR\nIvj1r/3vd/75Mh1UGbVflWbHDmnW66Rgwu2cFRWV6Z9SUSovR49C7docPQq1akU6GEVRgkWkxVmF\nMcZMMsYsM8Ys27dvX2guMmYMHDoE//lPmQ4bNEiElz9iYuDSS+Hzz8VBiwaysyUFE6nRmk8+CZ06\nRc/nrVRjXDVnKs4UpXoRaXG2EzjP43Vz1zpf68/CWvuqtbaPtbZPo0aNQhPlZZeJkvjgg5Ccftgw\nyMmJnoEB2dnSocT5Mgm3c7Z6NWzbBrt3h/e6ihJ0XGlNFWeKUr2ItDibB9zoGrV5IZBnrd0FLAQu\nN8bUcw0EuNy1LjLUrAmjRsFHH4Wknf2wYbIsT2pzzRr4xS8ik6b7/vvyfRyOOEuS1kxhd8527ZJl\ntLUwUaohLnHmWiiKUk0IdSuNWcD3QEdjTLYx5hZjzGRjzGTXLvOBrUAG8BrwWwBr7QHgEWCp6/Gw\na13k+PnP4cAB+O9/g37q5s0hLa184uyDD2DWLFi/Puhh+SUzE/r1g3feKdtxTgPaSDpnKs6UaoPL\nMlPnTFGqF7GhPLm1dlwp2y1wm49t04HpoYirXFx+ufw0/eADSXMGmWHD4MUXRag4jlIgbNwoy/Xr\noWfPoIflk23b3NctCwcPynts3lyG/RsTfucsJ0eWKs6CQ2amzCXrtEZRwojWnClKtSTSac2qQ2Ii\nXHklzJ4dktRmv36SmiyrYHDE2YYNQQ/JL047jM2by3bcTlflYLNmIsySksLrnB0/Dnl58lzFWXC4\n/XaYMCHSUUQpJ05AYqKKM0WpZqg4Kwtjx8rcQ2UctRkIbdvKcuvWwI8pKID0dHke7rSmI7Kc64O0\nyJg2zf9xe/bIskkTWdaq5d8527cPvvqq/HGWxElpxsSoOAsW6enyb6+EmYICKCqiILYmBQUqzhSl\nOqHirCwMHy6dHt99N+indnp+bdniXjd3Lrz3nu9jtm0Tty02NvzOmSPOtmyBwkJ5/uyz8KtfweHD\nvo9zRkg64qw05+zJJyXlGyyz0hFnffrI53fyZHDOG60UFUFWloho5+9ACRMnTgBwysi0ADogQFGq\nDyrOykLNmnDNNTBnzpkbY7BIToYGDYo7Z3/9qzx84aQ0L71URNLx40ENyS9OWvPkSbdrsnKlLP21\nm3Ocs8aNZVmac7ZpkxgEwZreyqk3GzxYhEVGRuDHrlkD//pXcOKoLuzdK38DRUUyXkYJI65fFiep\nCahzpijVCRVnZWXsWLGGFiwI+qnbtHGLs6Ii+OkncXeKirzv74iza6+VUZDhTNPt3On+MkhPl+uv\nWCGv/Qmp3btlIIAz52hpzpmTNt27t+Ixg9s5GzxYlmX5zB5/HG65RR0iTzIz3c8d4R0NGGOGG2M2\nGWMyjDFT/Ox3rTHGGmP6BD0I1w/EEy7nTMWZolQfVJyVlUsukQkzQ5TadMRZVpY4YSdOuAVFSTZu\nhHPPhYsuktfhrDvbuVMmbQcZFJCVJZMoQOnOWePGMhgA/DtnRUXuNG+wvvh37YK4OPdnVhZxtmaN\npJG3bw9OLNWBrCz382gRZ8aYGOBFYASQBowzxqR52a8OcCfwY0gCcTlnJ6w6Z4pS3VBxVlZiY6Xn\n2ccfyzD2INK2rXzZFRQUryHzNUjgp59kGqL27cNbd1ZQIA5Y797yhZCe7k5pQunOmVNvBuKc+RJn\nOTnu7HGwnLOcHLl+crJMIRWoODtxQj5vKPsI1epMNIozoC+QYa3daq09BbwLjPay3yPA34Hg1kA4\nOM6ZVedMUaobKs7Kw7hxYmvNnRvU07ZpI8Jnxw53yhK8izNrZZ/UVIiPF4EWLuds925xtc47T667\neXPg4sxxzhxq1fKd1vSsBwumc3buufK8Y8fAxdnGje50ZqDibN++oJcmVjoyM+XvD6JKnDUDPMen\nZrvWncEY0ws4z1r7aciicDlnxwrVOVOU6oaKs/LQv78ok5kzg3paZ8Tm1q3igjVoADVqeBdnu3ZJ\n6Vtqqrzu3Dl8zplnr7L27d3OWVqafFH7S2uWxTnzFGfBrDlr2lSeO+IskAnQV692P/dsH+KPvn3h\ngQfKHmNVIivL/e8eReLML8aYGsBTwD0B7DvJGLPMGLNsn7//ON5wKf9jRTpaU1GqGyrOykONGjKh\n5eef+1ciZaSkOOvWTTSgN3HmOGuOOEtLk/qscDg1zkjN5s2hQwcZtLBsmcxQ0KiRb+essFC2lcU5\ni48XMRUq5+zgwcD+CdeskT7EPXoE5pwVFIir9OWXFQq30pOZCa1awTnnRJU42wmc5/G6uWudQx2g\nC/BfY0wmcCEwz9ugAGvtq9baPtbaPo0aNSpbFC7n7GiBOmeKUt1QcVZexo8XtfHvfwftlM2bS7H6\nli0ivtLSig8S8MSbOCsqCs+IzZLOWWGhOGI9e8pYCV/ibN8+idHTOfM3ICA9Xd7/uecGxzk7eVJ6\nCHuKMwjsM1uzBrp0kc87EHF28KAs16713/etMrJ6tfSBy831v5+14py1aiWCO4rE2VKgvTGmtTEm\nHhgLzHM2Wmv/f3tnHh9Vef3/95MQEvYtgEBkVWQLaxQVUZBFsAKiVMEVXFDqglJbqW1dsHyLrVXU\n8tOiFa1VEEWBiogoVKEqmwKyCmJQwiKLQgIkkOT5/XHmMjeTmck2k1k479drXjNz5y7P3IF7P/mc\n85xz2Fqbaq1taa1tCXwBDLXWrg7pKDx/ieXka86ZosQbKs7KS3q63K1DGNpMTJQb3fLl3pBl69ZF\nC9M6bNki5SgcodGxozx/8UXIhhOQrCxxtFJTRZw5OOIskBPlW+MMJKyZm+u/XMj27XDWWeLKhEKc\nOQVwnXPmOJUlzb60VgRL587yfXfuLLl4rSNsCgthRXjm6oWNZctgzRr44IPg6x06JMK6RYvTS5xZ\na/OBu4FFwGZgtrV2ozFmkjFmaKUNRJ0zRYlbVJxVhOuvh88+83YBDwGtW8Pnn8trxznbt6+4u7Ru\nnXzulKRo105CbvffLzfXcLJrl7c3Ztu23uVduwYPa/p2BwDvDcU3tGmtV5yF6sbvlCRxcs6aeVK4\nnTBtIPbule/UubN838LCkttsuV2nzz4r33gjheOMltSlzKlxdho6Z1hr37fWtrXWtrHWTvYse9ha\nO9/Pun1C7prBKecs+6Q4Z9Wrh/wIiqJECBVnFWHUKHl+5ZWQ7bJ1a6+L5IgzKKr/fvpJBNyll3qX\nVakCixZB8+bwi1/AqlUhG1IxsrK8wqZBA6hbV9yT+vXL55yBiLN9+8QlW7xYBNGxY0Wds9Ik7gfD\nEWeOc1azpoy9JHHmTAbo0sUrRksKbTrirGpV+N//yjfeSOEWZ8HOuVNGw3HOyvob/e53MH58+cd5\n2uNxzrJPJFOtmqTCKooSH+h/54rQogUMGQJ//3vIap45YqxePREl7kkCDosWSZ7XL35RdNtGjeSG\nWreuOGgV5dAhidx+8knR5VlZkh8H4p717g0DB8r71FQpRuuvF6bjnPlOCABxBjduFGH32GPemZqO\nc3biBBw+XLHv47RucsQZyPcoqWn3+vXy7IQ1oeQZm444u/RSCTXHUlcBR5x9/73/kLqDr3N28qQ3\n1640vP++PJRy4nHODuel6ExNRYkzVJxVlIceEhUzfXpIdtemjTw7IUt/4mzBAnGsevYsvn2zZjBs\nmLg9/vK45s0r/Q3x3XdFMLn7SVrrDWs6zJ8P//iHvHYmnPnrs7hvnzhl7huJ2zlzRMH//gevvSav\nHecMKp53tmePuAvuSXFpaaVzzs48UwRz3bqyfWmdsyFDIDu7fDXo1q2DuXPLvl1FycoSlxCChzZ3\n7oRateScOIK7LKHN77+XY1XUET1t8Thnh3OTNd9MUeIMFWcV5fzzoW9fePLJkrPES4EjxpxZmA0a\nSOK/I84KCqSt5+DBMoHAH+npYuS5q7eDOB0jR8KNNwbvZ+nw9tvy/OGH3hvoTz/JH+zNmhVd18l9\nS02VZ3+hzb17i7ZugqLOmSPO6tSBl16SUK0TMoOK5zTt2SP5bu7zVhpxtn69uGYObduWTpxVqQKX\nXSbvyxPa/L//k8h5sMbw4SArS3qPNmtWsjhr0UJ+z7L+RtnZ4rAeP142t01x4XLOVJwpSnyh4iwU\nPPSQ3PlffbXCu2rTRsTYhRfKe8c9c8JLK1bIjd83pOkmPV2enXCcw4QJUn/r0CH417+Kb/fIIzBp\nkrz+6Sep0dW0qTgcjhhxBJQT1vTFcaX8TQrYt6/oZAAo7pzVqQN33y1isGVLETihcs527y4a0gRx\nxPbtk7CpPwoL5bs7Yhm8hXeDcfCgCOvWrUW4lGdSwA8/yP33ww+Dr5edHTr36cgREfbNmkG/frBk\niX8HFrw1zsD7G5VWnLlDyVlZgddTguD5Y/Dn4+qcKUq8oeIsFPTrB+eeK1ZHBd2zmjXlxnXzzd5l\n7lpnCxaI8+M4Mv7o1Emev/7au2zRIglTPvaY1LB6+umiN92cHPjLX+DRR6WMwn/+I0Lub3+Tzx2B\n4LhMvs6ZQ2mcMze+zlmzZnDPPZCc7M3vCqVz5ivOHJHp5KP52+bECa+jCeKc7d4dPM3QEWfGSEOJ\nJUvEJSoLzrkO1iVs+XIR83XqiIn73/+W7Ri+uGvY9esn38NX5Ds4zhmU/Tdyly8pyblUApCbCwkJ\nHDlWRcWZosQZKs5CgTEizHbulMkBFaR27aIzr1q3ltmay5aJwOrVS/KfAlGrFrRq5RVn+flw770i\ndn79a3HQvvmmaO7Z++/Ltb5qVZlB99Zb4ipde624eWUVZ6V1ztylNBxx1rgxvPmmCEVnn8aUzjl7\n5RX/AiU/XyYZOE6PgyPOAgkERxT7ijMILFrAK84AfvUrEXOPP17C4F0UFHgFoyOU/bFypTxfd518\nv7/8pfTH8IevOAP/oc0DByQs6ZyXBg3kjwZ1ziqRvDxITuboMaPiTFHiDBVnoaJ/fxg0CP70J//Z\n8BWgXTu5Dl98sXQGuPLKkrdJT/eKs08/FTH2+OPiSI0YIcLrqae867/9toSmnn1W8qPeew+uvlpE\n0cCBsHSpfK2//U1CnWUVZydPFm/dBN6w5tGjRScaDBsmvSlBQpsNGnjF2fjx8OKLxY+dnQ133AF3\n3VU8zLd+vRzDCRc7lCTOnBImrVp5l/XuLWVDxo71umcnTxZ10g4c8J6Lfv1g9GgRTmvX+j+OLz/+\nKAKtXz8578uX+1/vm29kLC+8IG7rxx9XrCOBW5w1ayZCdOnS4uu5y4uAd6JFWZyzhAT596XOWTnJ\nzYWUFI4e1b6aihJvqDgLJX/9q9wZ//SnkO72xhvFufroI6lvds89JW+Tni437rw8CYulpMAVV8hn\nSUlw331y0124UFyrBQvgqqvg1luhe3dZb8QIeR44UIRN//6yz3//WwSTP5KSJMTmG9Z03gdyzo4c\nkbBnINHn9G48eFDMyTfeKL7OBx9ICHLTpuJ13pyE/F69ii4vjXNmjDd854zlzTdFKI8eDbNniyuZ\n4eqc6HbOQERtgwZyfgO5YG6c8dx6qwjqQKHNbdu84d/hw+X7V6Q8hVucgcx1+fTT4mN2RKYjzqBs\nhWh/+EFEfqNG6pyVG49zlpOj3QEUJd4Iqzgzxgwyxmw1xmw3xkz08/nTxpi1nsc3xpifXZ8VuD4r\nVnU7KunUCcaMEfWwYUPIdlu1KgwYIC7K+ecHFkZu0tPFedm8WW7s/fsXvYDfdZckuY8bB3PmiEAb\nMUJCU6++ChMnwgUXyLp9+8ryr76Chx+W98Hw1yXAX40z8DpnO3ZIDlwwcfbjjyLACgv9J+TPmycu\nUkpK8brA//ufCLHmzYsur11bwsDBnLNmzUQguenfX5ywOXMk9Lt/v/ToPHpUXDtfcVa/vkzo/fLL\n4nXj/OGMp107Odbcuf6T/r/5xhtmveACOU/vvlvy/gORlSUh82rV5H2fPuJIfvVV0fXWrpXz6biD\nUDZx9v338luUZrasEgCXc6biTFHii7CJM2NMIjANGAx0AEYZYzq417HW3m+t7Wqt7Qo8B7zj+vi4\n85m1tvL61VWUyZPl7jZqVNkzwEOIM2Pz3/+WVLhhw4p+npwsocGdOyUU2KABXHKJfNapE/z5z968\ntzp1pHTHZZfBH/9Y8rH9dQlwbtqBZms6YiuQOHNu/I4rlJVVNIx48qSEYocNEwdw5sxTlQawVsKC\nvq6ZQzCBsGNH0XwzNxMmyHl6+WVvnbfMTBnXyZNFxRnIOYTiQufAAcmTe+strwBz5/YNGyb79dX7\nx47Jeo44S0yUdZ38wfLg7v4AIs6geGhz7Vpp1+UmmDgrLPR2aABxzs48U46lzlk5ycvDJierOFOU\nOCScztl5wHZr7Q5r7QlgFjAsyPqjgJlhHE/l0Lix2DYbNsCDD0ZsGG3biuP2/PMSlhsypPg6vXpJ\nsvrx45LHFsyRmz9fbvqBaqu5SU0tvXOWkCBOl1OqI5hztnevOGeO6HG6CIC4UYcPy/cYM0aS1Z0C\nrk6x02DiLFCXgO++CyzOjBGHccwYb/Hg777zFqD1FWepqfL93Hlnjz8uTmPfvnDNNRJChKLN5R1R\n51tSw/n+7ubzw4eLOCypL6bD4cPinjoTC3zF2RlniMPqFme5ueLI+hNngVo4vf22hIa//14+/+EH\ndc4qTG4uNjmFggIVZ4oSb4RTnDUD3Le8XZ5lxTDGtABaAUtci1OMMauNMV8YYwKmwBtjxnrWW70/\nUFPHymbwYEnqeu45byXXSqZKFekycOyYhEJ9RZHDn/8Mv/yl1BYLhjGl793nL6zpuCP+xlGjhjfx\nPphzlp0tyfF33CHL3KHNuXPFhRswQFomNW8OM2bIZ06+2UUX+d/3mWf6Fwi5uTJu92SAQDjrBBNn\nIILGSaYHCUF27Sp5a+AVbs7kiIQEETDt2knOoRtH0Lqbz196qYRpSxPa3LtX3NIXXvDO8vQVZyDu\n2bJl3pZcGzdKyNydbwbyG+Xmyu/ky5dfyvZLloirmpcn5z0tTWrqlaYosuJDXh6FVSTeruJMUeKL\naJkQMBJ421rr7kDYwlqbAVwHTDXGtPG3obV2urU2w1qb0dDdlyfSTJkiqui66yTeFgGc0KZvSNNN\n7doiDHxdkIrghDXdDsp//yvhUn83kerV5WbvLjjri7M8MVHcPvCKE2tFnF12GacaQN9+uzhNb70l\n4qxmTe/58CUtTUJuvv1And6RgZwzN40by7EzM0sWZ5s3e0XMunUwdKjk+6WmemfY7tpVtNDvgAHi\nDrrL6Dnf3+2cJSfLpOFFi/yPMydH/l6YPFmcxO3b5Z/phx+Kg7pvX3Fx1rev5NKtWSPvHQHp+2/G\nGcecOcWP6wjpTz7x1jhr3tx7LA1tloPcXPKTUgCdrako8UY4xVkWcKbrfZpnmT9G4hPStNZmeZ53\nAP8FuoV+iGEkOVmmQnbpIjUpFi6s9CF085yxYOIsHKSmiohw2g4dPSrOS6DCuY5ga9IksDvnOG69\nennLPDg3/LVr5ebu/p6//a2IjltukTphwSZSpKWJwNu7V56d4rz+ymgEwhipoVYa56ygQNynL76Q\nY/XqJdu7y5/4irP+/UU8uTsNbNsm58z3xpyRIdv7q+gycqQ4pX/4gwjdjz+W6Ht2tog2f5MyfPPO\n1q6VY/qK1iFDpMzIhAlF88ucsYKIdCeE7DhnoOKsXOTlkZ+ozpmixCPhFGergLONMa2MMVURAVZs\n1qUxph1QD/jctayeMSbZ8zoV6AVsCuNYw0PdumJJdOwo1ohjPVQSY8dKDlO7dpV62FMtnJwo8yef\nSImHQOLMmRQQqCUUeJ0zp23V2Wd7nSMnv2rAAO/6VauKa1atmoiBQPlm7uPu2iXR6LZtxUXzV4A2\nGK1alU6cgQic//1PxOj558uy9HRvyNBfYn5iYtHQpnumphsn3OgOn4KMbcECKUScnS3b9+wpwq9q\nVW/tOF9x1rChuJ7vvSfibe1aOYavkE5IkJ6oublFw+TWikNXq5Y4i07NNrdzpnln5SA3l/xEcc5U\nnClKfBE2cWatzQfuBhYBm4HZ1tqNxphJxhj37MuRwCxri6QRtwdWG2PWAUuBKdba2BNnIDM3Fy4U\nO2nIkEq9C9WoIU5GZeNbiHbRIhFJgcbi3FgC5ZsB9OgBv/+91P0CESWOG7N0qbxv2rToNmlpMGuW\nhG4vvzzwvh1xNn26FOH99luZePDddzJZwXeGaSB8xVn9+sXXad1aXCdHnKWny/hABNDRo5KflZtb\nVKzWri1CavFi77KyirOXXhIBNX58UbetZk3JPVu2TN77+x3uuENcu0mTZL+BwuBt20qLsHfe8Ubz\nd+8W1++66+T9zJlyXhs00LBmhcjL42SCOmeKEo+ENefMWvu+tbattbaNtXayZ9nD1tr5rnUetdZO\n9NnuM2tturW2i+f5n+EcZ9hp3FjuVDk5ItDiPPvZt/n5okVy809J8b9+acRZUpLU9nXcqLZtZf/7\n94uoCFR77dJLJeHc6TbgD0cEvfKKCKRGjWQywY4dIriMCbytm5YtZfbj9u1SfsRfGDUhQcTTmjUS\n1nQ7ek5OnBMB93USBwyA1aslXPnTT/L93flmDmecId/BLc5OnpSSH5dfLuFEXxxHEoqLXJC6eDfd\nJMIrOzt4juKECWIa/+c/8t4R0cOHy98qe/aIa2aMCMM6ddQ5Kxe5uZxIUOdMUeKRaJkQEP+kp0tZ\n+bVrJa4Uxzg33pdeEidp69bgjdqdsGYwceaLI0pmzRKxEKwwbkmzTOvWlTEkJUlduJtuEmGxZk3p\n8s0cnHVXry5anNWXLl2k00NOTlFx1rGjPH/wgTz7irP+/SVEuGSJV/D4c86cY7jF2YIFklM3dqz/\n9R1xlpTkFdcXWnoYAAAgAElEQVRujJFabk4INpg4q1JFxPCKFfLeKflxzjnSggyKCsS0NHXOykVe\nHnlGnTNFiUdUnFUmgwfDAw9I7YJA/XjigKZNpRr+O+/ITESQFlCBKI1z5osjSqZPl2cnab08GAN3\n3ik14bp0kbpl+fkyq7C0+WbgFWdbt/rPN3NwCxu3OHMa1juixvd89OwpwmnCBKk7B8HF2caN3hmo\n06fL/pyaab6cdZbsK9ikjJQUOe7zz0uYORg9e8rkhqNHRUhWrSqCzCl07O7UoLXOykleHnnobE1F\niUdUnFU2kydL88pbbpHkpjjl/vul5MWGDXLzbd8+8Lrlcc5atxYRsWGD1HMLVMettPztb958tg4d\nvA5RecSZtaUTZ82aFW8nlZ4uSfcJCcVz3ZKSvP1DJ0+WdQKNr0sXWW/rVnEvP/hA/skFKzT8f/8n\nRXWD0bChCNmSQr09e8r3WLNGxFnr1jKhwRHRbudMuwSUk9xccq06Z4oSj6g4q2yqVpWu3SdPimK5\n667idQfiAGPgmWfkRv7gg8Fv5uVxzqpWlRwvKLnXZ3kYM0aeyyLO6tWT/CkILs46dRJh5ZTQcOPk\nnTVp4l9Ide8uiflnnSXr+vb8dHBPCnjxRTnO7bcHH//VV0u3gFDg5PitWFG0OXuXLmIeX3ONd920\nNAm5lqYhvOIiL4/jaM6ZosQjpWihrYScc86RmNOf/iTxprlzJTvcX6Z2DFOlioTASqJ2bREP/hLR\ng9G2rSTth0Oc3XSTzDAMlivnj1atJK0wmDirVk3y8bp3L/6ZI86ClRVp3VpChsHmlbRrJwJ21SqZ\nHXnFFZX7z6thQzkXX3whBrET1k5IgL/+tfhYCwvFuZw2TUN0paKgAPLzyS0Ude40qlcUJT5Q5yxS\nnHmmZFivWiWZ4YMHS0PI05CxY73tl8rCOefIs5PHFEpSUqTkRKAZpoFw3Lxg4gzEmfNtfwTiqkFw\nceaMz1+pDoekJJlg8OKL0u8yVI5YWejZU2bqHj8uTl8grrkGHn1UJmP06FG096gSAE+riGOFKVSv\nXvrWaoqixAb6XzrSdO0qmfPffCNdu6OlP2gl0qSJd+JAWZgwQU5dsJmRlY2Td1aSOAtE27YiUssy\nSzQQXbqIu9aqVfAJGeGiZ09vlwh/JT8cEhPhkUekmHBOjoqzUpGbC8CxgmR1GhUlDtGwZjTQr58U\n1ho9WmI8U6ZIjEf/HA5K8+bFE+ojTUXFWVKSdHVo0aLiY3GcuTvuiMw/pZ49va+DiTOHPn2k72it\nWmEbUvzgcc5y8lM030xR4hC9+0cL118vlkGnThLnu/pqr+2gxAxt2shzoAbupaFHj9C4gcOGyeO2\n2yq+r/LQrZuIzapVSw7TOjj5h9GOMWaQMWarMWa7MabYHFdjzJ3GmK+NMWuNMcuNMR1COgCPc5Zz\nMlnFmaLEISrOoomOHaUz9NNPS0GpSy6R3jdKzDBwoJigTrHVSNKqleTyldfFqygpKRK1P+ssCV3G\nC8aYRGAaMBjoAIzyI77e8HQ36Qr8BXgqpINQ50xR4hoVZ9GGMdJ9e9482LLFW81TiQmqVJHodDyJ\nkYrw3HOlm7EbY5wHbLfW7rDWngBmAcPcK1hrj7je1gDcvYMrjsc5yz6hzpmixCMqzqKVK66QppEF\nBXDRRdKzR1FijJ49o8NFDDHNgB9c73d5lhXBGHOXMeZbxDm7N6Qj8DhnR06oc6Yo8YiKs2imWzdv\n/bNBg2D58kiPSFGUUmKtnWatbQM8CPzB3zrGmLHGmNXGmNX7yzJT2+OcHcnT2ZqKEo+oOIt2mjf3\nTt8bMUL73ChK5MkC3CV90zzLAjELuNLfB9ba6dbaDGttRkN/HecD4XHODuepc6Yo8YiKs1igfn3J\n7D56FK66SiYJ2NCmsCiKUmpWAWcbY1oZY6oCI4H57hWMMe7iIb8AtoV0BB7n7OfjmnOmKPGIirNY\noWNHePVVWLlSmlDWri29hebNk7w0RVEqBWttPnA3sAjYDMy21m40xkwyxjjllO82xmw0xqwFJgA3\nh3QQHufs4LGUU/1cFUWJH7QIbSxx1VUizlasgK1b4d13patA69bwyivQu3ekR6gopwXW2veB932W\nPex6PT6sA/A4Z3kkB23jpShKbKLOWaxx7rlw991SoyAzE95+W8q/9+kDkyZJ+ycNeSpKfONxznJJ\nUXGmKHGIirNYpkoV6STw5ZcwapQ0KGzUSEKeV10FO3ZEeoSKooQDdc4UJa5RcRYP1KoFr70m3QWe\neQZuugkWL5Y8tUcfhR9/jPQIFUUJJS7nrF69CI9FUZSQo+IsXjBG2j3dey9MmyYdpIcMgccekwkE\nI0ZIxwFFUWIfdc4UJa4JqzgrRXPg0caY/Z7mwGuNMbe5PrvZGLPN8wjtTKfTgbQ0mD0bNm2C8eOl\nw0CPHjLjU1GU2CYvD2sMJ0lScaYocUjYxFkpmwMDvGmt7ep5vOTZtj7wCNAT6WP3iDFGzfvy0L49\nPPkkbNggkwlGj4bhw2XGJ8jkgWPHIjpERVHKSG4u+YnJgNGwpqLEIeEspXGqOTCAMcZpDrypFNte\nBiy21h7ybLsYGATMDNNY45+mTeGjj2DKFBFrc+dKCY4DB+DIEQl7TpsmEwoURYlu8vI4mZhCzRRI\nSor0YJRIcfLkSXbt2kWuJ8ytRC8pKSmkpaWRVMr/sOEUZ/6aA/f0s97VxpiLgW+A+621PwTYtlhj\nYaWMVKkCf/iDhDlnzIClSyX8WaUK/L//JxMK/vpXuOEGWaYoSnSSm8sJo/lmpzu7du2iVq1atGzZ\nEmNMpIejBMBay8GDB9m1axetWrUq1TaRnhDwH6CltbYzsBgoc0JUuRsHn87UqiUTB959V+qlPf20\nlONo0wbGjIFzzoHp0yEnJ9IjVRTFH3l55BmtcXa6k5ubS4MGDVSYRTnGGBo0aFAmhzOc9kiJzYGt\ntQddb18C/uLato/Ptv/1dxBr7XRgOkBGRoZWXy0vHTvCZ5/Bf/4Djz8Od9wBEyZIuPOMM+DkSWjQ\nADp1kjy2Jk2gZs1Ij1pRTk9yc8mz6pwpqDCLEcr6O4XTOStNc+AmrrdDkT51ID3rBhpj6nkmAgz0\nLFPCSUICDBsGq1bB8uUwcqTkpk2dKk7a738vn7dtK+5bzZrSMurBB+GTT7QzgaJUFnl5HLNa40yJ\nLH379mXRoqK35qlTpzJu3Lig29X0/GG/e/duRowY4XedPn36sHr16qD7mTp1KsdcE9ouv/xyfv75\n59IMvVR07dqVkSNHhmx/ZSFs4qyUzYHv9TQHXgfcC4z2bHsIeBwReKuASc7kAKUSMAZ69YKXXoKf\nf5aaStnZMnHgs8+kj+cTT8Att0B+voRF+/QRR+1vf4OsrJKOoChKRcjN5XiBOmdKZBk1ahSzZs0q\nsmzWrFmMGjWqVNs3bdqUt99+u9zH9xVn77//PnXr1i33/txs3ryZgoICli1bxtGjR0Oyz7IQ1pwz\na+371tq21to21trJnmUPW2vne17/zlrb0VrbxVrb11q7xbXty9baszyPGeEcp1JKatWCCy6Am2+G\n3/4Wnn0WPv9cBNwrr0D9+vDAAzLJoFcvuPFGGDdORJ6W61CUkGHz8sgp0JwzJbKMGDGCBQsWcOLE\nCQAyMzPZvXs3vXv3Jicnh379+tG9e3fS09OZN29ese0zMzPp1KkTAMePH2fkyJG0b9+e4cOHc/z4\n8VPrjRs3joyMDDp27MgjjzwCwLPPPsvu3bvp27cvffv2BaBly5YcOHAAgKeeeopOnTrRqVMnpk6d\neup47du35/bbb6djx44MHDiwyHHczJw5kxtvvJGBAwcWGfv27dvp378/Xbp0oXv37nz77bcAPPHE\nE6Snp9OlSxcmTixW1rXM6JQ8peJUry6C7eab4Ztv4M034b33xGU7fBheeEHE3C23yISDjh3FiVuy\nREp3nH++uHWKopSKwmO55NoaKs6UU9x3H6xdG9p9du0qWS2BqF+/Pueddx4LFy5k2LBhzJo1i2uu\nuQZjDCkpKbz77rvUrl2bAwcOcP755zN06NCAuVfPP/881atXZ/Pmzaxfv57u3buf+mzy5MnUr1+f\ngoIC+vXrx/r167n33nt56qmnWLp0KampqUX2tWbNGmbMmMGKFSuw1tKzZ08uueQS6tWrx7Zt25g5\ncyYvvvgi11xzDXPmzOGGG24oNp4333yTxYsXs2XLFp577jmuu+46AK6//nomTpzI8OHDyc3NpbCw\nkIULFzJv3jxWrFhB9erVOXSo4oG+SM/WVOKNtm3hj3+UIrfffgv798Onn0K/ftL3s1MnaNcOUlPh\nyivhwgtFnL32GuzaFenRK0pMUHA0T/tqKlGBO7TpDmlaa3nooYfo3Lkz/fv3Jysri3379gXcz6ef\nfnpKJHXu3JnOnTuf+mz27Nl0796dbt26sXHjRjZtCl4udfny5QwfPpwaNWpQs2ZNrrrqKpYtWwZA\nq1at6Nq1KwA9evQgMzOz2ParV68mNTWV5s2b069fP7766isOHTpEdnY2WVlZDB8+HJDaZdWrV+ej\njz5izJgxVK9eHRDRWlHUOVPCizEyaaB3b2nA/sYb8P778ItfSO/PzZvhqaekWTtAixYwYAAMHgwD\nB+qMUEXxQ+GxXO2rqRQhmMMVToYNG8b999/Pl19+ybFjx+jRowcAr7/+Ovv372fNmjUkJSXRsmXL\nchXL/e6773jyySdZtWoV9erVY/To0RUqupucnHzqdWJiot+w5syZM9myZQstW7YE4MiRI8yZM6dS\nJweoc6ZUHo0aiff+4YcycaBPH8lJ27JFZohOnSr9P2fPhquvlq4G99wD77wDEyfCoEEyY3TlSigs\njPS3UZSIYXPFOVNxpkSamjVr0rdvX2655ZYiEwEOHz5Mo0aNSEpKYunSpezcuTPofi6++GLeeOMN\nADZs2MD69esBEUY1atSgTp067Nu3j4ULF57aplatWmRnZxfbV+/evZk7dy7Hjh3j6NGjvPvuu/Tu\n3btU36ewsJDZs2fz9ddfk5mZSWZmJvPmzWPmzJnUqlWLtLQ05s6dC0BeXh7Hjh1jwIABzJgx49Tk\nBA1rKvFBYiJkZEjngjlzpKXU0qVStmP6dBFqTz0lYc8nnoCePaFzZxFtTm/Qb7+FgoJIfxNFqRxy\n1TlToodRo0axbt26IuLs+uuvZ/Xq1aSnp/Ovf/2Ldu3aBd3HuHHjyMnJoX379jz88MOnHLguXbrQ\nrVs32rVrx3XXXUevXr1ObTN27FgGDRp0akKAQ/fu3Rk9ejTnnXcePXv25LbbbqNbt26l+i7Lli2j\nWbNmNG3a9NSyiy++mE2bNrFnzx5ee+01nn32WTp37syFF17I3r17GTRoEEOHDiUjI4OuXbvy5JNP\nlupYwTA2jmpTZWRk2JLqoigxxv79MsmgWzeZeHDoEMyfLz1Ct26FunVltijI6/79oUMHmVlav770\nDz3nHCmaq8Qdxpg11tqMSI8jFJTl+nW8VkNezrmGITun0bx5mAemRC2bN2+mffv2kR6GUkr8/V6B\nrmGac6ZENw0bysOhfn0YPVr6f77+uhTLbdkSGjeWsh6LF4O/ujlDhsCkSZCeLg7cnj1w9Ki4bRdf\nDCkplfWNFKXCJJ5Q50xR4hkVZ0psUqWKt3yHw223yXNhoQiv/fsl3PnZZ5LP1q0bJCVJKyo3Z5wh\nIdXzz5d+ok2aSO6bLzk54tKlpYXveylKKUjIz+NkQgo1akR6JIqihAMVZ0r8kZAgYc1atSSsOWCA\niK8XXpAuB61aQbNmMhP0yBFp/v673xXdx+jRMmmhfn3Ja3v9dfjNb+Cnn2DGDChlBWxFCTmFhVQp\nPElCjWQtD6gocYqKM+X0oG5dmfHpjyuugE2bpNRHjRrST/SJJ+S5SRNx4b7/Hs49F846C667DjZu\nlKbwDRpI7tvSpbLswAFx5n79a/jlL7W4rhJ68vIASKiuoXhFiVdUnCkKyCSCDh3k9bnnwjXXwJNP\nSl/RhAR45BFx0/Lz4c47YfJkeTgkJkpv0UaNJPR57bXitt16K9SrJyJt716ZWTpkCFo9VCk3nhpP\nVWokl7CioiixioozRfFHly7StcCXqlXhn/+U+mw//CBO2ZlnwkUXSRgVRMA984x0Spg/v/g+ateG\nu++WYxw4IMIuPV1cue+/h23bxMFr00acu6QkmbCQlBTe76zEBh7nrEotdc4UJV5RcaYoZcUYcdfO\nPdf/51WqSFhz9GjIzJQ8tcJCmXhw/Lg4cn/+s+SylZaqVeGuu0Twqet2euNxzpJrqXOmRJaDBw/S\nr18/APbu3UtiYiINPbPrV65cSdWqVUvcx5gxY5g4cSLnnHNOwHWmTZtG3bp1uf7660My7n379tGs\nWTNeeOEFbnMmkkUZKs4UJVw0aCAPX956C3bulNmfqanihKxbJzNLW7SAs8+W8Of27ZIHl58PGzbI\njNNXX4XLLoPmzaV+28UXyzaffgoLF0pYtV8/2ceBA3Ijb9dOc99CjDFmEPAMkAi8ZK2d4vP5BOA2\nIB/YD9xirQ1eIr20eJyzqrXVOVMiS4MGDVjr6bb+6KOPUrNmTR544IEi61hrsdaSkOC/5v2MGTNK\nPM5dd91V8cG6mD17NhdccAEzZ85UcaYoiosWLYq+91dJ9Lzzir6fMAEefVSayr/9trckSNWqcOKE\n/zIhILNTR42ScOr334tj17GjiLbGjUVApqRIbl316t7w6eHD0iqrQweZ3QqybXa27Os0xRiTCEwD\nBgC7gFXGmPnWWnc35q+ADGvtMWPMOOAvwLWhOP7J7FySgJQ66pwp0cn27dsZOnQo3bp146uvvmLx\n4sU89thjfPnllxw/fpxrr72Whx9+GICLLrqIv//973Tq1InU1FTuvPNOFi5cSPXq1Zk3bx6NGjXi\nD3/4A6mpqdx3331cdNFFXHTRRSxZsoTDhw8zY8YMLrzwQo4ePcpNN93E5s2b6dChA5mZmbz00kun\nmpy7mTlzJs899xwjRoxgz549NPEUKV+wYAF//OMfKSgooHHjxnz44YdkZ2dz991389VXXwEwadIk\nrrzyyrCfQxVnihIrOC2rQMKkW7bAJ5/I8yWXiKN2+DAsWQK7d0vx3vx8ceqmTJFt6tWT58OH/R8j\nIUGEYu3a4tYVFsqyQYOkvtvChZJr17atuHbHj8ts1erVYehQabl11lmVd04iw3nAdmvtDgBjzCxg\nGHBKnFlrl7rW/wK4IVQHzzmYRz2gWj11zhQX990HHhcrZHTtWu6O6lu2bOFf//oXGRlS/H7KlCnU\nr1+f/Px8+vbty4gRI+jgTMLycPjwYS655BKmTJnChAkTePnll5noZ5a9tZaVK1cyf/58Jk2axAcf\nfMBzzz3HGWecwZw5c1i3bh3du3f3O67MzEwOHTpEjx49+OUvf8ns2bMZP348e/fuZdy4cSxbtowW\nLVqc6o/56KOP0rBhQ9avX4+1lp+djjRhRsWZosQiCQlFZ5g61Kgh3RPc3H675L0lJUltN2tFvG3d\nKqHPgwfFeSsslPZY330ny668Ei64QLowvPKKhE4HDJBivytXiuirXVvCqwcOwAMPyMOpLZeeLkKv\nQQOZ9FC3rog63xBrQYEcv1q1sJ6yENIM+MH1fhfQM8j6twILg3xeJrL351IPqF5PnTMlemnTps0p\nYQbiVv3zn/8kPz+f3bt3s2nTpmLirFq1agwePBiAHj16sGzZMr/7vuqqq06tk5mZCcDy5ct58MEH\nAenH2bFjR7/bzpo1i2uvFRN75MiR/OpXv2L8+PF8/vnn9O3blxaeqEZ9T/uNjz766FSjc2MM9Sop\n51fFmaKcDrgvKMZImNIJVZbEoEHS+spaEVmByMyE996TFlpvvCHhT186dxbxeOCAOHzffCOFgEGc\nuQ4dZCbrkCGl/mrRjDHmBiADuCTA52OBsQDNS9kkM+eg5JxVr6/OmeKinA5XuKjhal+xbds2nnnm\nGVauXEndunW54YYbyPVMbHHjnkCQmJhIfn6+330nJyeXuE4gZs6cyYEDB3j11VcB2L17Nzt27CjT\nPioD/xl6iqIobhISggszkB6nd98N8+ZJrbc9e+CLL+CDD2DBAvj73yE5GX77W3j6aQmF3nyz1JCb\nNAn69BHH7ujRyvhGFSELONP1Ps2zrAjGmP7A74Gh1to8fzuy1k631mZYazMaunvIBuHn/Jospxc1\n0nTWrhIbHDlyhFq1alG7dm327NnDokWLQn6MXr16MXv2bAC+/vprNm3aVGydTZs2kZ+fT1ZWFpmZ\nmWRmZvKb3/yGWbNmceGFF7J06VJ27pR5O05Yc8CAAUybNg2QcOpPP/0U8rH7Q50zRVFCT0KClA45\n44yiy++6S2aqNmwo4iw2WQWcbYxphYiykcB17hWMMd2AfwCDrLU/hvLg6WMv4Js+y2nTLpR7VZTw\n0b17dzp06EC7du1o0aIFvXr1Cvkx7rnnHm666SY6dOhw6lGnTp0i68ycOZPhw4cXWXb11Vdz8803\n89BDD/H8888zbNgwrLU0bdqUhQsX8sgjj/CrX/2KTp06kZiYyOOPP87QoUNDPn5fjC1LraUoJyMj\nw65evTrSw1AUpZIwxqyx1maUvGbIj3s5MBUppfGytXayMWYSsNpaO98Y8xGQDuzxbPK9tTboFV2v\nX0pZ2bx5M+3bt4/0MKKC/Px88vPzSUlJYdu2bQwcOJBt27ZRpUr0eFD+fq9A17DoGbWiKEqMYK19\nH3jfZ9nDrtf9K31QinIak5OTQ79+/cjPz8dayz/+8Y+oEmZlJawjr0ihRmNMAfC1Z9US/+pUFEVR\nFOX0pG7duqxZsybSwwgZYRNnISjUeNxaW7x6nKIoiqIoShwTztmapwo1WmtPAE6hxlNYa5daa495\n3n6BzHpSFEVRFKUUxFPeeDxT1t8pnOLMX6HGYIWVfAs1phhjVhtjvjDGhL9XgqIoiqLEECkpKRw8\neFAFWpRjreXgwYOkpJS+NmFUZMsFKNTYwlqbZYxpDSwxxnxtrf3Wz7ZlLuKoKIqiKLFOWloau3bt\nYv/+/ZEeilICKSkppKWVPjgYTnFW1kKNl7gLNVprszzPO4wx/wW6AcXEmbV2OjAdZCp6CMevKIqi\nKFFLUlISrVq1ivQwlDAQzrDmqUKNxpiqSKHG+e4VXIUah7oLNRpj6hljkj2vU4FeuJoKK4qiKIqi\nxCthc86stfnGmLuBRXgLNW50F2oE/grUBN4y0gzZKZnRHviHMaYQEZBTfGZ5KoqiKIqixCVhzTkr\nb6FGa+1nSHVtRVEURVGU04q4at9kjNkP7CxhtVTgQCUMJ9TE6rhBxx4JYnXcULaxt7DWlq5jeJQT\n59cviN2xx+q4IXbHHqvjhrKP3e81LK7EWWkwxqyORC++ihKr4wYdeySI1XFDbI893MTyuYnVscfq\nuCF2xx6r44bQjT2cEwIURVEURVGUMqLiTFEURVEUJYo4HcXZ9EgPoJzE6rhBxx4JYnXcENtjDzex\nfG5ideyxOm6I3bHH6rghRGM/7XLOFEVRFEVRopnT0TlTFEVRFEWJWk4bcWaMGWSM2WqM2W6MmRjp\n8QTDGHOmMWapMWaTMWajMWa8Z3l9Y8xiY8w2z3O9SI/VH8aYRGPMV8aY9zzvWxljVnjO/ZuejhFR\nhzGmrjHmbWPMFmPMZmPMBbFwzo0x93v+nWwwxsw0xqRE6zk3xrxsjPnRGLPBtczvOTbCs57vsN4Y\n0z1yI488sXIN0+tXZIjV6xfoNcwfp4U4M8YkAtOAwUAHYJQxpkNkRxWUfODX1toOwPnAXZ7xTgQ+\nttaeDXzseR+NjAc2u94/ATxtrT0L+Am4NSKjKplngA+ste2ALsh3iOpzboxpBtwLZFhrOyHdOEYS\nvef8FWCQz7JA53gwcLbnMRZ4vpLGGHXE2DVMr1+RIeauX6DXsIBYa+P+AVwALHK9/x3wu0iPqwzj\nnwcMALYCTTzLmgBbIz02P2NN8/zjvBR4DzBIQb4q/n6LaHkAdYDv8ORhupZH9TkHmgE/APWRjh/v\nAZdF8zkHWgIbSjrHSN/dUf7WO90esXwN0+tXpYw7Jq9fnnHpNczP47RwzvD++A67PMuiHmNMS6Ab\nsAJobK3d4/loL9A4QsMKxlTgt0Ch530D4Gdrbb7nfbSe+1bAfmCGJ6TxkjGmBlF+zq21WcCTwPfA\nHuAwsIbYOOcOgc5xzP6/DQMxeS70+lVpxOT1C/QaFojTRZzFJMaYmsAc4D5r7RH3Z1ZkeFRNtTXG\nXAH8aK1dE+mxlIMqQHfgeWttN+AoPiGAKD3n9YBhyMW5KVCD4pZ7zBCN51gpH3r9qlRi8voFeg0L\nxOkizrKAM13v0zzLohZjTBJyYXvdWvuOZ/E+Y0wTz+dNgB8jNb4A9AKGGmMygVlIaOAZoK4xpopn\nnWg997uAXdbaFZ73byMXu2g/5/2B76y1+621J4F3kN8hFs65Q6BzHHP/b8NITJ0LvX5VOrF6/QK9\nhvnldBFnq4CzPbM/qiLJhvMjPKaAGGMM8E9gs7X2KddH84GbPa9vRnI5ogZr7e+stWnW2pbIOV5i\nrb0eWAqM8KwWdeMGsNbuBX4wxpzjWdQP2ESUn3MkFHC+Maa659+NM+6oP+cuAp3j+cBNnhlP5wOH\nXaGD042YuYbp9avyieHrF+g1zD+RTqyrxAS+y4FvgG+B30d6PCWM9SLEFl0PrPU8LkfyHz4GtgEf\nAfUjPdYg36EP8J7ndWtgJbAdeAtIjvT4Aoy5K7Dac97nAvVi4ZwDjwFbgA3Aa0BytJ5zYCaSV3IS\n+Wv/1kDnGEnGnub5P/s1Mpsr4t8hgucuJq5hev2K2Jhj8vrlGbtew3we2iFAURRFURQlijhdwpqK\noiiKoigxgYozRVEURVGUKELFmaIoiqIoShSh4kxRFEVRFCWKUHGmKIqiKIoSRag4UyKOMabAGLPW\n9QhZc17AjFsAAAI9SURBVF5jTEtjzIZQ7U9RFMWNXr+UcFCl5FUUJewct9Z2jfQgFEVRyoFev5SQ\no86ZErUYYzKNMX8xxnxtjFlpjDnLs7ylMWaJMWa9MeZjY0xzz/LGxph3jTHrPI8LPbtKNMa8aIzZ\naIz50BhTzbP+vcaYTZ79zIrQ11QUJQ7R65dSEVScKdFANZ+wwLWuzw5ba9OBvwNTPcueA1611nYG\nXgee9Sx/FvjEWtsF6Su30bP8bGCatbYj8DNwtWf5RKCbZz93huvLKYoS1+j1Swk52iFAiTjGmBxr\nbU0/yzOBS621OzyNlPdaaxsYYw4ATay1Jz3L91hrU40x+4E0a22eax8tgcXW2rM97x8Ekqy1fzLG\nfADkIK1O5lprc8L8VRVFiTP0+qWEA3XOlGjHBnhdFvJcrwvw5lr+Aul71h1YZYzRHExFUUKJXr+U\ncqHiTIl2rnU9f+55/Rkw0vP6emCZ5/XHwDgAY0yiMaZOoJ0aYxKAM621S4EHgTpAsb9+FUVRKoBe\nv5RyoUpbiQaqGWPWut5/YK11pqPXM8asR/56HOVZdg8wwxjzG2A/MMazfDww3RhzK/IX5jhgT4Bj\nJgL/9lwADfCstfbnkH0jRVFOF/T6pYQczTlTohZPzkaGtfZApMeiKIpSFvT6pVQEDWsqiqIoiqJE\nEeqcKYqiKIqiRBHqnCmKoiiKokQRKs4URVEURVGiCBVniqIoiqIoUYSKM0VRFEVRlChCxZmiKIqi\nKEoUoeJMURRFURQlivj/71u0xMCzDhwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI57l8Zt15XM",
        "colab_type": "text"
      },
      "source": [
        "There is a little bit of overfitting as can be seen above. However the accuracy on unseen data has gone up by a small margin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJv6Eicwd962",
        "colab_type": "code",
        "outputId": "9816998c-84a3-4a33-c05f-35527088eadc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's increase the nodes in the hidden layer and increase drop out\n",
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(248, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(248, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.adam(lr = 0.001)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=2048 ,epochs = 200, verbose = 1, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/200\n",
            "33600/33600 [==============================] - 2s 48us/step - loss: 2.4538 - acc: 0.1627 - val_loss: 2.1505 - val_acc: 0.2402\n",
            "Epoch 2/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.8854 - acc: 0.3537 - val_loss: 2.0947 - val_acc: 0.2933\n",
            "Epoch 3/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.5165 - acc: 0.4960 - val_loss: 1.7049 - val_acc: 0.4157\n",
            "Epoch 4/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 1.2971 - acc: 0.5815 - val_loss: 1.4829 - val_acc: 0.4941\n",
            "Epoch 5/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.1596 - acc: 0.6335 - val_loss: 1.3911 - val_acc: 0.5312\n",
            "Epoch 6/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.0795 - acc: 0.6606 - val_loss: 1.3867 - val_acc: 0.5438\n",
            "Epoch 7/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.0177 - acc: 0.6819 - val_loss: 1.2971 - val_acc: 0.5793\n",
            "Epoch 8/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.9711 - acc: 0.6976 - val_loss: 1.1785 - val_acc: 0.6338\n",
            "Epoch 9/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.9185 - acc: 0.7124 - val_loss: 1.0650 - val_acc: 0.6788\n",
            "Epoch 10/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8766 - acc: 0.7264 - val_loss: 1.0219 - val_acc: 0.6781\n",
            "Epoch 11/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.8487 - acc: 0.7317 - val_loss: 1.1056 - val_acc: 0.6519\n",
            "Epoch 12/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8194 - acc: 0.7449 - val_loss: 1.0046 - val_acc: 0.7045\n",
            "Epoch 13/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7940 - acc: 0.7517 - val_loss: 0.9922 - val_acc: 0.7046\n",
            "Epoch 14/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7712 - acc: 0.7581 - val_loss: 1.0003 - val_acc: 0.6864\n",
            "Epoch 15/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7548 - acc: 0.7628 - val_loss: 0.8473 - val_acc: 0.7429\n",
            "Epoch 16/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7296 - acc: 0.7715 - val_loss: 0.9251 - val_acc: 0.6944\n",
            "Epoch 17/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.7178 - acc: 0.7740 - val_loss: 0.8409 - val_acc: 0.7442\n",
            "Epoch 18/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.6979 - acc: 0.7807 - val_loss: 0.9288 - val_acc: 0.7080\n",
            "Epoch 19/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.6894 - acc: 0.7844 - val_loss: 0.8438 - val_acc: 0.7414\n",
            "Epoch 20/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.6711 - acc: 0.7894 - val_loss: 0.7970 - val_acc: 0.7592\n",
            "Epoch 21/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.6620 - acc: 0.7955 - val_loss: 0.7849 - val_acc: 0.7514\n",
            "Epoch 22/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.6477 - acc: 0.7983 - val_loss: 0.8431 - val_acc: 0.7465\n",
            "Epoch 23/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.6429 - acc: 0.7965 - val_loss: 0.7709 - val_acc: 0.7569\n",
            "Epoch 24/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.6353 - acc: 0.8029 - val_loss: 0.7907 - val_acc: 0.7583\n",
            "Epoch 25/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.6271 - acc: 0.8062 - val_loss: 0.7886 - val_acc: 0.7604\n",
            "Epoch 26/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.6038 - acc: 0.8106 - val_loss: 0.8185 - val_acc: 0.7366\n",
            "Epoch 27/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5935 - acc: 0.8123 - val_loss: 0.8344 - val_acc: 0.7402\n",
            "Epoch 28/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5900 - acc: 0.8133 - val_loss: 0.8152 - val_acc: 0.7458\n",
            "Epoch 29/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5889 - acc: 0.8167 - val_loss: 0.6956 - val_acc: 0.7831\n",
            "Epoch 30/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5769 - acc: 0.8204 - val_loss: 0.8075 - val_acc: 0.7504\n",
            "Epoch 31/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5790 - acc: 0.8175 - val_loss: 0.7151 - val_acc: 0.7851\n",
            "Epoch 32/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5585 - acc: 0.8225 - val_loss: 0.8957 - val_acc: 0.7233\n",
            "Epoch 33/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5540 - acc: 0.8269 - val_loss: 0.6880 - val_acc: 0.7876\n",
            "Epoch 34/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5464 - acc: 0.8298 - val_loss: 0.6723 - val_acc: 0.7956\n",
            "Epoch 35/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5446 - acc: 0.8282 - val_loss: 0.7444 - val_acc: 0.7606\n",
            "Epoch 36/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5356 - acc: 0.8325 - val_loss: 0.6508 - val_acc: 0.8074\n",
            "Epoch 37/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5329 - acc: 0.8341 - val_loss: 0.7037 - val_acc: 0.7833\n",
            "Epoch 38/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5263 - acc: 0.8341 - val_loss: 0.7191 - val_acc: 0.7792\n",
            "Epoch 39/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5250 - acc: 0.8346 - val_loss: 0.6604 - val_acc: 0.8018\n",
            "Epoch 40/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5185 - acc: 0.8342 - val_loss: 0.7138 - val_acc: 0.7822\n",
            "Epoch 41/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5097 - acc: 0.8374 - val_loss: 0.6715 - val_acc: 0.7933\n",
            "Epoch 42/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5046 - acc: 0.8390 - val_loss: 0.7018 - val_acc: 0.7846\n",
            "Epoch 43/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5013 - acc: 0.8416 - val_loss: 0.6886 - val_acc: 0.7925\n",
            "Epoch 44/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5073 - acc: 0.8406 - val_loss: 0.6515 - val_acc: 0.7976\n",
            "Epoch 45/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.5011 - acc: 0.8402 - val_loss: 0.6787 - val_acc: 0.7834\n",
            "Epoch 46/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4941 - acc: 0.8444 - val_loss: 0.7317 - val_acc: 0.7727\n",
            "Epoch 47/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4957 - acc: 0.8432 - val_loss: 0.7166 - val_acc: 0.7716\n",
            "Epoch 48/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4821 - acc: 0.8480 - val_loss: 0.7260 - val_acc: 0.7786\n",
            "Epoch 49/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4845 - acc: 0.8444 - val_loss: 0.7112 - val_acc: 0.7727\n",
            "Epoch 50/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4742 - acc: 0.8504 - val_loss: 0.6360 - val_acc: 0.8039\n",
            "Epoch 51/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4706 - acc: 0.8515 - val_loss: 0.6751 - val_acc: 0.7936\n",
            "Epoch 52/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4675 - acc: 0.8518 - val_loss: 0.6623 - val_acc: 0.7913\n",
            "Epoch 53/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4661 - acc: 0.8524 - val_loss: 0.7184 - val_acc: 0.7802\n",
            "Epoch 54/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4642 - acc: 0.8526 - val_loss: 0.6870 - val_acc: 0.7841\n",
            "Epoch 55/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4652 - acc: 0.8525 - val_loss: 0.6509 - val_acc: 0.7944\n",
            "Epoch 56/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4543 - acc: 0.8534 - val_loss: 0.6507 - val_acc: 0.7996\n",
            "Epoch 57/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4539 - acc: 0.8555 - val_loss: 0.6895 - val_acc: 0.7872\n",
            "Epoch 58/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4498 - acc: 0.8571 - val_loss: 0.6750 - val_acc: 0.7846\n",
            "Epoch 59/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4466 - acc: 0.8575 - val_loss: 0.6284 - val_acc: 0.8033\n",
            "Epoch 60/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4418 - acc: 0.8587 - val_loss: 0.6116 - val_acc: 0.8123\n",
            "Epoch 61/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4441 - acc: 0.8577 - val_loss: 0.6341 - val_acc: 0.8132\n",
            "Epoch 62/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4355 - acc: 0.8622 - val_loss: 0.6557 - val_acc: 0.7963\n",
            "Epoch 63/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4316 - acc: 0.8627 - val_loss: 0.7146 - val_acc: 0.7772\n",
            "Epoch 64/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4367 - acc: 0.8603 - val_loss: 0.7003 - val_acc: 0.7797\n",
            "Epoch 65/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4356 - acc: 0.8612 - val_loss: 0.7038 - val_acc: 0.7773\n",
            "Epoch 66/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4301 - acc: 0.8625 - val_loss: 0.6257 - val_acc: 0.8084\n",
            "Epoch 67/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4279 - acc: 0.8633 - val_loss: 0.8087 - val_acc: 0.7606\n",
            "Epoch 68/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4280 - acc: 0.8639 - val_loss: 0.7025 - val_acc: 0.7849\n",
            "Epoch 69/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4182 - acc: 0.8659 - val_loss: 0.6713 - val_acc: 0.7903\n",
            "Epoch 70/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4255 - acc: 0.8644 - val_loss: 0.6419 - val_acc: 0.8047\n",
            "Epoch 71/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4146 - acc: 0.8685 - val_loss: 0.6988 - val_acc: 0.7785\n",
            "Epoch 72/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4159 - acc: 0.8673 - val_loss: 0.6914 - val_acc: 0.7869\n",
            "Epoch 73/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4150 - acc: 0.8657 - val_loss: 0.8263 - val_acc: 0.7493\n",
            "Epoch 74/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4055 - acc: 0.8704 - val_loss: 0.7449 - val_acc: 0.7738\n",
            "Epoch 75/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4087 - acc: 0.8672 - val_loss: 0.6794 - val_acc: 0.7857\n",
            "Epoch 76/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4109 - acc: 0.8684 - val_loss: 0.5928 - val_acc: 0.8191\n",
            "Epoch 77/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.4006 - acc: 0.8734 - val_loss: 0.6697 - val_acc: 0.7978\n",
            "Epoch 78/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3996 - acc: 0.8708 - val_loss: 0.6750 - val_acc: 0.7879\n",
            "Epoch 79/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3920 - acc: 0.8732 - val_loss: 0.5729 - val_acc: 0.8266\n",
            "Epoch 80/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3983 - acc: 0.8729 - val_loss: 0.6580 - val_acc: 0.7917\n",
            "Epoch 81/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3986 - acc: 0.8715 - val_loss: 0.6265 - val_acc: 0.8123\n",
            "Epoch 82/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3861 - acc: 0.8754 - val_loss: 0.7004 - val_acc: 0.7914\n",
            "Epoch 83/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3896 - acc: 0.8746 - val_loss: 0.7136 - val_acc: 0.7831\n",
            "Epoch 84/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3880 - acc: 0.8773 - val_loss: 0.6334 - val_acc: 0.8065\n",
            "Epoch 85/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3911 - acc: 0.8731 - val_loss: 0.6016 - val_acc: 0.8157\n",
            "Epoch 86/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3934 - acc: 0.8743 - val_loss: 0.7681 - val_acc: 0.7791\n",
            "Epoch 87/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3950 - acc: 0.8726 - val_loss: 0.8198 - val_acc: 0.7476\n",
            "Epoch 88/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3883 - acc: 0.8746 - val_loss: 0.6118 - val_acc: 0.8131\n",
            "Epoch 89/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3813 - acc: 0.8773 - val_loss: 0.8452 - val_acc: 0.7635\n",
            "Epoch 90/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3816 - acc: 0.8781 - val_loss: 0.6055 - val_acc: 0.8142\n",
            "Epoch 91/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3780 - acc: 0.8771 - val_loss: 0.7336 - val_acc: 0.7751\n",
            "Epoch 92/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3813 - acc: 0.8791 - val_loss: 0.7323 - val_acc: 0.7917\n",
            "Epoch 93/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3849 - acc: 0.8750 - val_loss: 0.7266 - val_acc: 0.7696\n",
            "Epoch 94/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3771 - acc: 0.8797 - val_loss: 0.6789 - val_acc: 0.7911\n",
            "Epoch 95/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3761 - acc: 0.8790 - val_loss: 0.6825 - val_acc: 0.7969\n",
            "Epoch 96/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3676 - acc: 0.8814 - val_loss: 0.6898 - val_acc: 0.7836\n",
            "Epoch 97/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3735 - acc: 0.8796 - val_loss: 0.6846 - val_acc: 0.7885\n",
            "Epoch 98/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3627 - acc: 0.8842 - val_loss: 0.6887 - val_acc: 0.7917\n",
            "Epoch 99/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3674 - acc: 0.8807 - val_loss: 0.7353 - val_acc: 0.7747\n",
            "Epoch 100/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3645 - acc: 0.8830 - val_loss: 0.8142 - val_acc: 0.7674\n",
            "Epoch 101/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3534 - acc: 0.8874 - val_loss: 0.6400 - val_acc: 0.8074\n",
            "Epoch 102/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3683 - acc: 0.8812 - val_loss: 0.7255 - val_acc: 0.7831\n",
            "Epoch 103/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3607 - acc: 0.8839 - val_loss: 0.6510 - val_acc: 0.8003\n",
            "Epoch 104/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3664 - acc: 0.8830 - val_loss: 0.6378 - val_acc: 0.8014\n",
            "Epoch 105/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3592 - acc: 0.8851 - val_loss: 0.7523 - val_acc: 0.7712\n",
            "Epoch 106/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3600 - acc: 0.8846 - val_loss: 0.6869 - val_acc: 0.7952\n",
            "Epoch 107/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3566 - acc: 0.8846 - val_loss: 0.5920 - val_acc: 0.8136\n",
            "Epoch 108/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3513 - acc: 0.8853 - val_loss: 0.6289 - val_acc: 0.8064\n",
            "Epoch 109/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3498 - acc: 0.8877 - val_loss: 0.7985 - val_acc: 0.7647\n",
            "Epoch 110/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3520 - acc: 0.8849 - val_loss: 0.8468 - val_acc: 0.7572\n",
            "Epoch 111/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3467 - acc: 0.8877 - val_loss: 0.5962 - val_acc: 0.8221\n",
            "Epoch 112/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3473 - acc: 0.8901 - val_loss: 0.7034 - val_acc: 0.7969\n",
            "Epoch 113/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3485 - acc: 0.8858 - val_loss: 0.6996 - val_acc: 0.7925\n",
            "Epoch 114/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3514 - acc: 0.8865 - val_loss: 0.6079 - val_acc: 0.8142\n",
            "Epoch 115/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3488 - acc: 0.8875 - val_loss: 0.7565 - val_acc: 0.7722\n",
            "Epoch 116/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3435 - acc: 0.8882 - val_loss: 0.6569 - val_acc: 0.7981\n",
            "Epoch 117/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3416 - acc: 0.8898 - val_loss: 0.8037 - val_acc: 0.7586\n",
            "Epoch 118/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3389 - acc: 0.8901 - val_loss: 0.5906 - val_acc: 0.8185\n",
            "Epoch 119/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3371 - acc: 0.8907 - val_loss: 0.5675 - val_acc: 0.8306\n",
            "Epoch 120/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3397 - acc: 0.8913 - val_loss: 0.6214 - val_acc: 0.8122\n",
            "Epoch 121/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3372 - acc: 0.8922 - val_loss: 0.7902 - val_acc: 0.7723\n",
            "Epoch 122/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3381 - acc: 0.8904 - val_loss: 0.5915 - val_acc: 0.8183\n",
            "Epoch 123/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3336 - acc: 0.8915 - val_loss: 0.6369 - val_acc: 0.8054\n",
            "Epoch 124/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3374 - acc: 0.8905 - val_loss: 0.6157 - val_acc: 0.8158\n",
            "Epoch 125/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3295 - acc: 0.8955 - val_loss: 0.7350 - val_acc: 0.7927\n",
            "Epoch 126/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3327 - acc: 0.8924 - val_loss: 0.6661 - val_acc: 0.8029\n",
            "Epoch 127/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3335 - acc: 0.8911 - val_loss: 0.9108 - val_acc: 0.7293\n",
            "Epoch 128/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3285 - acc: 0.8937 - val_loss: 0.8108 - val_acc: 0.7729\n",
            "Epoch 129/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3258 - acc: 0.8948 - val_loss: 0.6992 - val_acc: 0.7876\n",
            "Epoch 130/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3259 - acc: 0.8934 - val_loss: 0.6147 - val_acc: 0.8113\n",
            "Epoch 131/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3208 - acc: 0.8974 - val_loss: 0.6465 - val_acc: 0.8009\n",
            "Epoch 132/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3268 - acc: 0.8953 - val_loss: 0.7187 - val_acc: 0.7876\n",
            "Epoch 133/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3331 - acc: 0.8924 - val_loss: 0.9748 - val_acc: 0.7056\n",
            "Epoch 134/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3301 - acc: 0.8925 - val_loss: 0.7942 - val_acc: 0.7679\n",
            "Epoch 135/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3321 - acc: 0.8910 - val_loss: 0.6744 - val_acc: 0.8033\n",
            "Epoch 136/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3262 - acc: 0.8939 - val_loss: 0.6943 - val_acc: 0.7928\n",
            "Epoch 137/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3193 - acc: 0.8971 - val_loss: 0.6891 - val_acc: 0.8000\n",
            "Epoch 138/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3242 - acc: 0.8934 - val_loss: 0.6216 - val_acc: 0.8083\n",
            "Epoch 139/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3191 - acc: 0.8959 - val_loss: 0.6582 - val_acc: 0.8010\n",
            "Epoch 140/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3193 - acc: 0.8972 - val_loss: 0.6019 - val_acc: 0.8171\n",
            "Epoch 141/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3161 - acc: 0.8984 - val_loss: 0.7430 - val_acc: 0.7742\n",
            "Epoch 142/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3096 - acc: 0.8998 - val_loss: 0.5636 - val_acc: 0.8315\n",
            "Epoch 143/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3146 - acc: 0.8972 - val_loss: 0.6290 - val_acc: 0.8131\n",
            "Epoch 144/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3112 - acc: 0.8979 - val_loss: 0.7312 - val_acc: 0.7852\n",
            "Epoch 145/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3066 - acc: 0.9004 - val_loss: 0.7091 - val_acc: 0.7941\n",
            "Epoch 146/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3165 - acc: 0.8959 - val_loss: 0.6605 - val_acc: 0.7954\n",
            "Epoch 147/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3103 - acc: 0.8989 - val_loss: 0.7508 - val_acc: 0.7762\n",
            "Epoch 148/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3080 - acc: 0.9000 - val_loss: 0.8288 - val_acc: 0.7483\n",
            "Epoch 149/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3130 - acc: 0.8979 - val_loss: 0.6257 - val_acc: 0.8142\n",
            "Epoch 150/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3055 - acc: 0.9004 - val_loss: 0.6882 - val_acc: 0.7964\n",
            "Epoch 151/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3097 - acc: 0.8980 - val_loss: 0.6849 - val_acc: 0.7937\n",
            "Epoch 152/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3016 - acc: 0.9017 - val_loss: 0.6024 - val_acc: 0.8217\n",
            "Epoch 153/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2999 - acc: 0.9024 - val_loss: 0.5810 - val_acc: 0.8261\n",
            "Epoch 154/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3097 - acc: 0.8980 - val_loss: 0.6262 - val_acc: 0.8156\n",
            "Epoch 155/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3028 - acc: 0.9004 - val_loss: 0.7048 - val_acc: 0.7985\n",
            "Epoch 156/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2991 - acc: 0.9028 - val_loss: 0.5824 - val_acc: 0.8280\n",
            "Epoch 157/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2992 - acc: 0.9021 - val_loss: 0.6240 - val_acc: 0.8188\n",
            "Epoch 158/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3021 - acc: 0.8997 - val_loss: 0.7411 - val_acc: 0.7752\n",
            "Epoch 159/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3111 - acc: 0.8986 - val_loss: 0.6145 - val_acc: 0.8216\n",
            "Epoch 160/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3054 - acc: 0.9010 - val_loss: 0.7515 - val_acc: 0.7797\n",
            "Epoch 161/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3059 - acc: 0.9003 - val_loss: 0.7055 - val_acc: 0.7938\n",
            "Epoch 162/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2987 - acc: 0.9029 - val_loss: 0.6391 - val_acc: 0.8121\n",
            "Epoch 163/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3001 - acc: 0.9029 - val_loss: 0.7979 - val_acc: 0.7643\n",
            "Epoch 164/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.3000 - acc: 0.9018 - val_loss: 0.5592 - val_acc: 0.8361\n",
            "Epoch 165/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2926 - acc: 0.9051 - val_loss: 0.5993 - val_acc: 0.8253\n",
            "Epoch 166/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2949 - acc: 0.9024 - val_loss: 0.6239 - val_acc: 0.8108\n",
            "Epoch 167/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2969 - acc: 0.9020 - val_loss: 0.5810 - val_acc: 0.8266\n",
            "Epoch 168/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2884 - acc: 0.9061 - val_loss: 0.6044 - val_acc: 0.8214\n",
            "Epoch 169/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2894 - acc: 0.9066 - val_loss: 0.5580 - val_acc: 0.8356\n",
            "Epoch 170/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2968 - acc: 0.9044 - val_loss: 0.7467 - val_acc: 0.7876\n",
            "Epoch 171/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2900 - acc: 0.9063 - val_loss: 0.6136 - val_acc: 0.8187\n",
            "Epoch 172/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2898 - acc: 0.9062 - val_loss: 0.5565 - val_acc: 0.8370\n",
            "Epoch 173/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2916 - acc: 0.9044 - val_loss: 0.5807 - val_acc: 0.8258\n",
            "Epoch 174/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2913 - acc: 0.9048 - val_loss: 0.5582 - val_acc: 0.8356\n",
            "Epoch 175/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2855 - acc: 0.9066 - val_loss: 0.7847 - val_acc: 0.7791\n",
            "Epoch 176/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2891 - acc: 0.9061 - val_loss: 0.6019 - val_acc: 0.8214\n",
            "Epoch 177/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2876 - acc: 0.9080 - val_loss: 0.6077 - val_acc: 0.8176\n",
            "Epoch 178/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2860 - acc: 0.9058 - val_loss: 0.6105 - val_acc: 0.8229\n",
            "Epoch 179/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2838 - acc: 0.9083 - val_loss: 0.6644 - val_acc: 0.7968\n",
            "Epoch 180/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2802 - acc: 0.9074 - val_loss: 0.6739 - val_acc: 0.7984\n",
            "Epoch 181/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2822 - acc: 0.9065 - val_loss: 0.6496 - val_acc: 0.8181\n",
            "Epoch 182/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2856 - acc: 0.9049 - val_loss: 0.6759 - val_acc: 0.7965\n",
            "Epoch 183/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2909 - acc: 0.9042 - val_loss: 0.6310 - val_acc: 0.8164\n",
            "Epoch 184/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2869 - acc: 0.9075 - val_loss: 0.7400 - val_acc: 0.7811\n",
            "Epoch 185/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2802 - acc: 0.9093 - val_loss: 0.5991 - val_acc: 0.8293\n",
            "Epoch 186/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2790 - acc: 0.9093 - val_loss: 0.6581 - val_acc: 0.8104\n",
            "Epoch 187/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2778 - acc: 0.9088 - val_loss: 0.6532 - val_acc: 0.8114\n",
            "Epoch 188/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2786 - acc: 0.9099 - val_loss: 0.6124 - val_acc: 0.8190\n",
            "Epoch 189/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2776 - acc: 0.9105 - val_loss: 0.5914 - val_acc: 0.8282\n",
            "Epoch 190/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2686 - acc: 0.9125 - val_loss: 0.7917 - val_acc: 0.7717\n",
            "Epoch 191/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2788 - acc: 0.9090 - val_loss: 0.7011 - val_acc: 0.8009\n",
            "Epoch 192/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2789 - acc: 0.9100 - val_loss: 0.7008 - val_acc: 0.7928\n",
            "Epoch 193/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2878 - acc: 0.9068 - val_loss: 0.7268 - val_acc: 0.7941\n",
            "Epoch 194/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2812 - acc: 0.9093 - val_loss: 0.6965 - val_acc: 0.8006\n",
            "Epoch 195/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2787 - acc: 0.9087 - val_loss: 0.8036 - val_acc: 0.7815\n",
            "Epoch 196/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2821 - acc: 0.9064 - val_loss: 0.5804 - val_acc: 0.8329\n",
            "Epoch 197/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2827 - acc: 0.9076 - val_loss: 0.6194 - val_acc: 0.8182\n",
            "Epoch 198/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2851 - acc: 0.9076 - val_loss: 0.7612 - val_acc: 0.7627\n",
            "Epoch 199/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2831 - acc: 0.9068 - val_loss: 0.6389 - val_acc: 0.8154\n",
            "Epoch 200/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.2686 - acc: 0.9125 - val_loss: 0.6255 - val_acc: 0.8209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1ZVuZo6qegI",
        "colab_type": "code",
        "outputId": "c3507d77-10cb-4334-9326-1161c35d179d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "results = model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 0s 58us/step\n",
            "Test accuracy:  0.8201190476190476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1rUsKnA2JE6",
        "colab_type": "code",
        "outputId": "4779daa7-1300-4b20-898a-9fbbf4e5e3fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "NN7_test_acc=0.82011\n",
        "NN7_val_loss = history.history['val_loss']\n",
        "NN7_train_loss = history.history['loss']\n",
        "NN7_val_acc = history.history['val_acc']\n",
        "NN7_train_acc = history.history['acc']\n",
        "\n",
        "\n",
        "epochs = range(1,201)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, NN7_val_loss, 'b', label='Validation Loss')\n",
        "plt.plot(epochs, NN7_train_loss, 'r', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, NN7_val_acc, 'b', label='Validation Acc')\n",
        "plt.plot(epochs, NN7_train_acc, 'r', label='Training Acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe2f61fd630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAE9CAYAAABDUbVaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCYIgCIIgOAERYYIgCIIgCE7g/wNfxg0KOBNvmQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kXZZynZ21Uj",
        "colab_type": "text"
      },
      "source": [
        "Test accuracy improved. A small amount of overfitting is still seen but not too much to be worried. Further finetuning to see if accuracy can be improved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YUqBS9ttbIH",
        "colab_type": "code",
        "outputId": "1e9cf75f-7ee4-4e20-bd89-45b06fda1d9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's increase the nodes in the hidden layer\n",
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(248*2, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(248*2, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.adam(lr = 0.001)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=2048 ,epochs = 200, verbose = 1, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/200\n",
            "33600/33600 [==============================] - 2s 59us/step - loss: 2.3244 - acc: 0.2242 - val_loss: 2.3511 - val_acc: 0.2321\n",
            "Epoch 2/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 1.6101 - acc: 0.4607 - val_loss: 1.7445 - val_acc: 0.3904\n",
            "Epoch 3/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.2610 - acc: 0.5931 - val_loss: 1.5503 - val_acc: 0.4767\n",
            "Epoch 4/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 1.0913 - acc: 0.6551 - val_loss: 1.3804 - val_acc: 0.5483\n",
            "Epoch 5/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.9870 - acc: 0.6901 - val_loss: 1.3661 - val_acc: 0.5634\n",
            "Epoch 6/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.9133 - acc: 0.7153 - val_loss: 1.4443 - val_acc: 0.5245\n",
            "Epoch 7/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.8626 - acc: 0.7326 - val_loss: 1.0689 - val_acc: 0.6595\n",
            "Epoch 8/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8150 - acc: 0.7487 - val_loss: 1.0852 - val_acc: 0.6588\n",
            "Epoch 9/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7701 - acc: 0.7614 - val_loss: 0.9093 - val_acc: 0.7362\n",
            "Epoch 10/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7438 - acc: 0.7699 - val_loss: 1.1757 - val_acc: 0.6276\n",
            "Epoch 11/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.7067 - acc: 0.7812 - val_loss: 1.0442 - val_acc: 0.6750\n",
            "Epoch 12/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.6790 - acc: 0.7881 - val_loss: 1.0192 - val_acc: 0.6689\n",
            "Epoch 13/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.6721 - acc: 0.7904 - val_loss: 0.9252 - val_acc: 0.7167\n",
            "Epoch 14/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.6419 - acc: 0.7985 - val_loss: 0.9332 - val_acc: 0.7113\n",
            "Epoch 15/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.6222 - acc: 0.8047 - val_loss: 0.8211 - val_acc: 0.7413\n",
            "Epoch 16/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.6089 - acc: 0.8094 - val_loss: 0.9120 - val_acc: 0.7134\n",
            "Epoch 17/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5875 - acc: 0.8174 - val_loss: 0.7758 - val_acc: 0.7626\n",
            "Epoch 18/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5775 - acc: 0.8216 - val_loss: 0.9955 - val_acc: 0.6929\n",
            "Epoch 19/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5615 - acc: 0.8252 - val_loss: 0.8551 - val_acc: 0.7273\n",
            "Epoch 20/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5512 - acc: 0.8273 - val_loss: 0.7885 - val_acc: 0.7606\n",
            "Epoch 21/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5355 - acc: 0.8331 - val_loss: 0.8273 - val_acc: 0.7416\n",
            "Epoch 22/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5253 - acc: 0.8340 - val_loss: 0.7705 - val_acc: 0.7641\n",
            "Epoch 23/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5241 - acc: 0.8362 - val_loss: 0.8367 - val_acc: 0.7344\n",
            "Epoch 24/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.5078 - acc: 0.8401 - val_loss: 0.7497 - val_acc: 0.7498\n",
            "Epoch 25/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4942 - acc: 0.8444 - val_loss: 0.7379 - val_acc: 0.7691\n",
            "Epoch 26/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4936 - acc: 0.8440 - val_loss: 1.0819 - val_acc: 0.6654\n",
            "Epoch 27/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4863 - acc: 0.8458 - val_loss: 0.8046 - val_acc: 0.7475\n",
            "Epoch 28/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4786 - acc: 0.8513 - val_loss: 0.7558 - val_acc: 0.7643\n",
            "Epoch 29/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4750 - acc: 0.8477 - val_loss: 0.6606 - val_acc: 0.8025\n",
            "Epoch 30/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4531 - acc: 0.8579 - val_loss: 0.7304 - val_acc: 0.7755\n",
            "Epoch 31/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4534 - acc: 0.8554 - val_loss: 0.8079 - val_acc: 0.7416\n",
            "Epoch 32/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4554 - acc: 0.8555 - val_loss: 0.7361 - val_acc: 0.7722\n",
            "Epoch 33/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4428 - acc: 0.8598 - val_loss: 0.6317 - val_acc: 0.8134\n",
            "Epoch 34/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4440 - acc: 0.8575 - val_loss: 0.8794 - val_acc: 0.7298\n",
            "Epoch 35/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4279 - acc: 0.8658 - val_loss: 0.7098 - val_acc: 0.7857\n",
            "Epoch 36/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4201 - acc: 0.8665 - val_loss: 0.8128 - val_acc: 0.7493\n",
            "Epoch 37/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4152 - acc: 0.8687 - val_loss: 0.8200 - val_acc: 0.7470\n",
            "Epoch 38/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.4043 - acc: 0.8715 - val_loss: 0.6924 - val_acc: 0.7871\n",
            "Epoch 39/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3957 - acc: 0.8743 - val_loss: 0.7502 - val_acc: 0.7682\n",
            "Epoch 40/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3929 - acc: 0.8748 - val_loss: 0.6684 - val_acc: 0.7987\n",
            "Epoch 41/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3931 - acc: 0.8748 - val_loss: 0.7069 - val_acc: 0.7857\n",
            "Epoch 42/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3884 - acc: 0.8760 - val_loss: 0.6386 - val_acc: 0.8094\n",
            "Epoch 43/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3852 - acc: 0.8775 - val_loss: 0.6318 - val_acc: 0.8124\n",
            "Epoch 44/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3739 - acc: 0.8800 - val_loss: 0.6838 - val_acc: 0.7888\n",
            "Epoch 45/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3726 - acc: 0.8800 - val_loss: 0.7706 - val_acc: 0.7648\n",
            "Epoch 46/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3685 - acc: 0.8835 - val_loss: 0.6922 - val_acc: 0.7847\n",
            "Epoch 47/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3683 - acc: 0.8833 - val_loss: 0.6181 - val_acc: 0.8133\n",
            "Epoch 48/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3556 - acc: 0.8846 - val_loss: 0.6964 - val_acc: 0.7846\n",
            "Epoch 49/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3582 - acc: 0.8854 - val_loss: 0.6442 - val_acc: 0.8028\n",
            "Epoch 50/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3514 - acc: 0.8874 - val_loss: 0.6227 - val_acc: 0.8132\n",
            "Epoch 51/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3488 - acc: 0.8888 - val_loss: 0.6703 - val_acc: 0.7947\n",
            "Epoch 52/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3470 - acc: 0.8896 - val_loss: 0.7564 - val_acc: 0.7596\n",
            "Epoch 53/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3424 - acc: 0.8896 - val_loss: 0.7532 - val_acc: 0.7627\n",
            "Epoch 54/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3455 - acc: 0.8875 - val_loss: 0.8180 - val_acc: 0.7544\n",
            "Epoch 55/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3337 - acc: 0.8917 - val_loss: 0.7945 - val_acc: 0.7396\n",
            "Epoch 56/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3266 - acc: 0.8938 - val_loss: 0.7440 - val_acc: 0.7824\n",
            "Epoch 57/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3315 - acc: 0.8934 - val_loss: 0.6677 - val_acc: 0.8012\n",
            "Epoch 58/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3282 - acc: 0.8931 - val_loss: 0.8390 - val_acc: 0.7329\n",
            "Epoch 59/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3214 - acc: 0.8965 - val_loss: 0.6028 - val_acc: 0.8246\n",
            "Epoch 60/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3150 - acc: 0.8989 - val_loss: 0.6231 - val_acc: 0.8115\n",
            "Epoch 61/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3074 - acc: 0.9004 - val_loss: 0.8337 - val_acc: 0.7461\n",
            "Epoch 62/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3114 - acc: 0.9016 - val_loss: 0.8769 - val_acc: 0.7344\n",
            "Epoch 63/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3111 - acc: 0.8987 - val_loss: 0.8945 - val_acc: 0.7394\n",
            "Epoch 64/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3033 - acc: 0.9027 - val_loss: 0.6057 - val_acc: 0.8174\n",
            "Epoch 65/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3025 - acc: 0.9040 - val_loss: 0.8382 - val_acc: 0.7644\n",
            "Epoch 66/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.3076 - acc: 0.9006 - val_loss: 0.6946 - val_acc: 0.7950\n",
            "Epoch 67/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2978 - acc: 0.9043 - val_loss: 0.8013 - val_acc: 0.7504\n",
            "Epoch 68/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2998 - acc: 0.9031 - val_loss: 0.7711 - val_acc: 0.7692\n",
            "Epoch 69/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2853 - acc: 0.9069 - val_loss: 0.6401 - val_acc: 0.8086\n",
            "Epoch 70/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2883 - acc: 0.9073 - val_loss: 0.7519 - val_acc: 0.7734\n",
            "Epoch 71/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2815 - acc: 0.9095 - val_loss: 0.9022 - val_acc: 0.7472\n",
            "Epoch 72/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2877 - acc: 0.9068 - val_loss: 0.7744 - val_acc: 0.7587\n",
            "Epoch 73/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2808 - acc: 0.9080 - val_loss: 0.7082 - val_acc: 0.7882\n",
            "Epoch 74/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2785 - acc: 0.9090 - val_loss: 0.6490 - val_acc: 0.8062\n",
            "Epoch 75/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2764 - acc: 0.9089 - val_loss: 0.7114 - val_acc: 0.7853\n",
            "Epoch 76/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2774 - acc: 0.9089 - val_loss: 0.6396 - val_acc: 0.8076\n",
            "Epoch 77/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2742 - acc: 0.9107 - val_loss: 0.6566 - val_acc: 0.7992\n",
            "Epoch 78/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2713 - acc: 0.9121 - val_loss: 0.8958 - val_acc: 0.7422\n",
            "Epoch 79/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2741 - acc: 0.9106 - val_loss: 0.7705 - val_acc: 0.7770\n",
            "Epoch 80/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2618 - acc: 0.9150 - val_loss: 0.7625 - val_acc: 0.7787\n",
            "Epoch 81/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2584 - acc: 0.9159 - val_loss: 0.6264 - val_acc: 0.8115\n",
            "Epoch 82/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2580 - acc: 0.9172 - val_loss: 0.6811 - val_acc: 0.7906\n",
            "Epoch 83/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2626 - acc: 0.9157 - val_loss: 0.6167 - val_acc: 0.8109\n",
            "Epoch 84/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2551 - acc: 0.9168 - val_loss: 0.6379 - val_acc: 0.8157\n",
            "Epoch 85/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2592 - acc: 0.9169 - val_loss: 0.7417 - val_acc: 0.7878\n",
            "Epoch 86/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2550 - acc: 0.9160 - val_loss: 0.7093 - val_acc: 0.7938\n",
            "Epoch 87/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2520 - acc: 0.9196 - val_loss: 0.7162 - val_acc: 0.7791\n",
            "Epoch 88/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2445 - acc: 0.9210 - val_loss: 0.6949 - val_acc: 0.7875\n",
            "Epoch 89/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2455 - acc: 0.9196 - val_loss: 0.6586 - val_acc: 0.8064\n",
            "Epoch 90/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2383 - acc: 0.9234 - val_loss: 0.7353 - val_acc: 0.7751\n",
            "Epoch 91/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2402 - acc: 0.9218 - val_loss: 0.5680 - val_acc: 0.8398\n",
            "Epoch 92/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2338 - acc: 0.9247 - val_loss: 0.6682 - val_acc: 0.8013\n",
            "Epoch 93/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2355 - acc: 0.9232 - val_loss: 0.7011 - val_acc: 0.7962\n",
            "Epoch 94/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2385 - acc: 0.9232 - val_loss: 0.7186 - val_acc: 0.7969\n",
            "Epoch 95/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2398 - acc: 0.9218 - val_loss: 0.8466 - val_acc: 0.7639\n",
            "Epoch 96/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2416 - acc: 0.9215 - val_loss: 0.7598 - val_acc: 0.7783\n",
            "Epoch 97/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2356 - acc: 0.9227 - val_loss: 0.8378 - val_acc: 0.7549\n",
            "Epoch 98/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2358 - acc: 0.9241 - val_loss: 0.7239 - val_acc: 0.7891\n",
            "Epoch 99/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2326 - acc: 0.9243 - val_loss: 0.5964 - val_acc: 0.8231\n",
            "Epoch 100/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2255 - acc: 0.9265 - val_loss: 0.6351 - val_acc: 0.8163\n",
            "Epoch 101/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2296 - acc: 0.9246 - val_loss: 0.7389 - val_acc: 0.7886\n",
            "Epoch 102/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2243 - acc: 0.9270 - val_loss: 0.8877 - val_acc: 0.7651\n",
            "Epoch 103/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2304 - acc: 0.9252 - val_loss: 0.6967 - val_acc: 0.7944\n",
            "Epoch 104/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2174 - acc: 0.9292 - val_loss: 0.8033 - val_acc: 0.7861\n",
            "Epoch 105/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2189 - acc: 0.9267 - val_loss: 0.7216 - val_acc: 0.7957\n",
            "Epoch 106/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2173 - acc: 0.9282 - val_loss: 0.7389 - val_acc: 0.7806\n",
            "Epoch 107/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2182 - acc: 0.9293 - val_loss: 0.8417 - val_acc: 0.7688\n",
            "Epoch 108/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2124 - acc: 0.9313 - val_loss: 1.0333 - val_acc: 0.7411\n",
            "Epoch 109/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2111 - acc: 0.9311 - val_loss: 0.6695 - val_acc: 0.8054\n",
            "Epoch 110/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2099 - acc: 0.9313 - val_loss: 0.6443 - val_acc: 0.8084\n",
            "Epoch 111/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2105 - acc: 0.9312 - val_loss: 0.7925 - val_acc: 0.7683\n",
            "Epoch 112/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2148 - acc: 0.9293 - val_loss: 0.5814 - val_acc: 0.8286\n",
            "Epoch 113/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2120 - acc: 0.9314 - val_loss: 0.6460 - val_acc: 0.8124\n",
            "Epoch 114/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2039 - acc: 0.9332 - val_loss: 0.7385 - val_acc: 0.7996\n",
            "Epoch 115/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2090 - acc: 0.9313 - val_loss: 0.6993 - val_acc: 0.8019\n",
            "Epoch 116/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2074 - acc: 0.9318 - val_loss: 0.7275 - val_acc: 0.7956\n",
            "Epoch 117/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1987 - acc: 0.9359 - val_loss: 0.6382 - val_acc: 0.8102\n",
            "Epoch 118/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2022 - acc: 0.9346 - val_loss: 0.6172 - val_acc: 0.8224\n",
            "Epoch 119/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1986 - acc: 0.9354 - val_loss: 0.7519 - val_acc: 0.7888\n",
            "Epoch 120/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2003 - acc: 0.9352 - val_loss: 0.6797 - val_acc: 0.7946\n",
            "Epoch 121/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1943 - acc: 0.9359 - val_loss: 0.7275 - val_acc: 0.7963\n",
            "Epoch 122/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1972 - acc: 0.9355 - val_loss: 0.7435 - val_acc: 0.7976\n",
            "Epoch 123/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1985 - acc: 0.9361 - val_loss: 0.6569 - val_acc: 0.8119\n",
            "Epoch 124/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1957 - acc: 0.9347 - val_loss: 0.8275 - val_acc: 0.7773\n",
            "Epoch 125/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.2002 - acc: 0.9349 - val_loss: 0.6439 - val_acc: 0.8109\n",
            "Epoch 126/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1888 - acc: 0.9393 - val_loss: 0.6498 - val_acc: 0.8128\n",
            "Epoch 127/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1926 - acc: 0.9369 - val_loss: 0.7789 - val_acc: 0.7835\n",
            "Epoch 128/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1909 - acc: 0.9372 - val_loss: 0.7577 - val_acc: 0.7928\n",
            "Epoch 129/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1949 - acc: 0.9359 - val_loss: 0.7628 - val_acc: 0.7858\n",
            "Epoch 130/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1895 - acc: 0.9382 - val_loss: 0.7252 - val_acc: 0.7978\n",
            "Epoch 131/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1914 - acc: 0.9372 - val_loss: 0.7284 - val_acc: 0.7985\n",
            "Epoch 132/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1940 - acc: 0.9347 - val_loss: 0.7939 - val_acc: 0.7783\n",
            "Epoch 133/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1905 - acc: 0.9376 - val_loss: 0.6369 - val_acc: 0.8151\n",
            "Epoch 134/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1943 - acc: 0.9361 - val_loss: 0.9145 - val_acc: 0.7563\n",
            "Epoch 135/200\n",
            "33600/33600 [==============================] - 0s 7us/step - loss: 0.1883 - acc: 0.9380 - val_loss: 0.7478 - val_acc: 0.8029\n",
            "Epoch 136/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1815 - acc: 0.9401 - val_loss: 0.7711 - val_acc: 0.7886\n",
            "Epoch 137/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1747 - acc: 0.9432 - val_loss: 0.7279 - val_acc: 0.7941\n",
            "Epoch 138/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1781 - acc: 0.9403 - val_loss: 1.0322 - val_acc: 0.7355\n",
            "Epoch 139/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1781 - acc: 0.9434 - val_loss: 0.7263 - val_acc: 0.7929\n",
            "Epoch 140/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1752 - acc: 0.9431 - val_loss: 0.7095 - val_acc: 0.8006\n",
            "Epoch 141/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1704 - acc: 0.9438 - val_loss: 0.8130 - val_acc: 0.7878\n",
            "Epoch 142/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1735 - acc: 0.9447 - val_loss: 0.6444 - val_acc: 0.8279\n",
            "Epoch 143/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1712 - acc: 0.9435 - val_loss: 0.7776 - val_acc: 0.7929\n",
            "Epoch 144/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1719 - acc: 0.9444 - val_loss: 0.8085 - val_acc: 0.7953\n",
            "Epoch 145/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1701 - acc: 0.9441 - val_loss: 0.6448 - val_acc: 0.8182\n",
            "Epoch 146/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1677 - acc: 0.9452 - val_loss: 0.7038 - val_acc: 0.8084\n",
            "Epoch 147/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1705 - acc: 0.9434 - val_loss: 0.7317 - val_acc: 0.7976\n",
            "Epoch 148/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1732 - acc: 0.9433 - val_loss: 0.6514 - val_acc: 0.8145\n",
            "Epoch 149/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1769 - acc: 0.9421 - val_loss: 0.7115 - val_acc: 0.7943\n",
            "Epoch 150/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1724 - acc: 0.9437 - val_loss: 0.7660 - val_acc: 0.7921\n",
            "Epoch 151/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1660 - acc: 0.9444 - val_loss: 0.8391 - val_acc: 0.7783\n",
            "Epoch 152/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1614 - acc: 0.9474 - val_loss: 0.7182 - val_acc: 0.8012\n",
            "Epoch 153/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1607 - acc: 0.9471 - val_loss: 0.6363 - val_acc: 0.8256\n",
            "Epoch 154/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1645 - acc: 0.9451 - val_loss: 0.6885 - val_acc: 0.8092\n",
            "Epoch 155/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1673 - acc: 0.9454 - val_loss: 0.7068 - val_acc: 0.8042\n",
            "Epoch 156/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1618 - acc: 0.9467 - val_loss: 0.7056 - val_acc: 0.8036\n",
            "Epoch 157/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1647 - acc: 0.9466 - val_loss: 0.7940 - val_acc: 0.8013\n",
            "Epoch 158/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1630 - acc: 0.9456 - val_loss: 0.7938 - val_acc: 0.7978\n",
            "Epoch 159/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1688 - acc: 0.9447 - val_loss: 0.6708 - val_acc: 0.8169\n",
            "Epoch 160/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1611 - acc: 0.9472 - val_loss: 0.7367 - val_acc: 0.8007\n",
            "Epoch 161/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1528 - acc: 0.9499 - val_loss: 0.7275 - val_acc: 0.8018\n",
            "Epoch 162/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1583 - acc: 0.9478 - val_loss: 0.6896 - val_acc: 0.8104\n",
            "Epoch 163/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1650 - acc: 0.9446 - val_loss: 0.9671 - val_acc: 0.7618\n",
            "Epoch 164/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1619 - acc: 0.9456 - val_loss: 0.6981 - val_acc: 0.8136\n",
            "Epoch 165/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1563 - acc: 0.9495 - val_loss: 0.8019 - val_acc: 0.7933\n",
            "Epoch 166/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1598 - acc: 0.9478 - val_loss: 0.7518 - val_acc: 0.8078\n",
            "Epoch 167/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1513 - acc: 0.9509 - val_loss: 0.8021 - val_acc: 0.7907\n",
            "Epoch 168/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1469 - acc: 0.9518 - val_loss: 0.6783 - val_acc: 0.8235\n",
            "Epoch 169/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1512 - acc: 0.9490 - val_loss: 0.7543 - val_acc: 0.8030\n",
            "Epoch 170/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1525 - acc: 0.9492 - val_loss: 0.7206 - val_acc: 0.7996\n",
            "Epoch 171/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1474 - acc: 0.9524 - val_loss: 0.8001 - val_acc: 0.7921\n",
            "Epoch 172/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1611 - acc: 0.9466 - val_loss: 0.6883 - val_acc: 0.8096\n",
            "Epoch 173/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1617 - acc: 0.9465 - val_loss: 0.8065 - val_acc: 0.7761\n",
            "Epoch 174/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1483 - acc: 0.9516 - val_loss: 0.7747 - val_acc: 0.7957\n",
            "Epoch 175/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1524 - acc: 0.9500 - val_loss: 0.6785 - val_acc: 0.8188\n",
            "Epoch 176/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1456 - acc: 0.9533 - val_loss: 0.6527 - val_acc: 0.8217\n",
            "Epoch 177/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1495 - acc: 0.9516 - val_loss: 0.6499 - val_acc: 0.8266\n",
            "Epoch 178/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1546 - acc: 0.9499 - val_loss: 0.7988 - val_acc: 0.7816\n",
            "Epoch 179/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1485 - acc: 0.9520 - val_loss: 0.6423 - val_acc: 0.8343\n",
            "Epoch 180/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1457 - acc: 0.9524 - val_loss: 0.6959 - val_acc: 0.8191\n",
            "Epoch 181/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1436 - acc: 0.9524 - val_loss: 0.7074 - val_acc: 0.8048\n",
            "Epoch 182/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1426 - acc: 0.9518 - val_loss: 0.7604 - val_acc: 0.8073\n",
            "Epoch 183/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1434 - acc: 0.9537 - val_loss: 0.7709 - val_acc: 0.8117\n",
            "Epoch 184/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1418 - acc: 0.9535 - val_loss: 0.7329 - val_acc: 0.8003\n",
            "Epoch 185/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1473 - acc: 0.9517 - val_loss: 1.0128 - val_acc: 0.7612\n",
            "Epoch 186/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1466 - acc: 0.9506 - val_loss: 0.6736 - val_acc: 0.8208\n",
            "Epoch 187/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1403 - acc: 0.9539 - val_loss: 0.7112 - val_acc: 0.8166\n",
            "Epoch 188/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1422 - acc: 0.9534 - val_loss: 0.7367 - val_acc: 0.8004\n",
            "Epoch 189/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1387 - acc: 0.9548 - val_loss: 1.2721 - val_acc: 0.7312\n",
            "Epoch 190/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1424 - acc: 0.9537 - val_loss: 0.6972 - val_acc: 0.8096\n",
            "Epoch 191/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1413 - acc: 0.9539 - val_loss: 1.1785 - val_acc: 0.7609\n",
            "Epoch 192/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1439 - acc: 0.9529 - val_loss: 0.7090 - val_acc: 0.8096\n",
            "Epoch 193/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1384 - acc: 0.9546 - val_loss: 0.9718 - val_acc: 0.7719\n",
            "Epoch 194/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1482 - acc: 0.9510 - val_loss: 0.7504 - val_acc: 0.8049\n",
            "Epoch 195/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1393 - acc: 0.9554 - val_loss: 0.7970 - val_acc: 0.7991\n",
            "Epoch 196/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1318 - acc: 0.9576 - val_loss: 0.6900 - val_acc: 0.8057\n",
            "Epoch 197/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1308 - acc: 0.9584 - val_loss: 0.6814 - val_acc: 0.8160\n",
            "Epoch 198/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1375 - acc: 0.9540 - val_loss: 0.8843 - val_acc: 0.7714\n",
            "Epoch 199/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1399 - acc: 0.9532 - val_loss: 0.9427 - val_acc: 0.7611\n",
            "Epoch 200/200\n",
            "33600/33600 [==============================] - 0s 8us/step - loss: 0.1400 - acc: 0.9540 - val_loss: 0.7921 - val_acc: 0.8035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7wU18ASqxu1",
        "colab_type": "code",
        "outputId": "2d9c63d1-eced-4402-f711-1d781ed8b559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "results = model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 0s 56us/step\n",
            "Test accuracy:  0.8028571428571428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGV-x3ct3QlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN8_test_acc=0.8028\n",
        "NN8_val_loss = history.history['val_loss']\n",
        "NN8_train_loss = history.history['loss']\n",
        "NN8_val_acc = history.history['val_acc']\n",
        "NN8_train_acc = history.history['acc']\n",
        "\n",
        "\n",
        "epochs = range(1,201)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, NN8_val_loss, 'b', label='Validation Loss')\n",
        "plt.plot(epochs, NN8_train_loss, 'r', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, NN8_val_acc, 'b', label='Validation Acc')\n",
        "plt.plot(epochs, NN8_train_acc, 'r', label='Training Acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyC3LTp8d94y",
        "colab_type": "code",
        "outputId": "2d6f3c34-85fe-47b6-b916-9e5a241c2f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's add a new layer & increase the nodes in the hidden layer\n",
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(248*3, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(248*3, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(248*3, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.adam(lr = 0.001)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=2048 ,epochs = 100, verbose = 1, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/100\n",
            "33600/33600 [==============================] - 2s 74us/step - loss: 2.2660 - acc: 0.2377 - val_loss: 2.1612 - val_acc: 0.2847\n",
            "Epoch 2/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.5115 - acc: 0.4903 - val_loss: 1.6673 - val_acc: 0.4521\n",
            "Epoch 3/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 1.1796 - acc: 0.6159 - val_loss: 1.7126 - val_acc: 0.4554\n",
            "Epoch 4/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.0263 - acc: 0.6720 - val_loss: 1.4154 - val_acc: 0.5418\n",
            "Epoch 5/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.9269 - acc: 0.7054 - val_loss: 1.3341 - val_acc: 0.5721\n",
            "Epoch 6/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.8580 - acc: 0.7275 - val_loss: 1.3500 - val_acc: 0.5626\n",
            "Epoch 7/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7968 - acc: 0.7471 - val_loss: 1.0086 - val_acc: 0.6897\n",
            "Epoch 8/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.7512 - acc: 0.7632 - val_loss: 0.9995 - val_acc: 0.6799\n",
            "Epoch 9/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7173 - acc: 0.7720 - val_loss: 1.0126 - val_acc: 0.6674\n",
            "Epoch 10/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6850 - acc: 0.7810 - val_loss: 1.1108 - val_acc: 0.6508\n",
            "Epoch 11/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6577 - acc: 0.7902 - val_loss: 1.0003 - val_acc: 0.6804\n",
            "Epoch 12/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6372 - acc: 0.7985 - val_loss: 1.0306 - val_acc: 0.6828\n",
            "Epoch 13/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6102 - acc: 0.8058 - val_loss: 0.8600 - val_acc: 0.7254\n",
            "Epoch 14/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5864 - acc: 0.8143 - val_loss: 0.8605 - val_acc: 0.7286\n",
            "Epoch 15/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5735 - acc: 0.8149 - val_loss: 0.9118 - val_acc: 0.7064\n",
            "Epoch 16/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5554 - acc: 0.8212 - val_loss: 0.8630 - val_acc: 0.7297\n",
            "Epoch 17/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5357 - acc: 0.8277 - val_loss: 0.8130 - val_acc: 0.7378\n",
            "Epoch 18/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5203 - acc: 0.8327 - val_loss: 0.7709 - val_acc: 0.7538\n",
            "Epoch 19/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.5133 - acc: 0.8352 - val_loss: 0.8742 - val_acc: 0.7278\n",
            "Epoch 20/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4986 - acc: 0.8408 - val_loss: 0.8907 - val_acc: 0.7155\n",
            "Epoch 21/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.4943 - acc: 0.8432 - val_loss: 0.7229 - val_acc: 0.7741\n",
            "Epoch 22/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4698 - acc: 0.8491 - val_loss: 0.7580 - val_acc: 0.7675\n",
            "Epoch 23/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.4575 - acc: 0.8547 - val_loss: 0.8357 - val_acc: 0.7376\n",
            "Epoch 24/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4560 - acc: 0.8524 - val_loss: 0.6841 - val_acc: 0.7879\n",
            "Epoch 25/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.4453 - acc: 0.8560 - val_loss: 0.7690 - val_acc: 0.7610\n",
            "Epoch 26/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4387 - acc: 0.8592 - val_loss: 0.7630 - val_acc: 0.7664\n",
            "Epoch 27/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4255 - acc: 0.8636 - val_loss: 0.8138 - val_acc: 0.7619\n",
            "Epoch 28/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4255 - acc: 0.8645 - val_loss: 0.7157 - val_acc: 0.7787\n",
            "Epoch 29/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4106 - acc: 0.8682 - val_loss: 0.6983 - val_acc: 0.7842\n",
            "Epoch 30/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3998 - acc: 0.8718 - val_loss: 0.7660 - val_acc: 0.7535\n",
            "Epoch 31/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3956 - acc: 0.8718 - val_loss: 0.7540 - val_acc: 0.7671\n",
            "Epoch 32/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3887 - acc: 0.8740 - val_loss: 0.8317 - val_acc: 0.7459\n",
            "Epoch 33/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3923 - acc: 0.8749 - val_loss: 0.6683 - val_acc: 0.8004\n",
            "Epoch 34/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3745 - acc: 0.8791 - val_loss: 0.7756 - val_acc: 0.7583\n",
            "Epoch 35/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3658 - acc: 0.8820 - val_loss: 0.6674 - val_acc: 0.7972\n",
            "Epoch 36/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3653 - acc: 0.8833 - val_loss: 0.9684 - val_acc: 0.6955\n",
            "Epoch 37/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3617 - acc: 0.8810 - val_loss: 0.6752 - val_acc: 0.7891\n",
            "Epoch 38/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3512 - acc: 0.8862 - val_loss: 0.6989 - val_acc: 0.7871\n",
            "Epoch 39/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3450 - acc: 0.8885 - val_loss: 0.8000 - val_acc: 0.7559\n",
            "Epoch 40/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3421 - acc: 0.8896 - val_loss: 0.6781 - val_acc: 0.7894\n",
            "Epoch 41/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3399 - acc: 0.8904 - val_loss: 0.6487 - val_acc: 0.8043\n",
            "Epoch 42/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3339 - acc: 0.8922 - val_loss: 0.7744 - val_acc: 0.7605\n",
            "Epoch 43/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3189 - acc: 0.8964 - val_loss: 0.7043 - val_acc: 0.7882\n",
            "Epoch 44/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3218 - acc: 0.8957 - val_loss: 0.7270 - val_acc: 0.7731\n",
            "Epoch 45/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3154 - acc: 0.8966 - val_loss: 0.6461 - val_acc: 0.8002\n",
            "Epoch 46/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3144 - acc: 0.8980 - val_loss: 0.8998 - val_acc: 0.7283\n",
            "Epoch 47/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3042 - acc: 0.9014 - val_loss: 0.9009 - val_acc: 0.7354\n",
            "Epoch 48/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3152 - acc: 0.8971 - val_loss: 0.7309 - val_acc: 0.7689\n",
            "Epoch 49/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2945 - acc: 0.9040 - val_loss: 0.7124 - val_acc: 0.7783\n",
            "Epoch 50/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2889 - acc: 0.9046 - val_loss: 0.6244 - val_acc: 0.8124\n",
            "Epoch 51/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2867 - acc: 0.9053 - val_loss: 0.5928 - val_acc: 0.8237\n",
            "Epoch 52/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2779 - acc: 0.9123 - val_loss: 0.9821 - val_acc: 0.7187\n",
            "Epoch 53/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2881 - acc: 0.9049 - val_loss: 0.7022 - val_acc: 0.7836\n",
            "Epoch 54/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2759 - acc: 0.9096 - val_loss: 0.7365 - val_acc: 0.7902\n",
            "Epoch 55/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.2825 - acc: 0.9066 - val_loss: 0.7860 - val_acc: 0.7544\n",
            "Epoch 56/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2643 - acc: 0.9128 - val_loss: 0.8562 - val_acc: 0.7477\n",
            "Epoch 57/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2607 - acc: 0.9144 - val_loss: 0.9178 - val_acc: 0.7448\n",
            "Epoch 58/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2659 - acc: 0.9110 - val_loss: 0.6486 - val_acc: 0.8073\n",
            "Epoch 59/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2574 - acc: 0.9153 - val_loss: 0.6983 - val_acc: 0.7899\n",
            "Epoch 60/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2583 - acc: 0.9157 - val_loss: 0.6880 - val_acc: 0.7959\n",
            "Epoch 61/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2503 - acc: 0.9185 - val_loss: 0.8488 - val_acc: 0.7634\n",
            "Epoch 62/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2532 - acc: 0.9159 - val_loss: 0.7196 - val_acc: 0.7844\n",
            "Epoch 63/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2520 - acc: 0.9158 - val_loss: 0.8757 - val_acc: 0.7447\n",
            "Epoch 64/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2552 - acc: 0.9139 - val_loss: 0.7960 - val_acc: 0.7809\n",
            "Epoch 65/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2487 - acc: 0.9164 - val_loss: 0.7547 - val_acc: 0.7816\n",
            "Epoch 66/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2384 - acc: 0.9221 - val_loss: 0.6642 - val_acc: 0.7987\n",
            "Epoch 67/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.2250 - acc: 0.9245 - val_loss: 0.7867 - val_acc: 0.7554\n",
            "Epoch 68/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2361 - acc: 0.9223 - val_loss: 0.6992 - val_acc: 0.7936\n",
            "Epoch 69/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.2296 - acc: 0.9239 - val_loss: 0.7017 - val_acc: 0.7862\n",
            "Epoch 70/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2163 - acc: 0.9281 - val_loss: 0.6613 - val_acc: 0.8087\n",
            "Epoch 71/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.2144 - acc: 0.9295 - val_loss: 0.6332 - val_acc: 0.8154\n",
            "Epoch 72/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2242 - acc: 0.9254 - val_loss: 0.7335 - val_acc: 0.7877\n",
            "Epoch 73/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.2104 - acc: 0.9302 - val_loss: 0.6143 - val_acc: 0.8113\n",
            "Epoch 74/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2066 - acc: 0.9315 - val_loss: 0.7408 - val_acc: 0.7828\n",
            "Epoch 75/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2099 - acc: 0.9297 - val_loss: 0.6517 - val_acc: 0.8068\n",
            "Epoch 76/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2011 - acc: 0.9322 - val_loss: 0.6992 - val_acc: 0.7995\n",
            "Epoch 77/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.2034 - acc: 0.9311 - val_loss: 0.8106 - val_acc: 0.7634\n",
            "Epoch 78/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1979 - acc: 0.9340 - val_loss: 0.8663 - val_acc: 0.7576\n",
            "Epoch 79/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.2008 - acc: 0.9328 - val_loss: 0.6813 - val_acc: 0.8098\n",
            "Epoch 80/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.1945 - acc: 0.9357 - val_loss: 0.7956 - val_acc: 0.7670\n",
            "Epoch 81/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1951 - acc: 0.9351 - val_loss: 0.8715 - val_acc: 0.7623\n",
            "Epoch 82/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.1979 - acc: 0.9345 - val_loss: 0.7290 - val_acc: 0.7731\n",
            "Epoch 83/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1950 - acc: 0.9348 - val_loss: 0.7704 - val_acc: 0.7827\n",
            "Epoch 84/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1892 - acc: 0.9369 - val_loss: 0.7629 - val_acc: 0.7732\n",
            "Epoch 85/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1846 - acc: 0.9382 - val_loss: 0.6820 - val_acc: 0.8073\n",
            "Epoch 86/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1833 - acc: 0.9392 - val_loss: 0.6128 - val_acc: 0.8266\n",
            "Epoch 87/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1819 - acc: 0.9413 - val_loss: 0.6628 - val_acc: 0.8097\n",
            "Epoch 88/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1786 - acc: 0.9399 - val_loss: 0.5753 - val_acc: 0.8358\n",
            "Epoch 89/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1760 - acc: 0.9402 - val_loss: 0.7154 - val_acc: 0.8048\n",
            "Epoch 90/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1707 - acc: 0.9424 - val_loss: 0.6662 - val_acc: 0.8149\n",
            "Epoch 91/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1680 - acc: 0.9439 - val_loss: 0.6793 - val_acc: 0.8048\n",
            "Epoch 92/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.1683 - acc: 0.9424 - val_loss: 0.5867 - val_acc: 0.8327\n",
            "Epoch 93/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.1638 - acc: 0.9455 - val_loss: 0.6713 - val_acc: 0.8032\n",
            "Epoch 94/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1644 - acc: 0.9452 - val_loss: 0.8566 - val_acc: 0.7768\n",
            "Epoch 95/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1643 - acc: 0.9446 - val_loss: 0.6375 - val_acc: 0.8298\n",
            "Epoch 96/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1585 - acc: 0.9469 - val_loss: 0.7008 - val_acc: 0.8006\n",
            "Epoch 97/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1633 - acc: 0.9458 - val_loss: 0.6832 - val_acc: 0.8098\n",
            "Epoch 98/100\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.1643 - acc: 0.9451 - val_loss: 0.6379 - val_acc: 0.8231\n",
            "Epoch 99/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1558 - acc: 0.9465 - val_loss: 0.6889 - val_acc: 0.7973\n",
            "Epoch 100/100\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1548 - acc: 0.9485 - val_loss: 0.6486 - val_acc: 0.8271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egrLEpR_sV2K",
        "colab_type": "code",
        "outputId": "09a173c1-8395-4004-be2d-db98980a7297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "results = model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 1s 67us/step\n",
            "Test accuracy:  0.8247619047619048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvV4g5K7qBG5",
        "colab_type": "code",
        "outputId": "eeaa116b-67bd-45b2-fb78-499a89aff97f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's increase the drop out rate since there was no increase in performance\n",
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(248*3, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(248*3, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(248*3, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.adam(lr = 0.001)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=2048 ,epochs = 200, verbose = 1, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/200\n",
            "33600/33600 [==============================] - 4s 105us/step - loss: 2.2504 - acc: 0.2415 - val_loss: 2.0561 - val_acc: 0.3013\n",
            "Epoch 2/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.4931 - acc: 0.4922 - val_loss: 1.8531 - val_acc: 0.4102\n",
            "Epoch 3/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.1701 - acc: 0.6187 - val_loss: 1.4467 - val_acc: 0.5362\n",
            "Epoch 4/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.0173 - acc: 0.6736 - val_loss: 1.2948 - val_acc: 0.5811\n",
            "Epoch 5/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.9197 - acc: 0.7074 - val_loss: 1.3959 - val_acc: 0.5730\n",
            "Epoch 6/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8508 - acc: 0.7281 - val_loss: 1.4381 - val_acc: 0.5505\n",
            "Epoch 7/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8004 - acc: 0.7449 - val_loss: 1.1790 - val_acc: 0.6361\n",
            "Epoch 8/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7492 - acc: 0.7607 - val_loss: 1.0252 - val_acc: 0.6632\n",
            "Epoch 9/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7152 - acc: 0.7731 - val_loss: 1.0619 - val_acc: 0.6611\n",
            "Epoch 10/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.6797 - acc: 0.7852 - val_loss: 0.9430 - val_acc: 0.7119\n",
            "Epoch 11/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.6486 - acc: 0.7936 - val_loss: 0.8369 - val_acc: 0.7357\n",
            "Epoch 12/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6336 - acc: 0.7989 - val_loss: 0.9645 - val_acc: 0.6936\n",
            "Epoch 13/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.6067 - acc: 0.8069 - val_loss: 0.7980 - val_acc: 0.7592\n",
            "Epoch 14/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.5879 - acc: 0.8133 - val_loss: 1.0540 - val_acc: 0.6869\n",
            "Epoch 15/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5762 - acc: 0.8166 - val_loss: 1.1005 - val_acc: 0.6479\n",
            "Epoch 16/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.5617 - acc: 0.8226 - val_loss: 0.7676 - val_acc: 0.7547\n",
            "Epoch 17/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5398 - acc: 0.8268 - val_loss: 0.8209 - val_acc: 0.7432\n",
            "Epoch 18/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.5164 - acc: 0.8343 - val_loss: 1.0300 - val_acc: 0.6860\n",
            "Epoch 19/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5161 - acc: 0.8365 - val_loss: 0.8058 - val_acc: 0.7509\n",
            "Epoch 20/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4992 - acc: 0.8384 - val_loss: 0.7717 - val_acc: 0.7605\n",
            "Epoch 21/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4883 - acc: 0.8421 - val_loss: 0.7331 - val_acc: 0.7732\n",
            "Epoch 22/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4726 - acc: 0.8476 - val_loss: 0.6946 - val_acc: 0.7822\n",
            "Epoch 23/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4521 - acc: 0.8553 - val_loss: 0.7491 - val_acc: 0.7648\n",
            "Epoch 24/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4479 - acc: 0.8552 - val_loss: 0.6942 - val_acc: 0.7832\n",
            "Epoch 25/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4407 - acc: 0.8585 - val_loss: 0.7660 - val_acc: 0.7534\n",
            "Epoch 26/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4300 - acc: 0.8621 - val_loss: 0.8673 - val_acc: 0.7353\n",
            "Epoch 27/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4306 - acc: 0.8616 - val_loss: 0.7199 - val_acc: 0.7803\n",
            "Epoch 28/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4234 - acc: 0.8626 - val_loss: 0.7379 - val_acc: 0.7628\n",
            "Epoch 29/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4076 - acc: 0.8689 - val_loss: 0.7195 - val_acc: 0.7798\n",
            "Epoch 30/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4065 - acc: 0.8692 - val_loss: 0.7180 - val_acc: 0.7687\n",
            "Epoch 31/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3958 - acc: 0.8726 - val_loss: 0.6559 - val_acc: 0.8009\n",
            "Epoch 32/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3883 - acc: 0.8740 - val_loss: 0.8924 - val_acc: 0.7338\n",
            "Epoch 33/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3847 - acc: 0.8757 - val_loss: 0.5997 - val_acc: 0.8174\n",
            "Epoch 34/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3736 - acc: 0.8788 - val_loss: 0.7916 - val_acc: 0.7533\n",
            "Epoch 35/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3741 - acc: 0.8796 - val_loss: 0.7964 - val_acc: 0.7367\n",
            "Epoch 36/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.3608 - acc: 0.8823 - val_loss: 0.7362 - val_acc: 0.7719\n",
            "Epoch 37/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.3512 - acc: 0.8870 - val_loss: 0.7801 - val_acc: 0.7603\n",
            "Epoch 38/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3497 - acc: 0.8861 - val_loss: 0.7519 - val_acc: 0.7731\n",
            "Epoch 39/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3459 - acc: 0.8871 - val_loss: 0.6760 - val_acc: 0.7901\n",
            "Epoch 40/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3368 - acc: 0.8916 - val_loss: 0.8040 - val_acc: 0.7617\n",
            "Epoch 41/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3325 - acc: 0.8921 - val_loss: 0.8446 - val_acc: 0.7377\n",
            "Epoch 42/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.3372 - acc: 0.8905 - val_loss: 0.8798 - val_acc: 0.7224\n",
            "Epoch 43/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3328 - acc: 0.8904 - val_loss: 0.8274 - val_acc: 0.7476\n",
            "Epoch 44/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3189 - acc: 0.8970 - val_loss: 0.6293 - val_acc: 0.8089\n",
            "Epoch 45/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.3130 - acc: 0.8984 - val_loss: 0.6676 - val_acc: 0.7906\n",
            "Epoch 46/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.3080 - acc: 0.8984 - val_loss: 0.5936 - val_acc: 0.8241\n",
            "Epoch 47/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2979 - acc: 0.9039 - val_loss: 1.0216 - val_acc: 0.7177\n",
            "Epoch 48/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3010 - acc: 0.9005 - val_loss: 1.1376 - val_acc: 0.7203\n",
            "Epoch 49/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.3016 - acc: 0.9009 - val_loss: 0.6918 - val_acc: 0.7842\n",
            "Epoch 50/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2990 - acc: 0.9024 - val_loss: 0.6970 - val_acc: 0.7865\n",
            "Epoch 51/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2965 - acc: 0.9012 - val_loss: 0.8259 - val_acc: 0.7579\n",
            "Epoch 52/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2862 - acc: 0.9070 - val_loss: 0.5944 - val_acc: 0.8174\n",
            "Epoch 53/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2779 - acc: 0.9078 - val_loss: 0.8396 - val_acc: 0.7418\n",
            "Epoch 54/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2712 - acc: 0.9118 - val_loss: 0.6872 - val_acc: 0.7961\n",
            "Epoch 55/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2704 - acc: 0.9107 - val_loss: 0.5839 - val_acc: 0.8290\n",
            "Epoch 56/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2684 - acc: 0.9131 - val_loss: 0.6864 - val_acc: 0.7902\n",
            "Epoch 57/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2647 - acc: 0.9147 - val_loss: 0.5954 - val_acc: 0.8227\n",
            "Epoch 58/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2547 - acc: 0.9168 - val_loss: 0.8371 - val_acc: 0.7677\n",
            "Epoch 59/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2606 - acc: 0.9140 - val_loss: 0.6082 - val_acc: 0.8156\n",
            "Epoch 60/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2539 - acc: 0.9154 - val_loss: 0.6356 - val_acc: 0.8093\n",
            "Epoch 61/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2479 - acc: 0.9179 - val_loss: 0.7457 - val_acc: 0.7787\n",
            "Epoch 62/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2421 - acc: 0.9193 - val_loss: 0.7242 - val_acc: 0.7790\n",
            "Epoch 63/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2422 - acc: 0.9218 - val_loss: 0.7266 - val_acc: 0.7907\n",
            "Epoch 64/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2424 - acc: 0.9195 - val_loss: 0.6663 - val_acc: 0.8031\n",
            "Epoch 65/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2396 - acc: 0.9226 - val_loss: 0.7782 - val_acc: 0.7651\n",
            "Epoch 66/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2360 - acc: 0.9224 - val_loss: 0.7840 - val_acc: 0.7682\n",
            "Epoch 67/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2423 - acc: 0.9214 - val_loss: 0.9606 - val_acc: 0.7385\n",
            "Epoch 68/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2455 - acc: 0.9190 - val_loss: 0.7437 - val_acc: 0.7776\n",
            "Epoch 69/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2307 - acc: 0.9256 - val_loss: 0.7472 - val_acc: 0.7879\n",
            "Epoch 70/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2212 - acc: 0.9271 - val_loss: 0.7177 - val_acc: 0.7809\n",
            "Epoch 71/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2186 - acc: 0.9281 - val_loss: 0.6930 - val_acc: 0.7931\n",
            "Epoch 72/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2211 - acc: 0.9271 - val_loss: 0.6479 - val_acc: 0.8074\n",
            "Epoch 73/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2115 - acc: 0.9301 - val_loss: 0.6901 - val_acc: 0.8014\n",
            "Epoch 74/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.2126 - acc: 0.9292 - val_loss: 0.7333 - val_acc: 0.7880\n",
            "Epoch 75/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2149 - acc: 0.9269 - val_loss: 0.7273 - val_acc: 0.7864\n",
            "Epoch 76/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2055 - acc: 0.9314 - val_loss: 0.6923 - val_acc: 0.8016\n",
            "Epoch 77/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2048 - acc: 0.9323 - val_loss: 0.9564 - val_acc: 0.7514\n",
            "Epoch 78/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2091 - acc: 0.9306 - val_loss: 0.8221 - val_acc: 0.7748\n",
            "Epoch 79/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.2006 - acc: 0.9315 - val_loss: 0.6306 - val_acc: 0.8224\n",
            "Epoch 80/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1944 - acc: 0.9348 - val_loss: 0.6906 - val_acc: 0.8015\n",
            "Epoch 81/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1967 - acc: 0.9346 - val_loss: 0.6750 - val_acc: 0.8100\n",
            "Epoch 82/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1870 - acc: 0.9371 - val_loss: 0.8686 - val_acc: 0.7638\n",
            "Epoch 83/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1822 - acc: 0.9397 - val_loss: 0.9229 - val_acc: 0.7565\n",
            "Epoch 84/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1793 - acc: 0.9415 - val_loss: 0.6745 - val_acc: 0.8074\n",
            "Epoch 85/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1859 - acc: 0.9391 - val_loss: 0.7101 - val_acc: 0.7928\n",
            "Epoch 86/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1836 - acc: 0.9385 - val_loss: 0.6638 - val_acc: 0.8056\n",
            "Epoch 87/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1819 - acc: 0.9401 - val_loss: 0.7114 - val_acc: 0.8009\n",
            "Epoch 88/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1827 - acc: 0.9393 - val_loss: 0.6949 - val_acc: 0.8000\n",
            "Epoch 89/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1759 - acc: 0.9406 - val_loss: 0.6783 - val_acc: 0.8148\n",
            "Epoch 90/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1772 - acc: 0.9405 - val_loss: 0.9558 - val_acc: 0.7568\n",
            "Epoch 91/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1779 - acc: 0.9401 - val_loss: 0.6821 - val_acc: 0.8102\n",
            "Epoch 92/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1663 - acc: 0.9445 - val_loss: 0.6484 - val_acc: 0.8169\n",
            "Epoch 93/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1610 - acc: 0.9466 - val_loss: 0.6778 - val_acc: 0.8006\n",
            "Epoch 94/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1640 - acc: 0.9445 - val_loss: 0.8126 - val_acc: 0.7795\n",
            "Epoch 95/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1709 - acc: 0.9429 - val_loss: 0.7033 - val_acc: 0.7981\n",
            "Epoch 96/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1711 - acc: 0.9431 - val_loss: 0.7414 - val_acc: 0.7960\n",
            "Epoch 97/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1641 - acc: 0.9451 - val_loss: 0.6585 - val_acc: 0.8074\n",
            "Epoch 98/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1693 - acc: 0.9434 - val_loss: 0.7030 - val_acc: 0.7942\n",
            "Epoch 99/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1636 - acc: 0.9451 - val_loss: 0.8215 - val_acc: 0.7680\n",
            "Epoch 100/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1595 - acc: 0.9471 - val_loss: 0.7645 - val_acc: 0.7921\n",
            "Epoch 101/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1574 - acc: 0.9465 - val_loss: 0.7219 - val_acc: 0.8085\n",
            "Epoch 102/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1553 - acc: 0.9487 - val_loss: 0.7322 - val_acc: 0.7920\n",
            "Epoch 103/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1515 - acc: 0.9488 - val_loss: 0.6400 - val_acc: 0.8223\n",
            "Epoch 104/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1500 - acc: 0.9497 - val_loss: 0.6494 - val_acc: 0.8219\n",
            "Epoch 105/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1401 - acc: 0.9536 - val_loss: 0.6822 - val_acc: 0.8112\n",
            "Epoch 106/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1411 - acc: 0.9537 - val_loss: 0.6450 - val_acc: 0.8266\n",
            "Epoch 107/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1443 - acc: 0.9519 - val_loss: 0.8443 - val_acc: 0.7723\n",
            "Epoch 108/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1432 - acc: 0.9518 - val_loss: 0.7250 - val_acc: 0.8002\n",
            "Epoch 109/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1446 - acc: 0.9517 - val_loss: 0.6218 - val_acc: 0.8242\n",
            "Epoch 110/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1477 - acc: 0.9511 - val_loss: 0.6991 - val_acc: 0.8091\n",
            "Epoch 111/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1399 - acc: 0.9527 - val_loss: 0.7899 - val_acc: 0.7758\n",
            "Epoch 112/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1421 - acc: 0.9520 - val_loss: 0.6603 - val_acc: 0.8180\n",
            "Epoch 113/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1438 - acc: 0.9528 - val_loss: 0.7072 - val_acc: 0.8149\n",
            "Epoch 114/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1400 - acc: 0.9538 - val_loss: 0.7045 - val_acc: 0.8106\n",
            "Epoch 115/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1348 - acc: 0.9545 - val_loss: 0.8085 - val_acc: 0.7801\n",
            "Epoch 116/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1372 - acc: 0.9530 - val_loss: 0.6982 - val_acc: 0.8212\n",
            "Epoch 117/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1335 - acc: 0.9550 - val_loss: 0.6361 - val_acc: 0.8307\n",
            "Epoch 118/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1253 - acc: 0.9577 - val_loss: 0.6695 - val_acc: 0.8278\n",
            "Epoch 119/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1306 - acc: 0.9573 - val_loss: 0.6603 - val_acc: 0.8184\n",
            "Epoch 120/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1323 - acc: 0.9546 - val_loss: 0.7033 - val_acc: 0.8122\n",
            "Epoch 121/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1292 - acc: 0.9561 - val_loss: 0.6735 - val_acc: 0.8149\n",
            "Epoch 122/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1208 - acc: 0.9591 - val_loss: 0.7930 - val_acc: 0.7938\n",
            "Epoch 123/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1303 - acc: 0.9557 - val_loss: 0.7162 - val_acc: 0.8049\n",
            "Epoch 124/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1350 - acc: 0.9538 - val_loss: 0.7377 - val_acc: 0.8047\n",
            "Epoch 125/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1305 - acc: 0.9567 - val_loss: 0.6735 - val_acc: 0.8228\n",
            "Epoch 126/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1264 - acc: 0.9572 - val_loss: 0.7639 - val_acc: 0.7962\n",
            "Epoch 127/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1186 - acc: 0.9607 - val_loss: 0.6693 - val_acc: 0.8146\n",
            "Epoch 128/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1267 - acc: 0.9564 - val_loss: 0.8147 - val_acc: 0.7923\n",
            "Epoch 129/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1211 - acc: 0.9587 - val_loss: 0.7881 - val_acc: 0.7932\n",
            "Epoch 130/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1175 - acc: 0.9615 - val_loss: 0.9037 - val_acc: 0.7778\n",
            "Epoch 131/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1151 - acc: 0.9613 - val_loss: 0.7292 - val_acc: 0.7966\n",
            "Epoch 132/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1189 - acc: 0.9599 - val_loss: 0.8223 - val_acc: 0.7742\n",
            "Epoch 133/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1166 - acc: 0.9615 - val_loss: 0.7085 - val_acc: 0.8054\n",
            "Epoch 134/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1152 - acc: 0.9621 - val_loss: 0.6892 - val_acc: 0.8221\n",
            "Epoch 135/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1155 - acc: 0.9609 - val_loss: 0.8605 - val_acc: 0.7756\n",
            "Epoch 136/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1140 - acc: 0.9616 - val_loss: 0.6937 - val_acc: 0.8202\n",
            "Epoch 137/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1103 - acc: 0.9632 - val_loss: 0.7879 - val_acc: 0.8054\n",
            "Epoch 138/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1113 - acc: 0.9622 - val_loss: 0.7493 - val_acc: 0.8084\n",
            "Epoch 139/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1116 - acc: 0.9621 - val_loss: 0.7124 - val_acc: 0.8157\n",
            "Epoch 140/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1093 - acc: 0.9628 - val_loss: 0.7138 - val_acc: 0.8132\n",
            "Epoch 141/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1086 - acc: 0.9627 - val_loss: 0.6470 - val_acc: 0.8336\n",
            "Epoch 142/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1130 - acc: 0.9618 - val_loss: 0.7052 - val_acc: 0.8049\n",
            "Epoch 143/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1079 - acc: 0.9629 - val_loss: 0.6478 - val_acc: 0.8293\n",
            "Epoch 144/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1084 - acc: 0.9643 - val_loss: 0.7228 - val_acc: 0.8138\n",
            "Epoch 145/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1109 - acc: 0.9622 - val_loss: 0.6703 - val_acc: 0.8249\n",
            "Epoch 146/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1025 - acc: 0.9660 - val_loss: 0.7018 - val_acc: 0.8234\n",
            "Epoch 147/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1021 - acc: 0.9654 - val_loss: 0.7994 - val_acc: 0.7977\n",
            "Epoch 148/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1041 - acc: 0.9649 - val_loss: 0.8068 - val_acc: 0.7877\n",
            "Epoch 149/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.1022 - acc: 0.9661 - val_loss: 0.7693 - val_acc: 0.8089\n",
            "Epoch 150/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1094 - acc: 0.9640 - val_loss: 0.8030 - val_acc: 0.7949\n",
            "Epoch 151/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1030 - acc: 0.9648 - val_loss: 0.7092 - val_acc: 0.8188\n",
            "Epoch 152/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.0995 - acc: 0.9655 - val_loss: 0.8204 - val_acc: 0.7943\n",
            "Epoch 153/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1008 - acc: 0.9654 - val_loss: 0.7000 - val_acc: 0.8264\n",
            "Epoch 154/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0927 - acc: 0.9691 - val_loss: 0.6572 - val_acc: 0.8254\n",
            "Epoch 155/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0945 - acc: 0.9673 - val_loss: 0.7354 - val_acc: 0.8124\n",
            "Epoch 156/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.1014 - acc: 0.9663 - val_loss: 0.8873 - val_acc: 0.7937\n",
            "Epoch 157/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0938 - acc: 0.9687 - val_loss: 0.6994 - val_acc: 0.8204\n",
            "Epoch 158/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.1020 - acc: 0.9660 - val_loss: 0.7055 - val_acc: 0.8180\n",
            "Epoch 159/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0964 - acc: 0.9671 - val_loss: 0.6674 - val_acc: 0.8307\n",
            "Epoch 160/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0968 - acc: 0.9672 - val_loss: 0.7540 - val_acc: 0.8128\n",
            "Epoch 161/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0978 - acc: 0.9669 - val_loss: 0.8720 - val_acc: 0.7943\n",
            "Epoch 162/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0985 - acc: 0.9666 - val_loss: 0.7591 - val_acc: 0.8038\n",
            "Epoch 163/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0998 - acc: 0.9668 - val_loss: 0.7441 - val_acc: 0.8141\n",
            "Epoch 164/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0953 - acc: 0.9680 - val_loss: 0.7460 - val_acc: 0.8197\n",
            "Epoch 165/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0929 - acc: 0.9682 - val_loss: 0.8145 - val_acc: 0.8001\n",
            "Epoch 166/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0962 - acc: 0.9692 - val_loss: 0.6656 - val_acc: 0.8327\n",
            "Epoch 167/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0889 - acc: 0.9701 - val_loss: 0.7767 - val_acc: 0.8067\n",
            "Epoch 168/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0938 - acc: 0.9677 - val_loss: 0.8381 - val_acc: 0.7902\n",
            "Epoch 169/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0947 - acc: 0.9687 - val_loss: 0.7702 - val_acc: 0.8162\n",
            "Epoch 170/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0877 - acc: 0.9707 - val_loss: 0.7458 - val_acc: 0.8094\n",
            "Epoch 171/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0936 - acc: 0.9688 - val_loss: 0.8072 - val_acc: 0.7980\n",
            "Epoch 172/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0862 - acc: 0.9708 - val_loss: 0.7042 - val_acc: 0.8191\n",
            "Epoch 173/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0909 - acc: 0.9690 - val_loss: 0.7310 - val_acc: 0.8174\n",
            "Epoch 174/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0913 - acc: 0.9690 - val_loss: 1.0406 - val_acc: 0.7548\n",
            "Epoch 175/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0955 - acc: 0.9687 - val_loss: 0.8318 - val_acc: 0.7905\n",
            "Epoch 176/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0877 - acc: 0.9707 - val_loss: 0.7253 - val_acc: 0.8188\n",
            "Epoch 177/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0842 - acc: 0.9714 - val_loss: 0.8176 - val_acc: 0.7971\n",
            "Epoch 178/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0891 - acc: 0.9700 - val_loss: 0.7550 - val_acc: 0.8115\n",
            "Epoch 179/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0930 - acc: 0.9697 - val_loss: 0.7059 - val_acc: 0.8197\n",
            "Epoch 180/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0837 - acc: 0.9714 - val_loss: 0.8405 - val_acc: 0.7986\n",
            "Epoch 181/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0869 - acc: 0.9707 - val_loss: 0.8218 - val_acc: 0.7938\n",
            "Epoch 182/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0841 - acc: 0.9722 - val_loss: 0.7153 - val_acc: 0.8219\n",
            "Epoch 183/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0817 - acc: 0.9734 - val_loss: 0.7649 - val_acc: 0.8183\n",
            "Epoch 184/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0875 - acc: 0.9711 - val_loss: 0.8514 - val_acc: 0.7984\n",
            "Epoch 185/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0900 - acc: 0.9695 - val_loss: 0.8078 - val_acc: 0.7980\n",
            "Epoch 186/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0874 - acc: 0.9707 - val_loss: 0.7333 - val_acc: 0.8183\n",
            "Epoch 187/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0828 - acc: 0.9722 - val_loss: 0.7415 - val_acc: 0.8206\n",
            "Epoch 188/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0809 - acc: 0.9732 - val_loss: 0.7019 - val_acc: 0.8223\n",
            "Epoch 189/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0831 - acc: 0.9721 - val_loss: 0.7705 - val_acc: 0.8138\n",
            "Epoch 190/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0811 - acc: 0.9726 - val_loss: 0.7900 - val_acc: 0.8137\n",
            "Epoch 191/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0817 - acc: 0.9729 - val_loss: 0.7016 - val_acc: 0.8248\n",
            "Epoch 192/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0797 - acc: 0.9729 - val_loss: 0.6580 - val_acc: 0.8321\n",
            "Epoch 193/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0802 - acc: 0.9728 - val_loss: 0.7925 - val_acc: 0.8152\n",
            "Epoch 194/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0798 - acc: 0.9742 - val_loss: 0.8693 - val_acc: 0.8024\n",
            "Epoch 195/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0794 - acc: 0.9731 - val_loss: 0.7582 - val_acc: 0.8160\n",
            "Epoch 196/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0763 - acc: 0.9743 - val_loss: 0.6795 - val_acc: 0.8321\n",
            "Epoch 197/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0752 - acc: 0.9746 - val_loss: 0.8472 - val_acc: 0.8062\n",
            "Epoch 198/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0711 - acc: 0.9760 - val_loss: 0.8345 - val_acc: 0.8106\n",
            "Epoch 199/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.0718 - acc: 0.9756 - val_loss: 0.7856 - val_acc: 0.8223\n",
            "Epoch 200/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.0733 - acc: 0.9753 - val_loss: 0.7836 - val_acc: 0.8047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ_yOan3uRwz",
        "colab_type": "code",
        "outputId": "adc83790-4dbb-404f-fe37-8e9db039db8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 1s 65us/step\n",
            "Test accuracy:  0.8029761904761905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M45ArDS-qBWl",
        "colab_type": "code",
        "outputId": "ebe0f80c-12ca-4108-92fc-1799b1ea4451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's try leaky relu with increased nodes, drop out and reduced learning rate.\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(248*3, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(248*3, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.1))    \n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(248*3, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.1))    \n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.adam(lr = 0.0001)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=2048 ,epochs = 200, verbose = 1, validation_data=(X_test, y_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/200\n",
            "33600/33600 [==============================] - 5s 150us/step - loss: 2.8964 - acc: 0.1102 - val_loss: 2.2571 - val_acc: 0.1844\n",
            "Epoch 2/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 2.6533 - acc: 0.1365 - val_loss: 2.1105 - val_acc: 0.2785\n",
            "Epoch 3/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 2.4954 - acc: 0.1680 - val_loss: 2.0277 - val_acc: 0.3148\n",
            "Epoch 4/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 2.3752 - acc: 0.2004 - val_loss: 1.9428 - val_acc: 0.3577\n",
            "Epoch 5/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 2.2473 - acc: 0.2390 - val_loss: 1.8845 - val_acc: 0.3762\n",
            "Epoch 6/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 2.1133 - acc: 0.2808 - val_loss: 1.8214 - val_acc: 0.4050\n",
            "Epoch 7/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.9909 - acc: 0.3215 - val_loss: 1.7578 - val_acc: 0.4153\n",
            "Epoch 8/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.8718 - acc: 0.3608 - val_loss: 1.6891 - val_acc: 0.4457\n",
            "Epoch 9/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.7795 - acc: 0.3943 - val_loss: 1.5962 - val_acc: 0.4979\n",
            "Epoch 10/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.6832 - acc: 0.4321 - val_loss: 1.5219 - val_acc: 0.5244\n",
            "Epoch 11/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.6178 - acc: 0.4527 - val_loss: 1.4721 - val_acc: 0.5463\n",
            "Epoch 12/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.5477 - acc: 0.4802 - val_loss: 1.3875 - val_acc: 0.5695\n",
            "Epoch 13/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.4776 - acc: 0.5055 - val_loss: 1.3058 - val_acc: 0.6043\n",
            "Epoch 14/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.4334 - acc: 0.5231 - val_loss: 1.2342 - val_acc: 0.6249\n",
            "Epoch 15/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.3799 - acc: 0.5426 - val_loss: 1.2158 - val_acc: 0.6406\n",
            "Epoch 16/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.3360 - acc: 0.5561 - val_loss: 1.1444 - val_acc: 0.6593\n",
            "Epoch 17/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.3031 - acc: 0.5753 - val_loss: 1.0948 - val_acc: 0.6784\n",
            "Epoch 18/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.2731 - acc: 0.5879 - val_loss: 1.0571 - val_acc: 0.6919\n",
            "Epoch 19/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.2373 - acc: 0.5953 - val_loss: 1.0298 - val_acc: 0.6949\n",
            "Epoch 20/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.2071 - acc: 0.6097 - val_loss: 0.9899 - val_acc: 0.7101\n",
            "Epoch 21/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.1798 - acc: 0.6170 - val_loss: 0.9871 - val_acc: 0.7037\n",
            "Epoch 22/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.1556 - acc: 0.6277 - val_loss: 0.9559 - val_acc: 0.7154\n",
            "Epoch 23/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.1355 - acc: 0.6357 - val_loss: 0.9441 - val_acc: 0.7175\n",
            "Epoch 24/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.1134 - acc: 0.6436 - val_loss: 0.9011 - val_acc: 0.7298\n",
            "Epoch 25/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.0932 - acc: 0.6512 - val_loss: 0.9074 - val_acc: 0.7272\n",
            "Epoch 26/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.0807 - acc: 0.6555 - val_loss: 0.8856 - val_acc: 0.7352\n",
            "Epoch 27/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.0552 - acc: 0.6656 - val_loss: 0.8719 - val_acc: 0.7338\n",
            "Epoch 28/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.0386 - acc: 0.6700 - val_loss: 0.8785 - val_acc: 0.7302\n",
            "Epoch 29/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.0215 - acc: 0.6747 - val_loss: 0.8474 - val_acc: 0.7428\n",
            "Epoch 30/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.0205 - acc: 0.6762 - val_loss: 0.8441 - val_acc: 0.7467\n",
            "Epoch 31/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 1.0058 - acc: 0.6806 - val_loss: 0.8337 - val_acc: 0.7467\n",
            "Epoch 32/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.9854 - acc: 0.6896 - val_loss: 0.8454 - val_acc: 0.7454\n",
            "Epoch 33/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.9765 - acc: 0.6915 - val_loss: 0.8054 - val_acc: 0.7561\n",
            "Epoch 34/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.9516 - acc: 0.7001 - val_loss: 0.7994 - val_acc: 0.7572\n",
            "Epoch 35/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.9507 - acc: 0.7008 - val_loss: 0.7768 - val_acc: 0.7657\n",
            "Epoch 36/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.9365 - acc: 0.7035 - val_loss: 0.7800 - val_acc: 0.7661\n",
            "Epoch 37/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.9215 - acc: 0.7080 - val_loss: 0.7784 - val_acc: 0.7666\n",
            "Epoch 38/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.9215 - acc: 0.7096 - val_loss: 0.7634 - val_acc: 0.7667\n",
            "Epoch 39/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.9024 - acc: 0.7166 - val_loss: 0.7602 - val_acc: 0.7708\n",
            "Epoch 40/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.8999 - acc: 0.7180 - val_loss: 0.7505 - val_acc: 0.7721\n",
            "Epoch 41/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8885 - acc: 0.7219 - val_loss: 0.7505 - val_acc: 0.7739\n",
            "Epoch 42/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8771 - acc: 0.7228 - val_loss: 0.7415 - val_acc: 0.7774\n",
            "Epoch 43/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8635 - acc: 0.7282 - val_loss: 0.7301 - val_acc: 0.7793\n",
            "Epoch 44/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8650 - acc: 0.7285 - val_loss: 0.7337 - val_acc: 0.7776\n",
            "Epoch 45/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8493 - acc: 0.7333 - val_loss: 0.7185 - val_acc: 0.7850\n",
            "Epoch 46/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8475 - acc: 0.7331 - val_loss: 0.7309 - val_acc: 0.7787\n",
            "Epoch 47/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.8343 - acc: 0.7375 - val_loss: 0.7104 - val_acc: 0.7886\n",
            "Epoch 48/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8323 - acc: 0.7397 - val_loss: 0.6986 - val_acc: 0.7898\n",
            "Epoch 49/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8264 - acc: 0.7408 - val_loss: 0.6957 - val_acc: 0.7930\n",
            "Epoch 50/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8126 - acc: 0.7474 - val_loss: 0.6908 - val_acc: 0.7932\n",
            "Epoch 51/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.8063 - acc: 0.7490 - val_loss: 0.7274 - val_acc: 0.7820\n",
            "Epoch 52/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7993 - acc: 0.7512 - val_loss: 0.6859 - val_acc: 0.7957\n",
            "Epoch 53/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7944 - acc: 0.7493 - val_loss: 0.6863 - val_acc: 0.7933\n",
            "Epoch 54/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7815 - acc: 0.7549 - val_loss: 0.6857 - val_acc: 0.7963\n",
            "Epoch 55/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7749 - acc: 0.7542 - val_loss: 0.6652 - val_acc: 0.8013\n",
            "Epoch 56/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7694 - acc: 0.7578 - val_loss: 0.6790 - val_acc: 0.7957\n",
            "Epoch 57/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7670 - acc: 0.7598 - val_loss: 0.6626 - val_acc: 0.8009\n",
            "Epoch 58/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7615 - acc: 0.7638 - val_loss: 0.6585 - val_acc: 0.8036\n",
            "Epoch 59/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7573 - acc: 0.7649 - val_loss: 0.6543 - val_acc: 0.8050\n",
            "Epoch 60/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7524 - acc: 0.7635 - val_loss: 0.6775 - val_acc: 0.7969\n",
            "Epoch 61/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7471 - acc: 0.7651 - val_loss: 0.6474 - val_acc: 0.8052\n",
            "Epoch 62/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7319 - acc: 0.7710 - val_loss: 0.6466 - val_acc: 0.8073\n",
            "Epoch 63/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7341 - acc: 0.7702 - val_loss: 0.6579 - val_acc: 0.8019\n",
            "Epoch 64/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7218 - acc: 0.7751 - val_loss: 0.6565 - val_acc: 0.8014\n",
            "Epoch 65/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7217 - acc: 0.7740 - val_loss: 0.6525 - val_acc: 0.8006\n",
            "Epoch 66/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7148 - acc: 0.7783 - val_loss: 0.6314 - val_acc: 0.8098\n",
            "Epoch 67/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.7081 - acc: 0.7793 - val_loss: 0.6318 - val_acc: 0.8113\n",
            "Epoch 68/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7026 - acc: 0.7796 - val_loss: 0.6295 - val_acc: 0.8132\n",
            "Epoch 69/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7003 - acc: 0.7830 - val_loss: 0.6337 - val_acc: 0.8096\n",
            "Epoch 70/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.7014 - acc: 0.7805 - val_loss: 0.6262 - val_acc: 0.8127\n",
            "Epoch 71/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6934 - acc: 0.7827 - val_loss: 0.6270 - val_acc: 0.8127\n",
            "Epoch 72/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6835 - acc: 0.7855 - val_loss: 0.6296 - val_acc: 0.8138\n",
            "Epoch 73/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6897 - acc: 0.7854 - val_loss: 0.6223 - val_acc: 0.8111\n",
            "Epoch 74/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6825 - acc: 0.7861 - val_loss: 0.6232 - val_acc: 0.8127\n",
            "Epoch 75/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6835 - acc: 0.7881 - val_loss: 0.6288 - val_acc: 0.8152\n",
            "Epoch 76/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6753 - acc: 0.7887 - val_loss: 0.6052 - val_acc: 0.8213\n",
            "Epoch 77/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6707 - acc: 0.7920 - val_loss: 0.6117 - val_acc: 0.8153\n",
            "Epoch 78/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6662 - acc: 0.7924 - val_loss: 0.6233 - val_acc: 0.8147\n",
            "Epoch 79/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6641 - acc: 0.7894 - val_loss: 0.6063 - val_acc: 0.8194\n",
            "Epoch 80/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6551 - acc: 0.7970 - val_loss: 0.6115 - val_acc: 0.8176\n",
            "Epoch 81/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6537 - acc: 0.7957 - val_loss: 0.6116 - val_acc: 0.8188\n",
            "Epoch 82/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.6452 - acc: 0.7964 - val_loss: 0.6051 - val_acc: 0.8187\n",
            "Epoch 83/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6455 - acc: 0.7978 - val_loss: 0.6012 - val_acc: 0.8220\n",
            "Epoch 84/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6408 - acc: 0.8012 - val_loss: 0.5926 - val_acc: 0.8235\n",
            "Epoch 85/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6414 - acc: 0.8003 - val_loss: 0.6024 - val_acc: 0.8206\n",
            "Epoch 86/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6405 - acc: 0.7996 - val_loss: 0.5959 - val_acc: 0.8210\n",
            "Epoch 87/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6298 - acc: 0.8032 - val_loss: 0.6059 - val_acc: 0.8193\n",
            "Epoch 88/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6295 - acc: 0.8010 - val_loss: 0.6049 - val_acc: 0.8193\n",
            "Epoch 89/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6249 - acc: 0.8043 - val_loss: 0.5843 - val_acc: 0.8266\n",
            "Epoch 90/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6212 - acc: 0.8052 - val_loss: 0.5779 - val_acc: 0.8285\n",
            "Epoch 91/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6183 - acc: 0.8049 - val_loss: 0.6227 - val_acc: 0.8136\n",
            "Epoch 92/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6179 - acc: 0.8082 - val_loss: 0.5801 - val_acc: 0.8290\n",
            "Epoch 93/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6112 - acc: 0.8092 - val_loss: 0.5772 - val_acc: 0.8287\n",
            "Epoch 94/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6128 - acc: 0.8081 - val_loss: 0.5881 - val_acc: 0.8260\n",
            "Epoch 95/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6019 - acc: 0.8113 - val_loss: 0.5891 - val_acc: 0.8247\n",
            "Epoch 96/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6089 - acc: 0.8111 - val_loss: 0.5786 - val_acc: 0.8282\n",
            "Epoch 97/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6082 - acc: 0.8082 - val_loss: 0.6306 - val_acc: 0.8116\n",
            "Epoch 98/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.6042 - acc: 0.8113 - val_loss: 0.5764 - val_acc: 0.8289\n",
            "Epoch 99/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5923 - acc: 0.8136 - val_loss: 0.6023 - val_acc: 0.8233\n",
            "Epoch 100/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5954 - acc: 0.8138 - val_loss: 0.5703 - val_acc: 0.8319\n",
            "Epoch 101/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5928 - acc: 0.8134 - val_loss: 0.5677 - val_acc: 0.8338\n",
            "Epoch 102/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5896 - acc: 0.8133 - val_loss: 0.6028 - val_acc: 0.8211\n",
            "Epoch 103/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.5867 - acc: 0.8166 - val_loss: 0.5707 - val_acc: 0.8311\n",
            "Epoch 104/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5858 - acc: 0.8168 - val_loss: 0.5694 - val_acc: 0.8325\n",
            "Epoch 105/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5837 - acc: 0.8166 - val_loss: 0.6231 - val_acc: 0.8159\n",
            "Epoch 106/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5814 - acc: 0.8185 - val_loss: 0.6027 - val_acc: 0.8213\n",
            "Epoch 107/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5792 - acc: 0.8160 - val_loss: 0.5485 - val_acc: 0.8392\n",
            "Epoch 108/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5718 - acc: 0.8194 - val_loss: 0.5619 - val_acc: 0.8365\n",
            "Epoch 109/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5661 - acc: 0.8216 - val_loss: 0.5612 - val_acc: 0.8352\n",
            "Epoch 110/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5702 - acc: 0.8204 - val_loss: 0.5707 - val_acc: 0.8318\n",
            "Epoch 111/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5635 - acc: 0.8222 - val_loss: 0.5704 - val_acc: 0.8322\n",
            "Epoch 112/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5626 - acc: 0.8254 - val_loss: 0.5910 - val_acc: 0.8247\n",
            "Epoch 113/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5621 - acc: 0.8210 - val_loss: 0.5675 - val_acc: 0.8325\n",
            "Epoch 114/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5586 - acc: 0.8249 - val_loss: 0.5562 - val_acc: 0.8366\n",
            "Epoch 115/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5547 - acc: 0.8272 - val_loss: 0.5568 - val_acc: 0.8371\n",
            "Epoch 116/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5556 - acc: 0.8254 - val_loss: 0.5431 - val_acc: 0.8398\n",
            "Epoch 117/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5486 - acc: 0.8284 - val_loss: 0.5531 - val_acc: 0.8378\n",
            "Epoch 118/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5505 - acc: 0.8272 - val_loss: 0.5307 - val_acc: 0.8433\n",
            "Epoch 119/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5428 - acc: 0.8294 - val_loss: 0.5425 - val_acc: 0.8409\n",
            "Epoch 120/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5455 - acc: 0.8297 - val_loss: 0.5393 - val_acc: 0.8418\n",
            "Epoch 121/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5430 - acc: 0.8307 - val_loss: 0.5475 - val_acc: 0.8378\n",
            "Epoch 122/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5406 - acc: 0.8285 - val_loss: 0.5283 - val_acc: 0.8476\n",
            "Epoch 123/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5397 - acc: 0.8318 - val_loss: 0.5517 - val_acc: 0.8376\n",
            "Epoch 124/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5284 - acc: 0.8329 - val_loss: 0.5406 - val_acc: 0.8411\n",
            "Epoch 125/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5333 - acc: 0.8331 - val_loss: 0.5611 - val_acc: 0.8342\n",
            "Epoch 126/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5298 - acc: 0.8333 - val_loss: 0.5659 - val_acc: 0.8336\n",
            "Epoch 127/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5333 - acc: 0.8341 - val_loss: 0.5461 - val_acc: 0.8390\n",
            "Epoch 128/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5283 - acc: 0.8341 - val_loss: 0.5486 - val_acc: 0.8386\n",
            "Epoch 129/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5276 - acc: 0.8338 - val_loss: 0.5303 - val_acc: 0.8457\n",
            "Epoch 130/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5185 - acc: 0.8366 - val_loss: 0.5318 - val_acc: 0.8448\n",
            "Epoch 131/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5224 - acc: 0.8368 - val_loss: 0.5411 - val_acc: 0.8399\n",
            "Epoch 132/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5206 - acc: 0.8356 - val_loss: 0.5295 - val_acc: 0.8431\n",
            "Epoch 133/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5153 - acc: 0.8385 - val_loss: 0.5297 - val_acc: 0.8459\n",
            "Epoch 134/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5166 - acc: 0.8378 - val_loss: 0.5344 - val_acc: 0.8446\n",
            "Epoch 135/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5148 - acc: 0.8383 - val_loss: 0.5481 - val_acc: 0.8382\n",
            "Epoch 136/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5092 - acc: 0.8392 - val_loss: 0.5485 - val_acc: 0.8405\n",
            "Epoch 137/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.5128 - acc: 0.8398 - val_loss: 0.5559 - val_acc: 0.8392\n",
            "Epoch 138/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5126 - acc: 0.8392 - val_loss: 0.5283 - val_acc: 0.8473\n",
            "Epoch 139/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5042 - acc: 0.8413 - val_loss: 0.5670 - val_acc: 0.8307\n",
            "Epoch 140/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.5049 - acc: 0.8409 - val_loss: 0.5196 - val_acc: 0.8476\n",
            "Epoch 141/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4981 - acc: 0.8429 - val_loss: 0.5481 - val_acc: 0.8411\n",
            "Epoch 142/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4983 - acc: 0.8446 - val_loss: 0.5366 - val_acc: 0.8428\n",
            "Epoch 143/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4962 - acc: 0.8457 - val_loss: 0.5561 - val_acc: 0.8363\n",
            "Epoch 144/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4978 - acc: 0.8445 - val_loss: 0.5381 - val_acc: 0.8423\n",
            "Epoch 145/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4972 - acc: 0.8414 - val_loss: 0.5135 - val_acc: 0.8509\n",
            "Epoch 146/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4953 - acc: 0.8451 - val_loss: 0.5301 - val_acc: 0.8431\n",
            "Epoch 147/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4917 - acc: 0.8457 - val_loss: 0.5863 - val_acc: 0.8262\n",
            "Epoch 148/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4897 - acc: 0.8466 - val_loss: 0.5263 - val_acc: 0.8452\n",
            "Epoch 149/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4899 - acc: 0.8454 - val_loss: 0.5273 - val_acc: 0.8448\n",
            "Epoch 150/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4877 - acc: 0.8452 - val_loss: 0.5151 - val_acc: 0.8488\n",
            "Epoch 151/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4821 - acc: 0.8494 - val_loss: 0.5267 - val_acc: 0.8441\n",
            "Epoch 152/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4786 - acc: 0.8514 - val_loss: 0.5098 - val_acc: 0.8528\n",
            "Epoch 153/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4833 - acc: 0.8470 - val_loss: 0.5321 - val_acc: 0.8426\n",
            "Epoch 154/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4838 - acc: 0.8476 - val_loss: 0.5180 - val_acc: 0.8498\n",
            "Epoch 155/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4796 - acc: 0.8493 - val_loss: 0.5376 - val_acc: 0.8428\n",
            "Epoch 156/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4739 - acc: 0.8500 - val_loss: 0.5222 - val_acc: 0.8479\n",
            "Epoch 157/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4768 - acc: 0.8502 - val_loss: 0.5193 - val_acc: 0.8468\n",
            "Epoch 158/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4726 - acc: 0.8507 - val_loss: 0.5088 - val_acc: 0.8524\n",
            "Epoch 159/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4681 - acc: 0.8537 - val_loss: 0.5060 - val_acc: 0.8531\n",
            "Epoch 160/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4725 - acc: 0.8511 - val_loss: 0.5211 - val_acc: 0.8472\n",
            "Epoch 161/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4657 - acc: 0.8525 - val_loss: 0.5225 - val_acc: 0.8476\n",
            "Epoch 162/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4696 - acc: 0.8521 - val_loss: 0.5176 - val_acc: 0.8487\n",
            "Epoch 163/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4647 - acc: 0.8533 - val_loss: 0.4972 - val_acc: 0.8554\n",
            "Epoch 164/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4621 - acc: 0.8522 - val_loss: 0.5392 - val_acc: 0.8436\n",
            "Epoch 165/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4684 - acc: 0.8508 - val_loss: 0.5189 - val_acc: 0.8480\n",
            "Epoch 166/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4607 - acc: 0.8546 - val_loss: 0.5657 - val_acc: 0.8329\n",
            "Epoch 167/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4670 - acc: 0.8524 - val_loss: 0.5204 - val_acc: 0.8487\n",
            "Epoch 168/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4641 - acc: 0.8544 - val_loss: 0.5260 - val_acc: 0.8439\n",
            "Epoch 169/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4556 - acc: 0.8560 - val_loss: 0.5762 - val_acc: 0.8358\n",
            "Epoch 170/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4596 - acc: 0.8541 - val_loss: 0.5777 - val_acc: 0.8289\n",
            "Epoch 171/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4634 - acc: 0.8528 - val_loss: 0.5104 - val_acc: 0.8519\n",
            "Epoch 172/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4512 - acc: 0.8567 - val_loss: 0.4975 - val_acc: 0.8551\n",
            "Epoch 173/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4545 - acc: 0.8555 - val_loss: 0.5116 - val_acc: 0.8505\n",
            "Epoch 174/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4488 - acc: 0.8593 - val_loss: 0.5161 - val_acc: 0.8499\n",
            "Epoch 175/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4496 - acc: 0.8587 - val_loss: 0.4943 - val_acc: 0.8573\n",
            "Epoch 176/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4451 - acc: 0.8588 - val_loss: 0.5291 - val_acc: 0.8451\n",
            "Epoch 177/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4480 - acc: 0.8583 - val_loss: 0.5047 - val_acc: 0.8536\n",
            "Epoch 178/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4466 - acc: 0.8586 - val_loss: 0.5193 - val_acc: 0.8510\n",
            "Epoch 179/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4454 - acc: 0.8613 - val_loss: 0.5088 - val_acc: 0.8499\n",
            "Epoch 180/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4438 - acc: 0.8599 - val_loss: 0.5190 - val_acc: 0.8468\n",
            "Epoch 181/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4433 - acc: 0.8603 - val_loss: 0.5208 - val_acc: 0.8467\n",
            "Epoch 182/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4381 - acc: 0.8610 - val_loss: 0.5100 - val_acc: 0.8510\n",
            "Epoch 183/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4373 - acc: 0.8624 - val_loss: 0.4877 - val_acc: 0.8576\n",
            "Epoch 184/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4329 - acc: 0.8614 - val_loss: 0.5108 - val_acc: 0.8531\n",
            "Epoch 185/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4323 - acc: 0.8620 - val_loss: 0.5134 - val_acc: 0.8514\n",
            "Epoch 186/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4356 - acc: 0.8639 - val_loss: 0.5246 - val_acc: 0.8474\n",
            "Epoch 187/200\n",
            "33600/33600 [==============================] - 0s 9us/step - loss: 0.4339 - acc: 0.8616 - val_loss: 0.5472 - val_acc: 0.8395\n",
            "Epoch 188/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4354 - acc: 0.8617 - val_loss: 0.5069 - val_acc: 0.8537\n",
            "Epoch 189/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4253 - acc: 0.8661 - val_loss: 0.5087 - val_acc: 0.8514\n",
            "Epoch 190/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4317 - acc: 0.8647 - val_loss: 0.5303 - val_acc: 0.8476\n",
            "Epoch 191/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4270 - acc: 0.8643 - val_loss: 0.5042 - val_acc: 0.8522\n",
            "Epoch 192/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4287 - acc: 0.8647 - val_loss: 0.5293 - val_acc: 0.8454\n",
            "Epoch 193/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4266 - acc: 0.8636 - val_loss: 0.5007 - val_acc: 0.8549\n",
            "Epoch 194/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4215 - acc: 0.8669 - val_loss: 0.4868 - val_acc: 0.8597\n",
            "Epoch 195/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4226 - acc: 0.8659 - val_loss: 0.5054 - val_acc: 0.8536\n",
            "Epoch 196/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4242 - acc: 0.8657 - val_loss: 0.4997 - val_acc: 0.8561\n",
            "Epoch 197/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4174 - acc: 0.8694 - val_loss: 0.5095 - val_acc: 0.8502\n",
            "Epoch 198/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4187 - acc: 0.8679 - val_loss: 0.4861 - val_acc: 0.8599\n",
            "Epoch 199/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4134 - acc: 0.8687 - val_loss: 0.4997 - val_acc: 0.8557\n",
            "Epoch 200/200\n",
            "33600/33600 [==============================] - 0s 10us/step - loss: 0.4174 - acc: 0.8675 - val_loss: 0.5257 - val_acc: 0.8482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXJO6myDvxY1",
        "colab_type": "code",
        "outputId": "dded6cd5-9dd1-42ad-fe64-cdae725ee994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 1s 72us/step\n",
            "Test accuracy:  0.8458333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZtvD5Ecd90j",
        "colab_type": "code",
        "outputId": "c371e606-9c35-4a6c-a393-5373a40fe2ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Would the same combination work well with relu\n",
        "\n",
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(248*3, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(248*3, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu')) \n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(248*3, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.adam(lr = 0.0001)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=2048 ,epochs = 200, verbose = 1, validation_data=(X_test, y_test))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "33600/33600 [==============================] - 11s 322us/step - loss: 2.7842 - acc: 0.1097 - val_loss: 2.2464 - val_acc: 0.1743\n",
            "Epoch 2/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.6252 - acc: 0.1337 - val_loss: 2.1459 - val_acc: 0.2396\n",
            "Epoch 3/200\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 2.4924 - acc: 0.1595 - val_loss: 2.0590 - val_acc: 0.2958\n",
            "Epoch 4/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.3930 - acc: 0.1933 - val_loss: 2.0085 - val_acc: 0.2993\n",
            "Epoch 5/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.2711 - acc: 0.2276 - val_loss: 1.9628 - val_acc: 0.3281\n",
            "Epoch 6/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.1609 - acc: 0.2626 - val_loss: 1.8743 - val_acc: 0.3754\n",
            "Epoch 7/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 2.0371 - acc: 0.3047 - val_loss: 1.8043 - val_acc: 0.4026\n",
            "Epoch 8/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.9215 - acc: 0.3440 - val_loss: 1.7549 - val_acc: 0.4279\n",
            "Epoch 9/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.8139 - acc: 0.3801 - val_loss: 1.6553 - val_acc: 0.4623\n",
            "Epoch 10/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.7145 - acc: 0.4134 - val_loss: 1.5877 - val_acc: 0.4818\n",
            "Epoch 11/200\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 1.6382 - acc: 0.4442 - val_loss: 1.5019 - val_acc: 0.5303\n",
            "Epoch 12/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.5701 - acc: 0.4679 - val_loss: 1.4240 - val_acc: 0.5641\n",
            "Epoch 13/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.4970 - acc: 0.4958 - val_loss: 1.3576 - val_acc: 0.5898\n",
            "Epoch 14/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.4402 - acc: 0.5180 - val_loss: 1.2691 - val_acc: 0.6259\n",
            "Epoch 15/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.3910 - acc: 0.5375 - val_loss: 1.2152 - val_acc: 0.6417\n",
            "Epoch 16/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.3553 - acc: 0.5537 - val_loss: 1.1742 - val_acc: 0.6554\n",
            "Epoch 17/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.3095 - acc: 0.5680 - val_loss: 1.0962 - val_acc: 0.6768\n",
            "Epoch 18/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.2739 - acc: 0.5833 - val_loss: 1.0619 - val_acc: 0.6871\n",
            "Epoch 19/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.2302 - acc: 0.5980 - val_loss: 1.0355 - val_acc: 0.6951\n",
            "Epoch 20/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.2008 - acc: 0.6087 - val_loss: 1.0144 - val_acc: 0.7022\n",
            "Epoch 21/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.1881 - acc: 0.6148 - val_loss: 0.9796 - val_acc: 0.7087\n",
            "Epoch 22/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.1500 - acc: 0.6267 - val_loss: 0.9753 - val_acc: 0.7042\n",
            "Epoch 23/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.1328 - acc: 0.6367 - val_loss: 0.9519 - val_acc: 0.7100\n",
            "Epoch 24/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.1098 - acc: 0.6425 - val_loss: 0.9089 - val_acc: 0.7266\n",
            "Epoch 25/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.0895 - acc: 0.6471 - val_loss: 0.8831 - val_acc: 0.7330\n",
            "Epoch 26/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.0716 - acc: 0.6579 - val_loss: 0.8788 - val_acc: 0.7351\n",
            "Epoch 27/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.0464 - acc: 0.6636 - val_loss: 0.8731 - val_acc: 0.7336\n",
            "Epoch 28/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.0333 - acc: 0.6693 - val_loss: 0.8407 - val_acc: 0.7417\n",
            "Epoch 29/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 1.0198 - acc: 0.6744 - val_loss: 0.8439 - val_acc: 0.7429\n",
            "Epoch 30/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.9979 - acc: 0.6821 - val_loss: 0.8187 - val_acc: 0.7493\n",
            "Epoch 31/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.9927 - acc: 0.6838 - val_loss: 0.8154 - val_acc: 0.7514\n",
            "Epoch 32/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.9727 - acc: 0.6915 - val_loss: 0.8216 - val_acc: 0.7489\n",
            "Epoch 33/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.9613 - acc: 0.6942 - val_loss: 0.8027 - val_acc: 0.7520\n",
            "Epoch 34/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.9436 - acc: 0.6990 - val_loss: 0.7790 - val_acc: 0.7633\n",
            "Epoch 35/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.9332 - acc: 0.7030 - val_loss: 0.8024 - val_acc: 0.7519\n",
            "Epoch 36/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.9228 - acc: 0.7093 - val_loss: 0.7668 - val_acc: 0.7653\n",
            "Epoch 37/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.9079 - acc: 0.7141 - val_loss: 0.7569 - val_acc: 0.7687\n",
            "Epoch 38/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.9016 - acc: 0.7107 - val_loss: 0.7444 - val_acc: 0.7742\n",
            "Epoch 39/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.8901 - acc: 0.7176 - val_loss: 0.7443 - val_acc: 0.7726\n",
            "Epoch 40/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.8826 - acc: 0.7207 - val_loss: 0.7319 - val_acc: 0.7778\n",
            "Epoch 41/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.8743 - acc: 0.7225 - val_loss: 0.7383 - val_acc: 0.7742\n",
            "Epoch 42/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.8614 - acc: 0.7287 - val_loss: 0.7283 - val_acc: 0.7779\n",
            "Epoch 43/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.8534 - acc: 0.7262 - val_loss: 0.7075 - val_acc: 0.7826\n",
            "Epoch 44/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.8424 - acc: 0.7321 - val_loss: 0.7226 - val_acc: 0.7771\n",
            "Epoch 45/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.8330 - acc: 0.7367 - val_loss: 0.7179 - val_acc: 0.7817\n",
            "Epoch 46/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.8271 - acc: 0.7382 - val_loss: 0.6993 - val_acc: 0.7868\n",
            "Epoch 47/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.8153 - acc: 0.7409 - val_loss: 0.6975 - val_acc: 0.7865\n",
            "Epoch 48/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.8119 - acc: 0.7421 - val_loss: 0.6981 - val_acc: 0.7882\n",
            "Epoch 49/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7981 - acc: 0.7488 - val_loss: 0.6753 - val_acc: 0.7959\n",
            "Epoch 50/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7913 - acc: 0.7487 - val_loss: 0.7028 - val_acc: 0.7851\n",
            "Epoch 51/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7889 - acc: 0.7503 - val_loss: 0.6757 - val_acc: 0.7953\n",
            "Epoch 52/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7766 - acc: 0.7555 - val_loss: 0.6546 - val_acc: 0.8027\n",
            "Epoch 53/200\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.7716 - acc: 0.7560 - val_loss: 0.6591 - val_acc: 0.8009\n",
            "Epoch 54/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7660 - acc: 0.7594 - val_loss: 0.6671 - val_acc: 0.7948\n",
            "Epoch 55/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7589 - acc: 0.7597 - val_loss: 0.6657 - val_acc: 0.7963\n",
            "Epoch 56/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7567 - acc: 0.7605 - val_loss: 0.6384 - val_acc: 0.8078\n",
            "Epoch 57/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7442 - acc: 0.7649 - val_loss: 0.6372 - val_acc: 0.8097\n",
            "Epoch 58/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7401 - acc: 0.7655 - val_loss: 0.6456 - val_acc: 0.8032\n",
            "Epoch 59/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7337 - acc: 0.7705 - val_loss: 0.6352 - val_acc: 0.8095\n",
            "Epoch 60/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7332 - acc: 0.7685 - val_loss: 0.6347 - val_acc: 0.8092\n",
            "Epoch 61/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7195 - acc: 0.7732 - val_loss: 0.6507 - val_acc: 0.8019\n",
            "Epoch 62/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7185 - acc: 0.7731 - val_loss: 0.6275 - val_acc: 0.8109\n",
            "Epoch 63/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7040 - acc: 0.7780 - val_loss: 0.6324 - val_acc: 0.8074\n",
            "Epoch 64/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.7102 - acc: 0.7744 - val_loss: 0.6250 - val_acc: 0.8125\n",
            "Epoch 65/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6909 - acc: 0.7810 - val_loss: 0.6230 - val_acc: 0.8137\n",
            "Epoch 66/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6900 - acc: 0.7825 - val_loss: 0.6105 - val_acc: 0.8136\n",
            "Epoch 67/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6923 - acc: 0.7813 - val_loss: 0.6064 - val_acc: 0.8166\n",
            "Epoch 68/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6880 - acc: 0.7843 - val_loss: 0.6138 - val_acc: 0.8121\n",
            "Epoch 69/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6754 - acc: 0.7873 - val_loss: 0.6040 - val_acc: 0.8167\n",
            "Epoch 70/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6783 - acc: 0.7835 - val_loss: 0.5926 - val_acc: 0.8217\n",
            "Epoch 71/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6737 - acc: 0.7862 - val_loss: 0.5938 - val_acc: 0.8217\n",
            "Epoch 72/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6659 - acc: 0.7900 - val_loss: 0.6021 - val_acc: 0.8155\n",
            "Epoch 73/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6671 - acc: 0.7897 - val_loss: 0.5883 - val_acc: 0.8219\n",
            "Epoch 74/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6573 - acc: 0.7924 - val_loss: 0.5838 - val_acc: 0.8249\n",
            "Epoch 75/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6492 - acc: 0.7947 - val_loss: 0.6265 - val_acc: 0.8117\n",
            "Epoch 76/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6486 - acc: 0.7965 - val_loss: 0.5804 - val_acc: 0.8268\n",
            "Epoch 77/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6489 - acc: 0.7958 - val_loss: 0.6028 - val_acc: 0.8170\n",
            "Epoch 78/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6425 - acc: 0.7983 - val_loss: 0.5878 - val_acc: 0.8231\n",
            "Epoch 79/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6365 - acc: 0.7987 - val_loss: 0.5672 - val_acc: 0.8308\n",
            "Epoch 80/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6342 - acc: 0.7992 - val_loss: 0.5769 - val_acc: 0.8262\n",
            "Epoch 81/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6304 - acc: 0.8013 - val_loss: 0.5889 - val_acc: 0.8233\n",
            "Epoch 82/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6278 - acc: 0.8031 - val_loss: 0.5682 - val_acc: 0.8282\n",
            "Epoch 83/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6169 - acc: 0.8040 - val_loss: 0.5679 - val_acc: 0.8320\n",
            "Epoch 84/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6175 - acc: 0.8060 - val_loss: 0.5704 - val_acc: 0.8273\n",
            "Epoch 85/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6155 - acc: 0.8050 - val_loss: 0.5704 - val_acc: 0.8284\n",
            "Epoch 86/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6058 - acc: 0.8092 - val_loss: 0.5864 - val_acc: 0.8199\n",
            "Epoch 87/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6062 - acc: 0.8068 - val_loss: 0.5926 - val_acc: 0.8188\n",
            "Epoch 88/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6077 - acc: 0.8077 - val_loss: 0.5676 - val_acc: 0.8298\n",
            "Epoch 89/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.6011 - acc: 0.8109 - val_loss: 0.5675 - val_acc: 0.8288\n",
            "Epoch 90/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5972 - acc: 0.8117 - val_loss: 0.5781 - val_acc: 0.8259\n",
            "Epoch 91/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5896 - acc: 0.8124 - val_loss: 0.5437 - val_acc: 0.8367\n",
            "Epoch 92/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5909 - acc: 0.8129 - val_loss: 0.5424 - val_acc: 0.8383\n",
            "Epoch 93/200\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.5819 - acc: 0.8167 - val_loss: 0.5512 - val_acc: 0.8351\n",
            "Epoch 94/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5826 - acc: 0.8153 - val_loss: 0.5469 - val_acc: 0.8354\n",
            "Epoch 95/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5837 - acc: 0.8158 - val_loss: 0.5586 - val_acc: 0.8296\n",
            "Epoch 96/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5794 - acc: 0.8154 - val_loss: 0.5533 - val_acc: 0.8326\n",
            "Epoch 97/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5738 - acc: 0.8175 - val_loss: 0.5457 - val_acc: 0.8371\n",
            "Epoch 98/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5727 - acc: 0.8198 - val_loss: 0.5736 - val_acc: 0.8259\n",
            "Epoch 99/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5684 - acc: 0.8195 - val_loss: 0.5302 - val_acc: 0.8411\n",
            "Epoch 100/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5694 - acc: 0.8204 - val_loss: 0.5451 - val_acc: 0.8345\n",
            "Epoch 101/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5594 - acc: 0.8230 - val_loss: 0.5401 - val_acc: 0.8384\n",
            "Epoch 102/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5570 - acc: 0.8252 - val_loss: 0.5325 - val_acc: 0.8401\n",
            "Epoch 103/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5581 - acc: 0.8251 - val_loss: 0.5516 - val_acc: 0.8320\n",
            "Epoch 104/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5533 - acc: 0.8262 - val_loss: 0.5351 - val_acc: 0.8399\n",
            "Epoch 105/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5564 - acc: 0.8247 - val_loss: 0.5450 - val_acc: 0.8373\n",
            "Epoch 106/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5475 - acc: 0.8260 - val_loss: 0.5490 - val_acc: 0.8358\n",
            "Epoch 107/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5512 - acc: 0.8272 - val_loss: 0.5409 - val_acc: 0.8367\n",
            "Epoch 108/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5463 - acc: 0.8261 - val_loss: 0.5531 - val_acc: 0.8351\n",
            "Epoch 109/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5434 - acc: 0.8274 - val_loss: 0.5323 - val_acc: 0.8397\n",
            "Epoch 110/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5436 - acc: 0.8252 - val_loss: 0.5467 - val_acc: 0.8372\n",
            "Epoch 111/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5348 - acc: 0.8294 - val_loss: 0.5278 - val_acc: 0.8418\n",
            "Epoch 112/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5344 - acc: 0.8310 - val_loss: 0.5376 - val_acc: 0.8388\n",
            "Epoch 113/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5311 - acc: 0.8310 - val_loss: 0.5325 - val_acc: 0.8383\n",
            "Epoch 114/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5235 - acc: 0.8356 - val_loss: 0.5319 - val_acc: 0.8408\n",
            "Epoch 115/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5271 - acc: 0.8338 - val_loss: 0.5151 - val_acc: 0.8454\n",
            "Epoch 116/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5198 - acc: 0.8351 - val_loss: 0.5143 - val_acc: 0.8455\n",
            "Epoch 117/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5178 - acc: 0.8371 - val_loss: 0.5274 - val_acc: 0.8417\n",
            "Epoch 118/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5206 - acc: 0.8341 - val_loss: 0.5273 - val_acc: 0.8402\n",
            "Epoch 119/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5165 - acc: 0.8349 - val_loss: 0.5270 - val_acc: 0.8400\n",
            "Epoch 120/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5133 - acc: 0.8369 - val_loss: 0.5315 - val_acc: 0.8407\n",
            "Epoch 121/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5130 - acc: 0.8374 - val_loss: 0.5262 - val_acc: 0.8424\n",
            "Epoch 122/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5035 - acc: 0.8389 - val_loss: 0.5510 - val_acc: 0.8333\n",
            "Epoch 123/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5078 - acc: 0.8391 - val_loss: 0.5487 - val_acc: 0.8324\n",
            "Epoch 124/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5010 - acc: 0.8394 - val_loss: 0.5292 - val_acc: 0.8428\n",
            "Epoch 125/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4973 - acc: 0.8415 - val_loss: 0.5089 - val_acc: 0.8455\n",
            "Epoch 126/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.5018 - acc: 0.8406 - val_loss: 0.5172 - val_acc: 0.8448\n",
            "Epoch 127/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4957 - acc: 0.8419 - val_loss: 0.5147 - val_acc: 0.8462\n",
            "Epoch 128/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4937 - acc: 0.8432 - val_loss: 0.5239 - val_acc: 0.8428\n",
            "Epoch 129/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4963 - acc: 0.8418 - val_loss: 0.5369 - val_acc: 0.8385\n",
            "Epoch 130/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4924 - acc: 0.8431 - val_loss: 0.5439 - val_acc: 0.8378\n",
            "Epoch 131/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4920 - acc: 0.8426 - val_loss: 0.5144 - val_acc: 0.8479\n",
            "Epoch 132/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4955 - acc: 0.8436 - val_loss: 0.4963 - val_acc: 0.8530\n",
            "Epoch 133/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4920 - acc: 0.8430 - val_loss: 0.5176 - val_acc: 0.8440\n",
            "Epoch 134/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4859 - acc: 0.8450 - val_loss: 0.5209 - val_acc: 0.8446\n",
            "Epoch 135/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4773 - acc: 0.8465 - val_loss: 0.5036 - val_acc: 0.8513\n",
            "Epoch 136/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4780 - acc: 0.8467 - val_loss: 0.5050 - val_acc: 0.8486\n",
            "Epoch 137/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4828 - acc: 0.8453 - val_loss: 0.5050 - val_acc: 0.8494\n",
            "Epoch 138/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4766 - acc: 0.8481 - val_loss: 0.4975 - val_acc: 0.8506\n",
            "Epoch 139/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4752 - acc: 0.8492 - val_loss: 0.5042 - val_acc: 0.8504\n",
            "Epoch 140/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4761 - acc: 0.8476 - val_loss: 0.5087 - val_acc: 0.8478\n",
            "Epoch 141/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4692 - acc: 0.8512 - val_loss: 0.5102 - val_acc: 0.8458\n",
            "Epoch 142/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4625 - acc: 0.8522 - val_loss: 0.5050 - val_acc: 0.8509\n",
            "Epoch 143/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4639 - acc: 0.8502 - val_loss: 0.5052 - val_acc: 0.8486\n",
            "Epoch 144/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4697 - acc: 0.8494 - val_loss: 0.5259 - val_acc: 0.8423\n",
            "Epoch 145/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4676 - acc: 0.8496 - val_loss: 0.5282 - val_acc: 0.8432\n",
            "Epoch 146/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4617 - acc: 0.8520 - val_loss: 0.5029 - val_acc: 0.8506\n",
            "Epoch 147/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4602 - acc: 0.8543 - val_loss: 0.5036 - val_acc: 0.8502\n",
            "Epoch 148/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4548 - acc: 0.8531 - val_loss: 0.5127 - val_acc: 0.8472\n",
            "Epoch 149/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4547 - acc: 0.8536 - val_loss: 0.4803 - val_acc: 0.8576\n",
            "Epoch 150/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4521 - acc: 0.8579 - val_loss: 0.4906 - val_acc: 0.8555\n",
            "Epoch 151/200\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.4458 - acc: 0.8562 - val_loss: 0.5028 - val_acc: 0.8520\n",
            "Epoch 152/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4478 - acc: 0.8573 - val_loss: 0.4944 - val_acc: 0.8539\n",
            "Epoch 153/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4462 - acc: 0.8561 - val_loss: 0.4945 - val_acc: 0.8522\n",
            "Epoch 154/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4483 - acc: 0.8576 - val_loss: 0.4977 - val_acc: 0.8521\n",
            "Epoch 155/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4418 - acc: 0.8587 - val_loss: 0.4840 - val_acc: 0.8563\n",
            "Epoch 156/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4475 - acc: 0.8550 - val_loss: 0.4850 - val_acc: 0.8559\n",
            "Epoch 157/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4413 - acc: 0.8587 - val_loss: 0.5101 - val_acc: 0.8493\n",
            "Epoch 158/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4393 - acc: 0.8612 - val_loss: 0.4901 - val_acc: 0.8553\n",
            "Epoch 159/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4417 - acc: 0.8588 - val_loss: 0.5178 - val_acc: 0.8466\n",
            "Epoch 160/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4392 - acc: 0.8590 - val_loss: 0.5233 - val_acc: 0.8456\n",
            "Epoch 161/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4370 - acc: 0.8607 - val_loss: 0.4920 - val_acc: 0.8527\n",
            "Epoch 162/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4330 - acc: 0.8616 - val_loss: 0.4869 - val_acc: 0.8572\n",
            "Epoch 163/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4354 - acc: 0.8626 - val_loss: 0.5117 - val_acc: 0.8464\n",
            "Epoch 164/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4335 - acc: 0.8615 - val_loss: 0.5371 - val_acc: 0.8407\n",
            "Epoch 165/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4333 - acc: 0.8638 - val_loss: 0.5295 - val_acc: 0.8445\n",
            "Epoch 166/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4287 - acc: 0.8631 - val_loss: 0.4993 - val_acc: 0.8521\n",
            "Epoch 167/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4316 - acc: 0.8609 - val_loss: 0.5427 - val_acc: 0.8431\n",
            "Epoch 168/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4247 - acc: 0.8644 - val_loss: 0.4824 - val_acc: 0.8562\n",
            "Epoch 169/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4199 - acc: 0.8660 - val_loss: 0.4796 - val_acc: 0.8598\n",
            "Epoch 170/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4215 - acc: 0.8634 - val_loss: 0.4855 - val_acc: 0.8573\n",
            "Epoch 171/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4190 - acc: 0.8659 - val_loss: 0.4925 - val_acc: 0.8542\n",
            "Epoch 172/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4191 - acc: 0.8650 - val_loss: 0.4999 - val_acc: 0.8522\n",
            "Epoch 173/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4172 - acc: 0.8658 - val_loss: 0.4940 - val_acc: 0.8542\n",
            "Epoch 174/200\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.4137 - acc: 0.8667 - val_loss: 0.4926 - val_acc: 0.8523\n",
            "Epoch 175/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4219 - acc: 0.8639 - val_loss: 0.4853 - val_acc: 0.8553\n",
            "Epoch 176/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4136 - acc: 0.8658 - val_loss: 0.4927 - val_acc: 0.8532\n",
            "Epoch 177/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4158 - acc: 0.8665 - val_loss: 0.4795 - val_acc: 0.8596\n",
            "Epoch 178/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4087 - acc: 0.8680 - val_loss: 0.5236 - val_acc: 0.8444\n",
            "Epoch 179/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4104 - acc: 0.8682 - val_loss: 0.4793 - val_acc: 0.8580\n",
            "Epoch 180/200\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.4049 - acc: 0.8707 - val_loss: 0.4796 - val_acc: 0.8568\n",
            "Epoch 181/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4012 - acc: 0.8704 - val_loss: 0.4844 - val_acc: 0.8579\n",
            "Epoch 182/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4029 - acc: 0.8702 - val_loss: 0.5425 - val_acc: 0.8424\n",
            "Epoch 183/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4037 - acc: 0.8719 - val_loss: 0.4745 - val_acc: 0.8591\n",
            "Epoch 184/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4039 - acc: 0.8690 - val_loss: 0.4796 - val_acc: 0.8601\n",
            "Epoch 185/200\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.4032 - acc: 0.8711 - val_loss: 0.5095 - val_acc: 0.8496\n",
            "Epoch 186/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4026 - acc: 0.8702 - val_loss: 0.4988 - val_acc: 0.8527\n",
            "Epoch 187/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4061 - acc: 0.8693 - val_loss: 0.4968 - val_acc: 0.8532\n",
            "Epoch 188/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4056 - acc: 0.8713 - val_loss: 0.4932 - val_acc: 0.8538\n",
            "Epoch 189/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.4028 - acc: 0.8715 - val_loss: 0.4954 - val_acc: 0.8527\n",
            "Epoch 190/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3969 - acc: 0.8745 - val_loss: 0.4712 - val_acc: 0.8606\n",
            "Epoch 191/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3902 - acc: 0.8751 - val_loss: 0.4800 - val_acc: 0.8585\n",
            "Epoch 192/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3910 - acc: 0.8741 - val_loss: 0.4790 - val_acc: 0.8596\n",
            "Epoch 193/200\n",
            "33600/33600 [==============================] - 0s 12us/step - loss: 0.3892 - acc: 0.8739 - val_loss: 0.4955 - val_acc: 0.8557\n",
            "Epoch 194/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3908 - acc: 0.8751 - val_loss: 0.4994 - val_acc: 0.8517\n",
            "Epoch 195/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3873 - acc: 0.8765 - val_loss: 0.4631 - val_acc: 0.8651\n",
            "Epoch 196/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3837 - acc: 0.8760 - val_loss: 0.4719 - val_acc: 0.8612\n",
            "Epoch 197/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3850 - acc: 0.8763 - val_loss: 0.4947 - val_acc: 0.8546\n",
            "Epoch 198/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3853 - acc: 0.8751 - val_loss: 0.5203 - val_acc: 0.8472\n",
            "Epoch 199/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3797 - acc: 0.8775 - val_loss: 0.4958 - val_acc: 0.8533\n",
            "Epoch 200/200\n",
            "33600/33600 [==============================] - 0s 11us/step - loss: 0.3780 - acc: 0.8785 - val_loss: 0.4737 - val_acc: 0.8592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Q-EJtid9xe",
        "colab_type": "code",
        "outputId": "74f70f42-f7fb-4921-b43b-cba0137977c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 0s 56us/step\n",
            "Test accuracy:  0.8567857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGXTUaJ24WRg",
        "colab_type": "text"
      },
      "source": [
        "Best scores so far with 3 layers and relu as the activation function and highest drop out which provides more regularization. Let's try more drop outs, more neurons in each layer and increase the learning rate to 0.001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwd36uidd9tq",
        "colab_type": "code",
        "outputId": "d3d632db-5039-4e8e-97b4-0452613362d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(248*4, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(248*4, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu')) \n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(248*4, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.adam(lr = 0.0001)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=2048 ,epochs = 200, verbose = 1, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/200\n",
            "33600/33600 [==============================] - 4s 113us/step - loss: 2.9243 - acc: 0.1049 - val_loss: 2.2426 - val_acc: 0.1880\n",
            "Epoch 2/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.7729 - acc: 0.1231 - val_loss: 2.1602 - val_acc: 0.2448\n",
            "Epoch 3/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.6738 - acc: 0.1399 - val_loss: 2.0937 - val_acc: 0.2945\n",
            "Epoch 4/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.5685 - acc: 0.1646 - val_loss: 2.0389 - val_acc: 0.3262\n",
            "Epoch 5/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.4749 - acc: 0.1840 - val_loss: 1.9746 - val_acc: 0.3476\n",
            "Epoch 6/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.3824 - acc: 0.2091 - val_loss: 1.9148 - val_acc: 0.3683\n",
            "Epoch 7/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 2.2655 - acc: 0.2433 - val_loss: 1.8685 - val_acc: 0.3753\n",
            "Epoch 8/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.1707 - acc: 0.2665 - val_loss: 1.7951 - val_acc: 0.4111\n",
            "Epoch 9/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.0607 - acc: 0.3001 - val_loss: 1.7662 - val_acc: 0.4196\n",
            "Epoch 10/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.9705 - acc: 0.3288 - val_loss: 1.7022 - val_acc: 0.4452\n",
            "Epoch 11/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.8805 - acc: 0.3562 - val_loss: 1.6394 - val_acc: 0.4583\n",
            "Epoch 12/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.7989 - acc: 0.3869 - val_loss: 1.5602 - val_acc: 0.4882\n",
            "Epoch 13/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.7396 - acc: 0.4103 - val_loss: 1.4916 - val_acc: 0.5244\n",
            "Epoch 14/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.6718 - acc: 0.4285 - val_loss: 1.4539 - val_acc: 0.5438\n",
            "Epoch 15/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.6147 - acc: 0.4492 - val_loss: 1.3518 - val_acc: 0.5816\n",
            "Epoch 16/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.5598 - acc: 0.4716 - val_loss: 1.3051 - val_acc: 0.6048\n",
            "Epoch 17/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.5059 - acc: 0.4898 - val_loss: 1.2522 - val_acc: 0.6081\n",
            "Epoch 18/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.4757 - acc: 0.5038 - val_loss: 1.2213 - val_acc: 0.6230\n",
            "Epoch 19/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.4262 - acc: 0.5233 - val_loss: 1.1522 - val_acc: 0.6474\n",
            "Epoch 20/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.3866 - acc: 0.5383 - val_loss: 1.1368 - val_acc: 0.6569\n",
            "Epoch 21/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.3677 - acc: 0.5469 - val_loss: 1.0886 - val_acc: 0.6754\n",
            "Epoch 22/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.3359 - acc: 0.5563 - val_loss: 1.0496 - val_acc: 0.6794\n",
            "Epoch 23/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.2901 - acc: 0.5723 - val_loss: 1.0314 - val_acc: 0.6943\n",
            "Epoch 24/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.2702 - acc: 0.5815 - val_loss: 1.0228 - val_acc: 0.6852\n",
            "Epoch 25/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.2503 - acc: 0.5898 - val_loss: 0.9779 - val_acc: 0.7049\n",
            "Epoch 26/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.2220 - acc: 0.6015 - val_loss: 0.9326 - val_acc: 0.7151\n",
            "Epoch 27/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.1986 - acc: 0.6090 - val_loss: 0.9298 - val_acc: 0.7198\n",
            "Epoch 28/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.1808 - acc: 0.6158 - val_loss: 0.9186 - val_acc: 0.7160\n",
            "Epoch 29/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.1566 - acc: 0.6251 - val_loss: 0.9205 - val_acc: 0.7189\n",
            "Epoch 30/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.1418 - acc: 0.6273 - val_loss: 0.9068 - val_acc: 0.7226\n",
            "Epoch 31/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.1141 - acc: 0.6406 - val_loss: 0.8841 - val_acc: 0.7243\n",
            "Epoch 32/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.1026 - acc: 0.6444 - val_loss: 0.8583 - val_acc: 0.7335\n",
            "Epoch 33/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0832 - acc: 0.6479 - val_loss: 0.8352 - val_acc: 0.7429\n",
            "Epoch 34/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0748 - acc: 0.6536 - val_loss: 0.8311 - val_acc: 0.7448\n",
            "Epoch 35/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0577 - acc: 0.6597 - val_loss: 0.8461 - val_acc: 0.7391\n",
            "Epoch 36/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0441 - acc: 0.6659 - val_loss: 0.8304 - val_acc: 0.7439\n",
            "Epoch 37/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0343 - acc: 0.6691 - val_loss: 0.8207 - val_acc: 0.7454\n",
            "Epoch 38/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0083 - acc: 0.6766 - val_loss: 0.7935 - val_acc: 0.7588\n",
            "Epoch 39/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0013 - acc: 0.6793 - val_loss: 0.7848 - val_acc: 0.7572\n",
            "Epoch 40/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9797 - acc: 0.6890 - val_loss: 0.7837 - val_acc: 0.7593\n",
            "Epoch 41/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9799 - acc: 0.6860 - val_loss: 0.7731 - val_acc: 0.7621\n",
            "Epoch 42/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9674 - acc: 0.6901 - val_loss: 0.7670 - val_acc: 0.7647\n",
            "Epoch 43/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9548 - acc: 0.6948 - val_loss: 0.7662 - val_acc: 0.7654\n",
            "Epoch 44/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9399 - acc: 0.7029 - val_loss: 0.7489 - val_acc: 0.7697\n",
            "Epoch 45/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9399 - acc: 0.7017 - val_loss: 0.7541 - val_acc: 0.7700\n",
            "Epoch 46/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9275 - acc: 0.7043 - val_loss: 0.7459 - val_acc: 0.7690\n",
            "Epoch 47/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9178 - acc: 0.7088 - val_loss: 0.7338 - val_acc: 0.7733\n",
            "Epoch 48/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9100 - acc: 0.7123 - val_loss: 0.7258 - val_acc: 0.7771\n",
            "Epoch 49/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8884 - acc: 0.7188 - val_loss: 0.7424 - val_acc: 0.7689\n",
            "Epoch 50/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8886 - acc: 0.7170 - val_loss: 0.7276 - val_acc: 0.7761\n",
            "Epoch 51/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8843 - acc: 0.7203 - val_loss: 0.7020 - val_acc: 0.7845\n",
            "Epoch 52/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8771 - acc: 0.7191 - val_loss: 0.7043 - val_acc: 0.7849\n",
            "Epoch 53/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8643 - acc: 0.7220 - val_loss: 0.6972 - val_acc: 0.7868\n",
            "Epoch 54/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8624 - acc: 0.7246 - val_loss: 0.6831 - val_acc: 0.7898\n",
            "Epoch 55/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8577 - acc: 0.7281 - val_loss: 0.7082 - val_acc: 0.7828\n",
            "Epoch 56/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8390 - acc: 0.7307 - val_loss: 0.6903 - val_acc: 0.7890\n",
            "Epoch 57/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8436 - acc: 0.7324 - val_loss: 0.6738 - val_acc: 0.7947\n",
            "Epoch 58/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8228 - acc: 0.7379 - val_loss: 0.6670 - val_acc: 0.7958\n",
            "Epoch 59/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8253 - acc: 0.7381 - val_loss: 0.6814 - val_acc: 0.7920\n",
            "Epoch 60/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.8169 - acc: 0.7427 - val_loss: 0.6801 - val_acc: 0.7902\n",
            "Epoch 61/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8096 - acc: 0.7457 - val_loss: 0.6906 - val_acc: 0.7861\n",
            "Epoch 62/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8037 - acc: 0.7446 - val_loss: 0.6634 - val_acc: 0.7976\n",
            "Epoch 63/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7898 - acc: 0.7522 - val_loss: 0.6735 - val_acc: 0.7937\n",
            "Epoch 64/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7904 - acc: 0.7489 - val_loss: 0.6459 - val_acc: 0.8028\n",
            "Epoch 65/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7880 - acc: 0.7503 - val_loss: 0.6511 - val_acc: 0.8008\n",
            "Epoch 66/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7794 - acc: 0.7539 - val_loss: 0.6403 - val_acc: 0.8058\n",
            "Epoch 67/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7799 - acc: 0.7541 - val_loss: 0.6485 - val_acc: 0.8022\n",
            "Epoch 68/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7685 - acc: 0.7572 - val_loss: 0.6442 - val_acc: 0.8016\n",
            "Epoch 69/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7622 - acc: 0.7583 - val_loss: 0.6395 - val_acc: 0.8044\n",
            "Epoch 70/200\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 0.7589 - acc: 0.7613 - val_loss: 0.6402 - val_acc: 0.8021\n",
            "Epoch 71/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7524 - acc: 0.7601 - val_loss: 0.6418 - val_acc: 0.8026\n",
            "Epoch 72/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7443 - acc: 0.7644 - val_loss: 0.6520 - val_acc: 0.8013\n",
            "Epoch 73/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7444 - acc: 0.7665 - val_loss: 0.6261 - val_acc: 0.8098\n",
            "Epoch 74/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7427 - acc: 0.7651 - val_loss: 0.6328 - val_acc: 0.8065\n",
            "Epoch 75/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7307 - acc: 0.7694 - val_loss: 0.6170 - val_acc: 0.8102\n",
            "Epoch 76/200\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 0.7329 - acc: 0.7678 - val_loss: 0.6225 - val_acc: 0.8116\n",
            "Epoch 77/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7211 - acc: 0.7743 - val_loss: 0.6332 - val_acc: 0.8081\n",
            "Epoch 78/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7126 - acc: 0.7721 - val_loss: 0.6343 - val_acc: 0.8060\n",
            "Epoch 79/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7185 - acc: 0.7730 - val_loss: 0.6318 - val_acc: 0.8084\n",
            "Epoch 80/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7125 - acc: 0.7767 - val_loss: 0.6514 - val_acc: 0.8025\n",
            "Epoch 81/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7095 - acc: 0.7760 - val_loss: 0.6080 - val_acc: 0.8156\n",
            "Epoch 82/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7027 - acc: 0.7767 - val_loss: 0.6195 - val_acc: 0.8100\n",
            "Epoch 83/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6973 - acc: 0.7793 - val_loss: 0.6242 - val_acc: 0.8124\n",
            "Epoch 84/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7026 - acc: 0.7786 - val_loss: 0.6308 - val_acc: 0.8053\n",
            "Epoch 85/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6905 - acc: 0.7806 - val_loss: 0.6073 - val_acc: 0.8146\n",
            "Epoch 86/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6910 - acc: 0.7819 - val_loss: 0.5980 - val_acc: 0.8178\n",
            "Epoch 87/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6797 - acc: 0.7835 - val_loss: 0.6115 - val_acc: 0.8152\n",
            "Epoch 88/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6751 - acc: 0.7858 - val_loss: 0.5891 - val_acc: 0.8226\n",
            "Epoch 89/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6671 - acc: 0.7883 - val_loss: 0.5877 - val_acc: 0.8217\n",
            "Epoch 90/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6737 - acc: 0.7866 - val_loss: 0.6162 - val_acc: 0.8129\n",
            "Epoch 91/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6643 - acc: 0.7891 - val_loss: 0.5878 - val_acc: 0.8219\n",
            "Epoch 92/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6562 - acc: 0.7919 - val_loss: 0.5955 - val_acc: 0.8184\n",
            "Epoch 93/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6615 - acc: 0.7879 - val_loss: 0.5785 - val_acc: 0.8253\n",
            "Epoch 94/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6605 - acc: 0.7909 - val_loss: 0.5835 - val_acc: 0.8236\n",
            "Epoch 95/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6542 - acc: 0.7946 - val_loss: 0.5804 - val_acc: 0.8250\n",
            "Epoch 96/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6462 - acc: 0.7954 - val_loss: 0.5717 - val_acc: 0.8265\n",
            "Epoch 97/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6463 - acc: 0.7966 - val_loss: 0.5763 - val_acc: 0.8243\n",
            "Epoch 98/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6417 - acc: 0.7971 - val_loss: 0.5690 - val_acc: 0.8283\n",
            "Epoch 99/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6431 - acc: 0.7944 - val_loss: 0.5880 - val_acc: 0.8234\n",
            "Epoch 100/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6339 - acc: 0.7999 - val_loss: 0.5663 - val_acc: 0.8289\n",
            "Epoch 101/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6350 - acc: 0.7989 - val_loss: 0.5668 - val_acc: 0.8291\n",
            "Epoch 102/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6271 - acc: 0.8023 - val_loss: 0.5562 - val_acc: 0.8334\n",
            "Epoch 103/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6284 - acc: 0.8000 - val_loss: 0.5650 - val_acc: 0.8300\n",
            "Epoch 104/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6207 - acc: 0.8029 - val_loss: 0.5648 - val_acc: 0.8303\n",
            "Epoch 105/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6206 - acc: 0.8064 - val_loss: 0.5560 - val_acc: 0.8299\n",
            "Epoch 106/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6158 - acc: 0.8047 - val_loss: 0.5662 - val_acc: 0.8312\n",
            "Epoch 107/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6165 - acc: 0.8054 - val_loss: 0.5789 - val_acc: 0.8222\n",
            "Epoch 108/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6116 - acc: 0.8067 - val_loss: 0.5653 - val_acc: 0.8274\n",
            "Epoch 109/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6052 - acc: 0.8069 - val_loss: 0.5660 - val_acc: 0.8285\n",
            "Epoch 110/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6045 - acc: 0.8097 - val_loss: 0.5745 - val_acc: 0.8274\n",
            "Epoch 111/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6007 - acc: 0.8081 - val_loss: 0.5428 - val_acc: 0.8374\n",
            "Epoch 112/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6039 - acc: 0.8074 - val_loss: 0.5605 - val_acc: 0.8309\n",
            "Epoch 113/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6005 - acc: 0.8102 - val_loss: 0.5894 - val_acc: 0.8213\n",
            "Epoch 114/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5997 - acc: 0.8111 - val_loss: 0.5401 - val_acc: 0.8385\n",
            "Epoch 115/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5913 - acc: 0.8134 - val_loss: 0.5428 - val_acc: 0.8394\n",
            "Epoch 116/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5850 - acc: 0.8149 - val_loss: 0.5588 - val_acc: 0.8316\n",
            "Epoch 117/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5840 - acc: 0.8138 - val_loss: 0.5666 - val_acc: 0.8318\n",
            "Epoch 118/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5845 - acc: 0.8138 - val_loss: 0.5559 - val_acc: 0.8307\n",
            "Epoch 119/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5851 - acc: 0.8169 - val_loss: 0.5614 - val_acc: 0.8306\n",
            "Epoch 120/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5777 - acc: 0.8176 - val_loss: 0.5422 - val_acc: 0.8374\n",
            "Epoch 121/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5733 - acc: 0.8184 - val_loss: 0.5429 - val_acc: 0.8374\n",
            "Epoch 122/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5777 - acc: 0.8163 - val_loss: 0.5442 - val_acc: 0.8360\n",
            "Epoch 123/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5683 - acc: 0.8190 - val_loss: 0.5480 - val_acc: 0.8359\n",
            "Epoch 124/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5705 - acc: 0.8207 - val_loss: 0.5400 - val_acc: 0.8376\n",
            "Epoch 125/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5596 - acc: 0.8228 - val_loss: 0.5360 - val_acc: 0.8393\n",
            "Epoch 126/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5685 - acc: 0.8185 - val_loss: 0.5631 - val_acc: 0.8318\n",
            "Epoch 127/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5633 - acc: 0.8197 - val_loss: 0.5473 - val_acc: 0.8346\n",
            "Epoch 128/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5599 - acc: 0.8221 - val_loss: 0.5369 - val_acc: 0.8413\n",
            "Epoch 129/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5545 - acc: 0.8241 - val_loss: 0.5253 - val_acc: 0.8412\n",
            "Epoch 130/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5569 - acc: 0.8232 - val_loss: 0.5365 - val_acc: 0.8396\n",
            "Epoch 131/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5579 - acc: 0.8238 - val_loss: 0.5324 - val_acc: 0.8428\n",
            "Epoch 132/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5466 - acc: 0.8278 - val_loss: 0.5430 - val_acc: 0.8380\n",
            "Epoch 133/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5442 - acc: 0.8273 - val_loss: 0.5278 - val_acc: 0.8433\n",
            "Epoch 134/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5464 - acc: 0.8276 - val_loss: 0.5267 - val_acc: 0.8424\n",
            "Epoch 135/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5419 - acc: 0.8290 - val_loss: 0.5317 - val_acc: 0.8381\n",
            "Epoch 136/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5422 - acc: 0.8285 - val_loss: 0.5355 - val_acc: 0.8381\n",
            "Epoch 137/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5405 - acc: 0.8292 - val_loss: 0.5394 - val_acc: 0.8362\n",
            "Epoch 138/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5347 - acc: 0.8285 - val_loss: 0.5236 - val_acc: 0.8448\n",
            "Epoch 139/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5347 - acc: 0.8312 - val_loss: 0.5177 - val_acc: 0.8451\n",
            "Epoch 140/200\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 0.5276 - acc: 0.8316 - val_loss: 0.5252 - val_acc: 0.8433\n",
            "Epoch 141/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5280 - acc: 0.8323 - val_loss: 0.5220 - val_acc: 0.8462\n",
            "Epoch 142/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5329 - acc: 0.8317 - val_loss: 0.5364 - val_acc: 0.8406\n",
            "Epoch 143/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5228 - acc: 0.8322 - val_loss: 0.5237 - val_acc: 0.8442\n",
            "Epoch 144/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5245 - acc: 0.8350 - val_loss: 0.5075 - val_acc: 0.8474\n",
            "Epoch 145/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5169 - acc: 0.8351 - val_loss: 0.5228 - val_acc: 0.8447\n",
            "Epoch 146/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5172 - acc: 0.8362 - val_loss: 0.5312 - val_acc: 0.8429\n",
            "Epoch 147/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5126 - acc: 0.8364 - val_loss: 0.5300 - val_acc: 0.8422\n",
            "Epoch 148/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5156 - acc: 0.8383 - val_loss: 0.5756 - val_acc: 0.8278\n",
            "Epoch 149/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5196 - acc: 0.8344 - val_loss: 0.5225 - val_acc: 0.8419\n",
            "Epoch 150/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5124 - acc: 0.8348 - val_loss: 0.5381 - val_acc: 0.8379\n",
            "Epoch 151/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5118 - acc: 0.8348 - val_loss: 0.5301 - val_acc: 0.8426\n",
            "Epoch 152/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5093 - acc: 0.8368 - val_loss: 0.5457 - val_acc: 0.8355\n",
            "Epoch 153/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5083 - acc: 0.8376 - val_loss: 0.5099 - val_acc: 0.8478\n",
            "Epoch 154/200\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 0.5021 - acc: 0.8394 - val_loss: 0.5200 - val_acc: 0.8452\n",
            "Epoch 155/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4993 - acc: 0.8432 - val_loss: 0.5051 - val_acc: 0.8503\n",
            "Epoch 156/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5057 - acc: 0.8398 - val_loss: 0.5049 - val_acc: 0.8497\n",
            "Epoch 157/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4986 - acc: 0.8422 - val_loss: 0.4961 - val_acc: 0.8550\n",
            "Epoch 158/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4947 - acc: 0.8427 - val_loss: 0.5181 - val_acc: 0.8448\n",
            "Epoch 159/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5003 - acc: 0.8410 - val_loss: 0.5111 - val_acc: 0.8461\n",
            "Epoch 160/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4977 - acc: 0.8412 - val_loss: 0.5100 - val_acc: 0.8482\n",
            "Epoch 161/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4929 - acc: 0.8437 - val_loss: 0.5159 - val_acc: 0.8459\n",
            "Epoch 162/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4931 - acc: 0.8434 - val_loss: 0.5034 - val_acc: 0.8518\n",
            "Epoch 163/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4861 - acc: 0.8449 - val_loss: 0.4974 - val_acc: 0.8524\n",
            "Epoch 164/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4899 - acc: 0.8415 - val_loss: 0.5015 - val_acc: 0.8506\n",
            "Epoch 165/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4902 - acc: 0.8437 - val_loss: 0.4904 - val_acc: 0.8563\n",
            "Epoch 166/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4826 - acc: 0.8457 - val_loss: 0.5350 - val_acc: 0.8391\n",
            "Epoch 167/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4897 - acc: 0.8434 - val_loss: 0.5036 - val_acc: 0.8492\n",
            "Epoch 168/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4769 - acc: 0.8471 - val_loss: 0.4981 - val_acc: 0.8516\n",
            "Epoch 169/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4731 - acc: 0.8500 - val_loss: 0.4971 - val_acc: 0.8503\n",
            "Epoch 170/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4810 - acc: 0.8471 - val_loss: 0.5539 - val_acc: 0.8322\n",
            "Epoch 171/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4810 - acc: 0.8450 - val_loss: 0.5113 - val_acc: 0.8484\n",
            "Epoch 172/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4732 - acc: 0.8494 - val_loss: 0.5051 - val_acc: 0.8477\n",
            "Epoch 173/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4731 - acc: 0.8506 - val_loss: 0.4898 - val_acc: 0.8554\n",
            "Epoch 174/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4721 - acc: 0.8482 - val_loss: 0.4896 - val_acc: 0.8553\n",
            "Epoch 175/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4761 - acc: 0.8508 - val_loss: 0.4981 - val_acc: 0.8509\n",
            "Epoch 176/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4712 - acc: 0.8487 - val_loss: 0.5006 - val_acc: 0.8504\n",
            "Epoch 177/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4657 - acc: 0.8521 - val_loss: 0.5042 - val_acc: 0.8494\n",
            "Epoch 178/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4626 - acc: 0.8524 - val_loss: 0.5068 - val_acc: 0.8498\n",
            "Epoch 179/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4619 - acc: 0.8520 - val_loss: 0.5079 - val_acc: 0.8476\n",
            "Epoch 180/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4582 - acc: 0.8553 - val_loss: 0.4999 - val_acc: 0.8514\n",
            "Epoch 181/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4532 - acc: 0.8558 - val_loss: 0.5424 - val_acc: 0.8379\n",
            "Epoch 182/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4589 - acc: 0.8534 - val_loss: 0.4898 - val_acc: 0.8538\n",
            "Epoch 183/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4580 - acc: 0.8547 - val_loss: 0.5010 - val_acc: 0.8514\n",
            "Epoch 184/200\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 0.4578 - acc: 0.8527 - val_loss: 0.4923 - val_acc: 0.8547\n",
            "Epoch 185/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4538 - acc: 0.8560 - val_loss: 0.5083 - val_acc: 0.8509\n",
            "Epoch 186/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4522 - acc: 0.8563 - val_loss: 0.5041 - val_acc: 0.8498\n",
            "Epoch 187/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4469 - acc: 0.8574 - val_loss: 0.5124 - val_acc: 0.8475\n",
            "Epoch 188/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4548 - acc: 0.8549 - val_loss: 0.4895 - val_acc: 0.8558\n",
            "Epoch 189/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4452 - acc: 0.8582 - val_loss: 0.5069 - val_acc: 0.8490\n",
            "Epoch 190/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4477 - acc: 0.8577 - val_loss: 0.4999 - val_acc: 0.8537\n",
            "Epoch 191/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4451 - acc: 0.8576 - val_loss: 0.4919 - val_acc: 0.8546\n",
            "Epoch 192/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4455 - acc: 0.8585 - val_loss: 0.4831 - val_acc: 0.8576\n",
            "Epoch 193/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4409 - acc: 0.8585 - val_loss: 0.5232 - val_acc: 0.8481\n",
            "Epoch 194/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4431 - acc: 0.8584 - val_loss: 0.5276 - val_acc: 0.8454\n",
            "Epoch 195/200\n",
            "33600/33600 [==============================] - 0s 13us/step - loss: 0.4407 - acc: 0.8585 - val_loss: 0.4950 - val_acc: 0.8530\n",
            "Epoch 196/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4411 - acc: 0.8588 - val_loss: 0.4789 - val_acc: 0.8578\n",
            "Epoch 197/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4444 - acc: 0.8564 - val_loss: 0.5334 - val_acc: 0.8412\n",
            "Epoch 198/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4444 - acc: 0.8605 - val_loss: 0.4809 - val_acc: 0.8586\n",
            "Epoch 199/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4388 - acc: 0.8593 - val_loss: 0.5092 - val_acc: 0.8508\n",
            "Epoch 200/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4318 - acc: 0.8620 - val_loss: 0.5039 - val_acc: 0.8512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBLNbBPC4TEE",
        "colab_type": "code",
        "outputId": "90b50986-c6e3-4ba2-d3b6-b5af2c77030f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "results = model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 1s 66us/step\n",
            "Test accuracy:  0.8523809523809524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2fMRQt-6aOf",
        "colab_type": "text"
      },
      "source": [
        "Comparable scores with the previous model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeZ4mYje4zlH",
        "colab_type": "code",
        "outputId": "a64e4605-011e-491e-a72e-dd472ed5616c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Let's add one more layer\n",
        "\n",
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(248*4, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(248*4, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu')) \n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(248*4, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu')) \n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(248*4, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.adam(lr = 0.0001)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=2048 ,epochs = 200, verbose = 1, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33600 samples, validate on 18000 samples\n",
            "Epoch 1/200\n",
            "33600/33600 [==============================] - 2s 68us/step - loss: 2.9415 - acc: 0.1016 - val_loss: 2.2952 - val_acc: 0.1313\n",
            "Epoch 2/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.8570 - acc: 0.1057 - val_loss: 2.2604 - val_acc: 0.1674\n",
            "Epoch 3/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.7947 - acc: 0.1118 - val_loss: 2.2273 - val_acc: 0.1969\n",
            "Epoch 4/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.7527 - acc: 0.1173 - val_loss: 2.2049 - val_acc: 0.2260\n",
            "Epoch 5/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.7250 - acc: 0.1183 - val_loss: 2.1755 - val_acc: 0.2593\n",
            "Epoch 6/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.6694 - acc: 0.1282 - val_loss: 2.1536 - val_acc: 0.2764\n",
            "Epoch 7/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.6301 - acc: 0.1345 - val_loss: 2.1203 - val_acc: 0.2952\n",
            "Epoch 8/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.5846 - acc: 0.1423 - val_loss: 2.0827 - val_acc: 0.3194\n",
            "Epoch 9/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.5243 - acc: 0.1541 - val_loss: 2.0380 - val_acc: 0.3452\n",
            "Epoch 10/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 2.4629 - acc: 0.1675 - val_loss: 1.9919 - val_acc: 0.3623\n",
            "Epoch 11/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.3963 - acc: 0.1860 - val_loss: 1.9652 - val_acc: 0.3523\n",
            "Epoch 12/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 2.3307 - acc: 0.2048 - val_loss: 1.9201 - val_acc: 0.3712\n",
            "Epoch 13/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.2576 - acc: 0.2228 - val_loss: 1.8844 - val_acc: 0.3705\n",
            "Epoch 14/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 2.1640 - acc: 0.2545 - val_loss: 1.8410 - val_acc: 0.3764\n",
            "Epoch 15/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.0950 - acc: 0.2691 - val_loss: 1.8156 - val_acc: 0.3807\n",
            "Epoch 16/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 2.0304 - acc: 0.2942 - val_loss: 1.7592 - val_acc: 0.4160\n",
            "Epoch 17/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.9610 - acc: 0.3139 - val_loss: 1.7261 - val_acc: 0.4243\n",
            "Epoch 18/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.9020 - acc: 0.3349 - val_loss: 1.6597 - val_acc: 0.4511\n",
            "Epoch 19/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.8359 - acc: 0.3577 - val_loss: 1.5836 - val_acc: 0.4638\n",
            "Epoch 20/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.7815 - acc: 0.3751 - val_loss: 1.5489 - val_acc: 0.5031\n",
            "Epoch 21/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 1.7309 - acc: 0.3935 - val_loss: 1.4603 - val_acc: 0.5245\n",
            "Epoch 22/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.6930 - acc: 0.4071 - val_loss: 1.4326 - val_acc: 0.5442\n",
            "Epoch 23/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.6323 - acc: 0.4326 - val_loss: 1.4034 - val_acc: 0.5473\n",
            "Epoch 24/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.5954 - acc: 0.4412 - val_loss: 1.3808 - val_acc: 0.5557\n",
            "Epoch 25/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.5521 - acc: 0.4610 - val_loss: 1.3044 - val_acc: 0.5824\n",
            "Epoch 26/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.5100 - acc: 0.4769 - val_loss: 1.1991 - val_acc: 0.6214\n",
            "Epoch 27/200\n",
            "33600/33600 [==============================] - 1s 15us/step - loss: 1.4822 - acc: 0.4901 - val_loss: 1.1735 - val_acc: 0.6297\n",
            "Epoch 28/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.4489 - acc: 0.5034 - val_loss: 1.1728 - val_acc: 0.6371\n",
            "Epoch 29/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 1.4187 - acc: 0.5152 - val_loss: 1.1180 - val_acc: 0.6504\n",
            "Epoch 30/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.3882 - acc: 0.5304 - val_loss: 1.0779 - val_acc: 0.6669\n",
            "Epoch 31/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.3638 - acc: 0.5349 - val_loss: 1.0592 - val_acc: 0.6664\n",
            "Epoch 32/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.3298 - acc: 0.5505 - val_loss: 1.0286 - val_acc: 0.6779\n",
            "Epoch 33/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.3127 - acc: 0.5575 - val_loss: 1.0320 - val_acc: 0.6737\n",
            "Epoch 34/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 1.2837 - acc: 0.5698 - val_loss: 0.9926 - val_acc: 0.6883\n",
            "Epoch 35/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 1.2628 - acc: 0.5760 - val_loss: 0.9820 - val_acc: 0.6938\n",
            "Epoch 36/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.2391 - acc: 0.5859 - val_loss: 0.9503 - val_acc: 0.7057\n",
            "Epoch 37/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.2225 - acc: 0.5928 - val_loss: 0.9421 - val_acc: 0.7069\n",
            "Epoch 38/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.1968 - acc: 0.6032 - val_loss: 0.9410 - val_acc: 0.7039\n",
            "Epoch 39/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.1804 - acc: 0.6097 - val_loss: 0.9065 - val_acc: 0.7187\n",
            "Epoch 40/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.1674 - acc: 0.6146 - val_loss: 0.8909 - val_acc: 0.7213\n",
            "Epoch 41/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.1450 - acc: 0.6212 - val_loss: 0.9178 - val_acc: 0.7181\n",
            "Epoch 42/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 1.1423 - acc: 0.6243 - val_loss: 0.8745 - val_acc: 0.7267\n",
            "Epoch 43/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.1211 - acc: 0.6340 - val_loss: 0.8676 - val_acc: 0.7289\n",
            "Epoch 44/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 1.1044 - acc: 0.6406 - val_loss: 0.8743 - val_acc: 0.7291\n",
            "Epoch 45/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0896 - acc: 0.6453 - val_loss: 0.8332 - val_acc: 0.7420\n",
            "Epoch 46/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0797 - acc: 0.6488 - val_loss: 0.8375 - val_acc: 0.7388\n",
            "Epoch 47/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0584 - acc: 0.6537 - val_loss: 0.8163 - val_acc: 0.7497\n",
            "Epoch 48/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0554 - acc: 0.6601 - val_loss: 0.8290 - val_acc: 0.7469\n",
            "Epoch 49/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 1.0291 - acc: 0.6680 - val_loss: 0.8208 - val_acc: 0.7487\n",
            "Epoch 50/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0212 - acc: 0.6698 - val_loss: 0.7889 - val_acc: 0.7571\n",
            "Epoch 51/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 1.0177 - acc: 0.6710 - val_loss: 0.8089 - val_acc: 0.7514\n",
            "Epoch 52/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9981 - acc: 0.6761 - val_loss: 0.8290 - val_acc: 0.7454\n",
            "Epoch 53/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9863 - acc: 0.6803 - val_loss: 0.7986 - val_acc: 0.7552\n",
            "Epoch 54/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9783 - acc: 0.6869 - val_loss: 0.7835 - val_acc: 0.7592\n",
            "Epoch 55/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.9639 - acc: 0.6935 - val_loss: 0.7539 - val_acc: 0.7699\n",
            "Epoch 56/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9567 - acc: 0.6913 - val_loss: 0.7932 - val_acc: 0.7559\n",
            "Epoch 57/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.9478 - acc: 0.6963 - val_loss: 0.7570 - val_acc: 0.7678\n",
            "Epoch 58/200\n",
            "33600/33600 [==============================] - 1s 15us/step - loss: 0.9389 - acc: 0.6986 - val_loss: 0.7586 - val_acc: 0.7693\n",
            "Epoch 59/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.9360 - acc: 0.6998 - val_loss: 0.7688 - val_acc: 0.7653\n",
            "Epoch 60/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.9204 - acc: 0.7044 - val_loss: 0.7527 - val_acc: 0.7704\n",
            "Epoch 61/200\n",
            "33600/33600 [==============================] - 1s 15us/step - loss: 0.9107 - acc: 0.7075 - val_loss: 0.7265 - val_acc: 0.7799\n",
            "Epoch 62/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8981 - acc: 0.7138 - val_loss: 0.7158 - val_acc: 0.7825\n",
            "Epoch 63/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.8942 - acc: 0.7143 - val_loss: 0.7261 - val_acc: 0.7797\n",
            "Epoch 64/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8894 - acc: 0.7155 - val_loss: 0.7151 - val_acc: 0.7853\n",
            "Epoch 65/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8782 - acc: 0.7167 - val_loss: 0.7199 - val_acc: 0.7816\n",
            "Epoch 66/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8748 - acc: 0.7227 - val_loss: 0.7256 - val_acc: 0.7775\n",
            "Epoch 67/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.8674 - acc: 0.7197 - val_loss: 0.7016 - val_acc: 0.7866\n",
            "Epoch 68/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.8590 - acc: 0.7255 - val_loss: 0.7149 - val_acc: 0.7828\n",
            "Epoch 69/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8461 - acc: 0.7291 - val_loss: 0.7072 - val_acc: 0.7876\n",
            "Epoch 70/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.8403 - acc: 0.7303 - val_loss: 0.6962 - val_acc: 0.7900\n",
            "Epoch 71/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8424 - acc: 0.7317 - val_loss: 0.6858 - val_acc: 0.7906\n",
            "Epoch 72/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.8321 - acc: 0.7357 - val_loss: 0.6887 - val_acc: 0.7934\n",
            "Epoch 73/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8239 - acc: 0.7367 - val_loss: 0.6748 - val_acc: 0.7950\n",
            "Epoch 74/200\n",
            "33600/33600 [==============================] - 1s 16us/step - loss: 0.8229 - acc: 0.7388 - val_loss: 0.7090 - val_acc: 0.7883\n",
            "Epoch 75/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8121 - acc: 0.7406 - val_loss: 0.6678 - val_acc: 0.7966\n",
            "Epoch 76/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.8029 - acc: 0.7434 - val_loss: 0.6810 - val_acc: 0.7958\n",
            "Epoch 77/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.8023 - acc: 0.7421 - val_loss: 0.6667 - val_acc: 0.7982\n",
            "Epoch 78/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.7975 - acc: 0.7476 - val_loss: 0.6643 - val_acc: 0.8010\n",
            "Epoch 79/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7919 - acc: 0.7491 - val_loss: 0.6669 - val_acc: 0.7969\n",
            "Epoch 80/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.7876 - acc: 0.7514 - val_loss: 0.6724 - val_acc: 0.7977\n",
            "Epoch 81/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7770 - acc: 0.7507 - val_loss: 0.6592 - val_acc: 0.8010\n",
            "Epoch 82/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7747 - acc: 0.7521 - val_loss: 0.6621 - val_acc: 0.8022\n",
            "Epoch 83/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7678 - acc: 0.7557 - val_loss: 0.6747 - val_acc: 0.7975\n",
            "Epoch 84/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7624 - acc: 0.7590 - val_loss: 0.6339 - val_acc: 0.8083\n",
            "Epoch 85/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7490 - acc: 0.7629 - val_loss: 0.6357 - val_acc: 0.8092\n",
            "Epoch 86/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7621 - acc: 0.7589 - val_loss: 0.6312 - val_acc: 0.8123\n",
            "Epoch 87/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7535 - acc: 0.7631 - val_loss: 0.6683 - val_acc: 0.7988\n",
            "Epoch 88/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.7539 - acc: 0.7592 - val_loss: 0.7051 - val_acc: 0.7879\n",
            "Epoch 89/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.7420 - acc: 0.7636 - val_loss: 0.6668 - val_acc: 0.7993\n",
            "Epoch 90/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7378 - acc: 0.7651 - val_loss: 0.6302 - val_acc: 0.8123\n",
            "Epoch 91/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7283 - acc: 0.7704 - val_loss: 0.6195 - val_acc: 0.8149\n",
            "Epoch 92/200\n",
            "33600/33600 [==============================] - 1s 15us/step - loss: 0.7283 - acc: 0.7698 - val_loss: 0.6172 - val_acc: 0.8149\n",
            "Epoch 93/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.7195 - acc: 0.7746 - val_loss: 0.6141 - val_acc: 0.8136\n",
            "Epoch 94/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7107 - acc: 0.7756 - val_loss: 0.6241 - val_acc: 0.8152\n",
            "Epoch 95/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7154 - acc: 0.7748 - val_loss: 0.6409 - val_acc: 0.8086\n",
            "Epoch 96/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7102 - acc: 0.7740 - val_loss: 0.6152 - val_acc: 0.8143\n",
            "Epoch 97/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7100 - acc: 0.7765 - val_loss: 0.6067 - val_acc: 0.8179\n",
            "Epoch 98/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.7037 - acc: 0.7774 - val_loss: 0.6377 - val_acc: 0.8088\n",
            "Epoch 99/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.7053 - acc: 0.7780 - val_loss: 0.6215 - val_acc: 0.8124\n",
            "Epoch 100/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6966 - acc: 0.7796 - val_loss: 0.6212 - val_acc: 0.8128\n",
            "Epoch 101/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6912 - acc: 0.7793 - val_loss: 0.6003 - val_acc: 0.8179\n",
            "Epoch 102/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6917 - acc: 0.7796 - val_loss: 0.5959 - val_acc: 0.8213\n",
            "Epoch 103/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.6824 - acc: 0.7819 - val_loss: 0.6081 - val_acc: 0.8167\n",
            "Epoch 104/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6752 - acc: 0.7826 - val_loss: 0.5988 - val_acc: 0.8229\n",
            "Epoch 105/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.6802 - acc: 0.7838 - val_loss: 0.6273 - val_acc: 0.8122\n",
            "Epoch 106/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6735 - acc: 0.7860 - val_loss: 0.5911 - val_acc: 0.8249\n",
            "Epoch 107/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6678 - acc: 0.7880 - val_loss: 0.5985 - val_acc: 0.8219\n",
            "Epoch 108/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6599 - acc: 0.7913 - val_loss: 0.5875 - val_acc: 0.8237\n",
            "Epoch 109/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6595 - acc: 0.7922 - val_loss: 0.5925 - val_acc: 0.8236\n",
            "Epoch 110/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6571 - acc: 0.7896 - val_loss: 0.5961 - val_acc: 0.8234\n",
            "Epoch 111/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6594 - acc: 0.7902 - val_loss: 0.5822 - val_acc: 0.8253\n",
            "Epoch 112/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6455 - acc: 0.7935 - val_loss: 0.5853 - val_acc: 0.8259\n",
            "Epoch 113/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6473 - acc: 0.7956 - val_loss: 0.5785 - val_acc: 0.8249\n",
            "Epoch 114/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.6469 - acc: 0.7956 - val_loss: 0.5948 - val_acc: 0.8239\n",
            "Epoch 115/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6477 - acc: 0.7964 - val_loss: 0.5787 - val_acc: 0.8268\n",
            "Epoch 116/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.6385 - acc: 0.7987 - val_loss: 0.6246 - val_acc: 0.8142\n",
            "Epoch 117/200\n",
            "33600/33600 [==============================] - 1s 15us/step - loss: 0.6372 - acc: 0.7966 - val_loss: 0.5579 - val_acc: 0.8342\n",
            "Epoch 118/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6359 - acc: 0.7982 - val_loss: 0.5706 - val_acc: 0.8310\n",
            "Epoch 119/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6324 - acc: 0.8018 - val_loss: 0.5773 - val_acc: 0.8295\n",
            "Epoch 120/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6312 - acc: 0.7990 - val_loss: 0.5950 - val_acc: 0.8239\n",
            "Epoch 121/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6275 - acc: 0.8013 - val_loss: 0.5813 - val_acc: 0.8284\n",
            "Epoch 122/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6245 - acc: 0.8013 - val_loss: 0.5732 - val_acc: 0.8297\n",
            "Epoch 123/200\n",
            "33600/33600 [==============================] - 1s 15us/step - loss: 0.6149 - acc: 0.8052 - val_loss: 0.5634 - val_acc: 0.8323\n",
            "Epoch 124/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6123 - acc: 0.8052 - val_loss: 0.5772 - val_acc: 0.8279\n",
            "Epoch 125/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.6136 - acc: 0.8040 - val_loss: 0.5825 - val_acc: 0.8305\n",
            "Epoch 126/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6076 - acc: 0.8077 - val_loss: 0.5820 - val_acc: 0.8285\n",
            "Epoch 127/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6057 - acc: 0.8077 - val_loss: 0.5577 - val_acc: 0.8351\n",
            "Epoch 128/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.6120 - acc: 0.8058 - val_loss: 0.5621 - val_acc: 0.8332\n",
            "Epoch 129/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.5996 - acc: 0.8121 - val_loss: 0.5508 - val_acc: 0.8364\n",
            "Epoch 130/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5977 - acc: 0.8114 - val_loss: 0.5698 - val_acc: 0.8296\n",
            "Epoch 131/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5952 - acc: 0.8095 - val_loss: 0.5568 - val_acc: 0.8340\n",
            "Epoch 132/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5967 - acc: 0.8124 - val_loss: 0.5481 - val_acc: 0.8377\n",
            "Epoch 133/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5879 - acc: 0.8140 - val_loss: 0.5857 - val_acc: 0.8276\n",
            "Epoch 134/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5938 - acc: 0.8154 - val_loss: 0.6097 - val_acc: 0.8193\n",
            "Epoch 135/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5872 - acc: 0.8136 - val_loss: 0.5855 - val_acc: 0.8291\n",
            "Epoch 136/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.5877 - acc: 0.8154 - val_loss: 0.5498 - val_acc: 0.8352\n",
            "Epoch 137/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5881 - acc: 0.8136 - val_loss: 0.5515 - val_acc: 0.8370\n",
            "Epoch 138/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5866 - acc: 0.8151 - val_loss: 0.5535 - val_acc: 0.8376\n",
            "Epoch 139/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5785 - acc: 0.8186 - val_loss: 0.5546 - val_acc: 0.8357\n",
            "Epoch 140/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5719 - acc: 0.8174 - val_loss: 0.5526 - val_acc: 0.8366\n",
            "Epoch 141/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5751 - acc: 0.8170 - val_loss: 0.5561 - val_acc: 0.8353\n",
            "Epoch 142/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.5772 - acc: 0.8153 - val_loss: 0.5471 - val_acc: 0.8376\n",
            "Epoch 143/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5699 - acc: 0.8184 - val_loss: 0.5819 - val_acc: 0.8284\n",
            "Epoch 144/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5710 - acc: 0.8177 - val_loss: 0.5392 - val_acc: 0.8394\n",
            "Epoch 145/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.5608 - acc: 0.8231 - val_loss: 0.5832 - val_acc: 0.8291\n",
            "Epoch 146/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5612 - acc: 0.8207 - val_loss: 0.5392 - val_acc: 0.8407\n",
            "Epoch 147/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5636 - acc: 0.8214 - val_loss: 0.5363 - val_acc: 0.8426\n",
            "Epoch 148/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5554 - acc: 0.8250 - val_loss: 0.6143 - val_acc: 0.8173\n",
            "Epoch 149/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5613 - acc: 0.8229 - val_loss: 0.5490 - val_acc: 0.8371\n",
            "Epoch 150/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5550 - acc: 0.8228 - val_loss: 0.5367 - val_acc: 0.8407\n",
            "Epoch 151/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5486 - acc: 0.8242 - val_loss: 0.5446 - val_acc: 0.8371\n",
            "Epoch 152/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5497 - acc: 0.8261 - val_loss: 0.5320 - val_acc: 0.8424\n",
            "Epoch 153/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5476 - acc: 0.8247 - val_loss: 0.5246 - val_acc: 0.8452\n",
            "Epoch 154/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5373 - acc: 0.8294 - val_loss: 0.5319 - val_acc: 0.8445\n",
            "Epoch 155/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5384 - acc: 0.8284 - val_loss: 0.5422 - val_acc: 0.8403\n",
            "Epoch 156/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5392 - acc: 0.8285 - val_loss: 0.5286 - val_acc: 0.8422\n",
            "Epoch 157/200\n",
            "33600/33600 [==============================] - 1s 15us/step - loss: 0.5368 - acc: 0.8303 - val_loss: 0.5342 - val_acc: 0.8436\n",
            "Epoch 158/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5358 - acc: 0.8293 - val_loss: 0.5327 - val_acc: 0.8439\n",
            "Epoch 159/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5286 - acc: 0.8326 - val_loss: 0.5198 - val_acc: 0.8461\n",
            "Epoch 160/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5311 - acc: 0.8310 - val_loss: 0.5329 - val_acc: 0.8426\n",
            "Epoch 161/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.5356 - acc: 0.8298 - val_loss: 0.5602 - val_acc: 0.8373\n",
            "Epoch 162/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5353 - acc: 0.8317 - val_loss: 0.5486 - val_acc: 0.8368\n",
            "Epoch 163/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5307 - acc: 0.8328 - val_loss: 0.5544 - val_acc: 0.8361\n",
            "Epoch 164/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5285 - acc: 0.8315 - val_loss: 0.5246 - val_acc: 0.8423\n",
            "Epoch 165/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5239 - acc: 0.8325 - val_loss: 0.5179 - val_acc: 0.8507\n",
            "Epoch 166/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5184 - acc: 0.8344 - val_loss: 0.5469 - val_acc: 0.8389\n",
            "Epoch 167/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5194 - acc: 0.8349 - val_loss: 0.5252 - val_acc: 0.8456\n",
            "Epoch 168/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5193 - acc: 0.8353 - val_loss: 0.5357 - val_acc: 0.8427\n",
            "Epoch 169/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5175 - acc: 0.8366 - val_loss: 0.5308 - val_acc: 0.8432\n",
            "Epoch 170/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5135 - acc: 0.8372 - val_loss: 0.5157 - val_acc: 0.8492\n",
            "Epoch 171/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5099 - acc: 0.8376 - val_loss: 0.5201 - val_acc: 0.8443\n",
            "Epoch 172/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.5082 - acc: 0.8385 - val_loss: 0.5330 - val_acc: 0.8451\n",
            "Epoch 173/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5095 - acc: 0.8387 - val_loss: 0.5166 - val_acc: 0.8459\n",
            "Epoch 174/200\n",
            "33600/33600 [==============================] - 0s 15us/step - loss: 0.5099 - acc: 0.8374 - val_loss: 0.5364 - val_acc: 0.8414\n",
            "Epoch 175/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5087 - acc: 0.8388 - val_loss: 0.5267 - val_acc: 0.8446\n",
            "Epoch 176/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5075 - acc: 0.8385 - val_loss: 0.5297 - val_acc: 0.8436\n",
            "Epoch 177/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.5042 - acc: 0.8391 - val_loss: 0.5378 - val_acc: 0.8406\n",
            "Epoch 178/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4933 - acc: 0.8433 - val_loss: 0.5068 - val_acc: 0.8508\n",
            "Epoch 179/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4988 - acc: 0.8421 - val_loss: 0.5438 - val_acc: 0.8388\n",
            "Epoch 180/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4949 - acc: 0.8406 - val_loss: 0.5028 - val_acc: 0.8525\n",
            "Epoch 181/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4939 - acc: 0.8434 - val_loss: 0.5536 - val_acc: 0.8385\n",
            "Epoch 182/200\n",
            "33600/33600 [==============================] - 1s 15us/step - loss: 0.4954 - acc: 0.8439 - val_loss: 0.5026 - val_acc: 0.8509\n",
            "Epoch 183/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4915 - acc: 0.8462 - val_loss: 0.5115 - val_acc: 0.8489\n",
            "Epoch 184/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4879 - acc: 0.8436 - val_loss: 0.5342 - val_acc: 0.8424\n",
            "Epoch 185/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4921 - acc: 0.8433 - val_loss: 0.5652 - val_acc: 0.8316\n",
            "Epoch 186/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4878 - acc: 0.8462 - val_loss: 0.5140 - val_acc: 0.8486\n",
            "Epoch 187/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4850 - acc: 0.8462 - val_loss: 0.5143 - val_acc: 0.8473\n",
            "Epoch 188/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4878 - acc: 0.8438 - val_loss: 0.5169 - val_acc: 0.8502\n",
            "Epoch 189/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4849 - acc: 0.8476 - val_loss: 0.4998 - val_acc: 0.8529\n",
            "Epoch 190/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4799 - acc: 0.8483 - val_loss: 0.5312 - val_acc: 0.8454\n",
            "Epoch 191/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4796 - acc: 0.8461 - val_loss: 0.5280 - val_acc: 0.8434\n",
            "Epoch 192/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4792 - acc: 0.8457 - val_loss: 0.4977 - val_acc: 0.8546\n",
            "Epoch 193/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4805 - acc: 0.8476 - val_loss: 0.4929 - val_acc: 0.8564\n",
            "Epoch 194/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4811 - acc: 0.8450 - val_loss: 0.5205 - val_acc: 0.8491\n",
            "Epoch 195/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4725 - acc: 0.8501 - val_loss: 0.5162 - val_acc: 0.8483\n",
            "Epoch 196/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4768 - acc: 0.8481 - val_loss: 0.5215 - val_acc: 0.8451\n",
            "Epoch 197/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4682 - acc: 0.8495 - val_loss: 0.4965 - val_acc: 0.8541\n",
            "Epoch 198/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4673 - acc: 0.8531 - val_loss: 0.5091 - val_acc: 0.8503\n",
            "Epoch 199/200\n",
            "33600/33600 [==============================] - 1s 15us/step - loss: 0.4669 - acc: 0.8518 - val_loss: 0.5233 - val_acc: 0.8463\n",
            "Epoch 200/200\n",
            "33600/33600 [==============================] - 0s 14us/step - loss: 0.4643 - acc: 0.8521 - val_loss: 0.5416 - val_acc: 0.8419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5WzQAx04zhh",
        "colab_type": "code",
        "outputId": "5d96f140-cc65-4c35-ac39-95ed2995f019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(X_val, y_val)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400/8400 [==============================] - 1s 80us/step\n",
            "Test accuracy:  0.8422619047619048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVQ_NxaU4zb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b43jbsPD6Vef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U57V3AO6lLN",
        "colab_type": "text"
      },
      "source": [
        "#Build a Convnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNVU8g9N6WP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Activation, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "\n",
        "ds=h5py.File('/content/drive/My Drive/Colab Notebooks/AIML/ANN/Project SVNH - NN & DL/SVHN_single_grey1.h5','r')\n",
        "\n",
        "X_train = ds['X_train'][:]\n",
        "y_train = ds['y_train'][:]\n",
        "X_test = ds['X_test'][:]\n",
        "y_test = ds['y_test'][:]\n",
        "\n",
        "# Close this file\n",
        "ds.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI6_O5mC6WfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting y data into categorical (one-hot encoding)\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "798WjXHz6WlB",
        "colab_type": "code",
        "outputId": "67e7bfb0-75e0-4c54-f12a-0e6def3e1677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, random_state=1, test_size=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train,y_train, random_state=1, test_size=0.1)\n",
        "\n",
        "X_train.shape, X_test.shape , X_val.shape,y_train.shape, y_test.shape, y_val.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((34020, 32, 32),\n",
              " (3780, 32, 32),\n",
              " (4200, 32, 32),\n",
              " (34020, 10),\n",
              " (3780, 10),\n",
              " (4200, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ommCD9Hk6XMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape = (32, 32, 1), activation = 'relu', padding = 'same'))\n",
        "model.add(MaxPooling2D(pool_size = (3, 3)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation = 'relu', padding = 'same'))\n",
        "model.add(MaxPooling2D(pool_size = (3, 3)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(248*3, kernel_initializer='he_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu')) \n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "model.add(Dense(128, kernel_initializer='he_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu')) \n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uE2hPw06XG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=X_train.reshape((34020,32,32,1))\n",
        "X_train=X_train.astype('float32')/255\n",
        "\n",
        "X_test=X_test.reshape((3780,32,32,1))\n",
        "X_test=X_test.astype('float32')/255\n",
        "\n",
        "X_val=X_val.reshape((4200,32,32,1))\n",
        "X_val=X_val.astype('float32')/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmB0VDI26XCX",
        "colab_type": "code",
        "outputId": "c77d72f0-2bae-47c2-dad0-0a536ae91b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "X_train.shape, X_test.shape , X_val.shape,y_train.shape, y_test.shape, y_val.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((34020, 32, 32, 1),\n",
              " (3780, 32, 32, 1),\n",
              " (4200, 32, 32, 1),\n",
              " (34020, 10),\n",
              " (3780, 10),\n",
              " (4200, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLrhEu2I6W91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = optimizers.adam(lr = 0.0001)\n",
        "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODrlEJyn6W7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia_Ka-Pf6W4l",
        "colab_type": "code",
        "outputId": "b4617946-662d-465a-c31e-21b61f2d144e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history=model.fit(X_train, y_train, epochs=100, batch_size=512, validation_data=(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 34020 samples, validate on 4200 samples\n",
            "Epoch 1/100\n",
            "34020/34020 [==============================] - 3s 98us/step - loss: 2.7306 - acc: 0.1146 - val_loss: 2.2789 - val_acc: 0.1607\n",
            "Epoch 2/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 2.3945 - acc: 0.1867 - val_loss: 2.0761 - val_acc: 0.2583\n",
            "Epoch 3/100\n",
            "34020/34020 [==============================] - 1s 26us/step - loss: 2.0624 - acc: 0.2947 - val_loss: 1.8309 - val_acc: 0.3693\n",
            "Epoch 4/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 1.7151 - acc: 0.4220 - val_loss: 1.5674 - val_acc: 0.5267\n",
            "Epoch 5/100\n",
            "34020/34020 [==============================] - 1s 26us/step - loss: 1.4527 - acc: 0.5234 - val_loss: 1.3579 - val_acc: 0.6019\n",
            "Epoch 6/100\n",
            "34020/34020 [==============================] - 1s 26us/step - loss: 1.2698 - acc: 0.5891 - val_loss: 1.1440 - val_acc: 0.7024\n",
            "Epoch 7/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 1.1277 - acc: 0.6406 - val_loss: 1.0052 - val_acc: 0.7221\n",
            "Epoch 8/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 1.0238 - acc: 0.6772 - val_loss: 0.8913 - val_acc: 0.7621\n",
            "Epoch 9/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.9515 - acc: 0.7036 - val_loss: 0.8112 - val_acc: 0.7702\n",
            "Epoch 10/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.8813 - acc: 0.7255 - val_loss: 0.7321 - val_acc: 0.8019\n",
            "Epoch 11/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.8310 - acc: 0.7403 - val_loss: 0.7147 - val_acc: 0.7919\n",
            "Epoch 12/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.7900 - acc: 0.7541 - val_loss: 0.6910 - val_acc: 0.7919\n",
            "Epoch 13/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.7546 - acc: 0.7646 - val_loss: 0.6263 - val_acc: 0.8226\n",
            "Epoch 14/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.7190 - acc: 0.7783 - val_loss: 0.5863 - val_acc: 0.8343\n",
            "Epoch 15/100\n",
            "34020/34020 [==============================] - 1s 26us/step - loss: 0.6908 - acc: 0.7855 - val_loss: 0.5884 - val_acc: 0.8217\n",
            "Epoch 16/100\n",
            "34020/34020 [==============================] - 1s 26us/step - loss: 0.6701 - acc: 0.7911 - val_loss: 0.5683 - val_acc: 0.8288\n",
            "Epoch 17/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.6448 - acc: 0.7999 - val_loss: 0.5521 - val_acc: 0.8398\n",
            "Epoch 18/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.6261 - acc: 0.8060 - val_loss: 0.5289 - val_acc: 0.8426\n",
            "Epoch 19/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.6127 - acc: 0.8096 - val_loss: 0.5227 - val_acc: 0.8436\n",
            "Epoch 20/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.5932 - acc: 0.8160 - val_loss: 0.5137 - val_acc: 0.8467\n",
            "Epoch 21/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.5799 - acc: 0.8203 - val_loss: 0.4947 - val_acc: 0.8552\n",
            "Epoch 22/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.5634 - acc: 0.8255 - val_loss: 0.5266 - val_acc: 0.8400\n",
            "Epoch 23/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.5439 - acc: 0.8325 - val_loss: 0.4783 - val_acc: 0.8574\n",
            "Epoch 24/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.5357 - acc: 0.8351 - val_loss: 0.4689 - val_acc: 0.8610\n",
            "Epoch 25/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.5266 - acc: 0.8378 - val_loss: 0.4753 - val_acc: 0.8571\n",
            "Epoch 26/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.5150 - acc: 0.8415 - val_loss: 0.4698 - val_acc: 0.8583\n",
            "Epoch 27/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.5001 - acc: 0.8456 - val_loss: 0.4819 - val_acc: 0.8526\n",
            "Epoch 28/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.4920 - acc: 0.8479 - val_loss: 0.4494 - val_acc: 0.8648\n",
            "Epoch 29/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.4845 - acc: 0.8495 - val_loss: 0.4369 - val_acc: 0.8660\n",
            "Epoch 30/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.4731 - acc: 0.8546 - val_loss: 0.4372 - val_acc: 0.8648\n",
            "Epoch 31/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.4678 - acc: 0.8550 - val_loss: 0.4319 - val_acc: 0.8698\n",
            "Epoch 32/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.4593 - acc: 0.8580 - val_loss: 0.4281 - val_acc: 0.8702\n",
            "Epoch 33/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.4504 - acc: 0.8591 - val_loss: 0.4216 - val_acc: 0.8738\n",
            "Epoch 34/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.4448 - acc: 0.8641 - val_loss: 0.4199 - val_acc: 0.8721\n",
            "Epoch 35/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.4352 - acc: 0.8632 - val_loss: 0.4165 - val_acc: 0.8724\n",
            "Epoch 36/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.4306 - acc: 0.8676 - val_loss: 0.4215 - val_acc: 0.8705\n",
            "Epoch 37/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.4254 - acc: 0.8690 - val_loss: 0.4135 - val_acc: 0.8717\n",
            "Epoch 38/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.4164 - acc: 0.8718 - val_loss: 0.4074 - val_acc: 0.8743\n",
            "Epoch 39/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.4069 - acc: 0.8737 - val_loss: 0.4102 - val_acc: 0.8743\n",
            "Epoch 40/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.4064 - acc: 0.8732 - val_loss: 0.4081 - val_acc: 0.8740\n",
            "Epoch 41/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.3940 - acc: 0.8795 - val_loss: 0.4040 - val_acc: 0.8731\n",
            "Epoch 42/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.3885 - acc: 0.8788 - val_loss: 0.4144 - val_acc: 0.8702\n",
            "Epoch 43/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.3857 - acc: 0.8804 - val_loss: 0.4027 - val_acc: 0.8760\n",
            "Epoch 44/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.3795 - acc: 0.8833 - val_loss: 0.4085 - val_acc: 0.8740\n",
            "Epoch 45/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.3728 - acc: 0.8848 - val_loss: 0.3929 - val_acc: 0.8764\n",
            "Epoch 46/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.3638 - acc: 0.8884 - val_loss: 0.3933 - val_acc: 0.8812\n",
            "Epoch 47/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.3611 - acc: 0.8889 - val_loss: 0.3891 - val_acc: 0.8781\n",
            "Epoch 48/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.3574 - acc: 0.8879 - val_loss: 0.4287 - val_acc: 0.8693\n",
            "Epoch 49/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.3527 - acc: 0.8896 - val_loss: 0.3782 - val_acc: 0.8829\n",
            "Epoch 50/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.3454 - acc: 0.8939 - val_loss: 0.3890 - val_acc: 0.8795\n",
            "Epoch 51/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.3394 - acc: 0.8937 - val_loss: 0.3891 - val_acc: 0.8762\n",
            "Epoch 52/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.3370 - acc: 0.8959 - val_loss: 0.3904 - val_acc: 0.8781\n",
            "Epoch 53/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.3327 - acc: 0.8965 - val_loss: 0.4226 - val_acc: 0.8679\n",
            "Epoch 54/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.3297 - acc: 0.8978 - val_loss: 0.3810 - val_acc: 0.8810\n",
            "Epoch 55/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.3220 - acc: 0.9012 - val_loss: 0.3727 - val_acc: 0.8819\n",
            "Epoch 56/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.3193 - acc: 0.9004 - val_loss: 0.3900 - val_acc: 0.8788\n",
            "Epoch 57/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.3121 - acc: 0.9036 - val_loss: 0.3863 - val_acc: 0.8798\n",
            "Epoch 58/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.3111 - acc: 0.9049 - val_loss: 0.3851 - val_acc: 0.8790\n",
            "Epoch 59/100\n",
            "34020/34020 [==============================] - 1s 26us/step - loss: 0.3068 - acc: 0.9058 - val_loss: 0.3943 - val_acc: 0.8750\n",
            "Epoch 60/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.3021 - acc: 0.9067 - val_loss: 0.3759 - val_acc: 0.8848\n",
            "Epoch 61/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2975 - acc: 0.9088 - val_loss: 0.3693 - val_acc: 0.8867\n",
            "Epoch 62/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.2956 - acc: 0.9089 - val_loss: 0.3880 - val_acc: 0.8783\n",
            "Epoch 63/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2883 - acc: 0.9129 - val_loss: 0.3953 - val_acc: 0.8786\n",
            "Epoch 64/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.2830 - acc: 0.9119 - val_loss: 0.3620 - val_acc: 0.8902\n",
            "Epoch 65/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.2844 - acc: 0.9118 - val_loss: 0.3704 - val_acc: 0.8874\n",
            "Epoch 66/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.2829 - acc: 0.9117 - val_loss: 0.3646 - val_acc: 0.8879\n",
            "Epoch 67/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2750 - acc: 0.9153 - val_loss: 0.3704 - val_acc: 0.8867\n",
            "Epoch 68/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2688 - acc: 0.9163 - val_loss: 0.3797 - val_acc: 0.8840\n",
            "Epoch 69/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2681 - acc: 0.9158 - val_loss: 0.3688 - val_acc: 0.8833\n",
            "Epoch 70/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2668 - acc: 0.9180 - val_loss: 0.3782 - val_acc: 0.8821\n",
            "Epoch 71/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2588 - acc: 0.9193 - val_loss: 0.3926 - val_acc: 0.8750\n",
            "Epoch 72/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2601 - acc: 0.9199 - val_loss: 0.3671 - val_acc: 0.8855\n",
            "Epoch 73/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2520 - acc: 0.9232 - val_loss: 0.3923 - val_acc: 0.8769\n",
            "Epoch 74/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2521 - acc: 0.9213 - val_loss: 0.3486 - val_acc: 0.8921\n",
            "Epoch 75/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2485 - acc: 0.9228 - val_loss: 0.3694 - val_acc: 0.8862\n",
            "Epoch 76/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.2375 - acc: 0.9253 - val_loss: 0.3760 - val_acc: 0.8879\n",
            "Epoch 77/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2383 - acc: 0.9254 - val_loss: 0.3740 - val_acc: 0.8850\n",
            "Epoch 78/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2392 - acc: 0.9262 - val_loss: 0.4007 - val_acc: 0.8819\n",
            "Epoch 79/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2297 - acc: 0.9292 - val_loss: 0.3613 - val_acc: 0.8886\n",
            "Epoch 80/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2306 - acc: 0.9290 - val_loss: 0.3554 - val_acc: 0.8921\n",
            "Epoch 81/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.2257 - acc: 0.9302 - val_loss: 0.3512 - val_acc: 0.8950\n",
            "Epoch 82/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.2232 - acc: 0.9303 - val_loss: 0.3618 - val_acc: 0.8960\n",
            "Epoch 83/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.2227 - acc: 0.9317 - val_loss: 0.3563 - val_acc: 0.8924\n",
            "Epoch 84/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2187 - acc: 0.9325 - val_loss: 0.3848 - val_acc: 0.8843\n",
            "Epoch 85/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2105 - acc: 0.9353 - val_loss: 0.3862 - val_acc: 0.8845\n",
            "Epoch 86/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2155 - acc: 0.9338 - val_loss: 0.3638 - val_acc: 0.8924\n",
            "Epoch 87/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2100 - acc: 0.9356 - val_loss: 0.4060 - val_acc: 0.8812\n",
            "Epoch 88/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2061 - acc: 0.9352 - val_loss: 0.3651 - val_acc: 0.8917\n",
            "Epoch 89/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2050 - acc: 0.9369 - val_loss: 0.3538 - val_acc: 0.8981\n",
            "Epoch 90/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.2009 - acc: 0.9376 - val_loss: 0.3775 - val_acc: 0.8864\n",
            "Epoch 91/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.1997 - acc: 0.9375 - val_loss: 0.3627 - val_acc: 0.8895\n",
            "Epoch 92/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.1970 - acc: 0.9394 - val_loss: 0.3712 - val_acc: 0.8874\n",
            "Epoch 93/100\n",
            "34020/34020 [==============================] - 1s 26us/step - loss: 0.1921 - acc: 0.9406 - val_loss: 0.3731 - val_acc: 0.8917\n",
            "Epoch 94/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.1945 - acc: 0.9390 - val_loss: 0.3841 - val_acc: 0.8836\n",
            "Epoch 95/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.1892 - acc: 0.9408 - val_loss: 0.3518 - val_acc: 0.8971\n",
            "Epoch 96/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.1843 - acc: 0.9425 - val_loss: 0.3660 - val_acc: 0.8950\n",
            "Epoch 97/100\n",
            "34020/34020 [==============================] - 1s 26us/step - loss: 0.1801 - acc: 0.9439 - val_loss: 0.3558 - val_acc: 0.8929\n",
            "Epoch 98/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.1770 - acc: 0.9451 - val_loss: 0.3541 - val_acc: 0.8960\n",
            "Epoch 99/100\n",
            "34020/34020 [==============================] - 1s 24us/step - loss: 0.1777 - acc: 0.9446 - val_loss: 0.3582 - val_acc: 0.8967\n",
            "Epoch 100/100\n",
            "34020/34020 [==============================] - 1s 25us/step - loss: 0.1757 - acc: 0.9460 - val_loss: 0.3625 - val_acc: 0.8921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf8mXcJS6Wt_",
        "colab_type": "code",
        "outputId": "eaa2650d-4942-4ce8-e889-7d2314d99e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3780/3780 [==============================] - 0s 75us/step\n",
            "Test accuracy:  0.8955026455026455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n87nK8yp6Wpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXqkKXi06WOO",
        "colab_type": "code",
        "outputId": "8811b7c3-fedc-4c14-ab72-23103d5f6ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "NN_val_loss = history.history['val_loss']\n",
        "NN_train_loss = history.history['loss']\n",
        "NN_val_acc = history.history['val_acc']\n",
        "NN_train_acc = history.history['acc']\n",
        "\n",
        "\n",
        "epochs = range(1,101)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, NN_val_loss, 'b', label='Validation Loss')\n",
        "plt.plot(epochs, NN_train_loss, 'r', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, NN_val_acc, 'b', label='Validation Acc')\n",
        "plt.plot(epochs, NN_train_acc, 'r', label='Training Acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f96a169f390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAE9CAYAAABDUbVaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSIriAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV-4VTYw6WJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G13yKeWk6WFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb02H65hx6m3",
        "colab_type": "text"
      },
      "source": [
        "Build the network again different architecture\n",
        "\n",
        "\n",
        "*   Introduce an additional convolution layer\n",
        "*   Increase the dimension of the filter of the first layer of the NN\n",
        "*   Add drop out before flattening to reduce dimension and prevent overfitting\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzMSwgUc6WBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (5, 5), input_shape = (32, 32, 1), activation = 'relu', padding = 'same'))\n",
        "model.add(MaxPooling2D(pool_size = (3, 3)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation = 'relu', padding = 'same'))\n",
        "model.add(MaxPooling2D(pool_size = (3, 3)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation = 'relu', padding = 'same'))\n",
        "model.add(MaxPooling2D(pool_size = (3, 3)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1024, kernel_initializer='he_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu')) \n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "model.add(Dense(128, kernel_initializer='he_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu')) \n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVpgEHF-6V7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = optimizers.adam(lr = 0.001)\n",
        "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dcSXXRh6V2m",
        "colab_type": "code",
        "outputId": "9cf4c03c-0b5f-46c1-e710-4fc233984e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history=model.fit(X_train, y_train, epochs=100, batch_size=512, validation_data=(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 34020 samples, validate on 4200 samples\n",
            "Epoch 1/100\n",
            "34020/34020 [==============================] - 7s 214us/step - loss: 0.5304 - acc: 0.8402 - val_loss: 0.4916 - val_acc: 0.8569\n",
            "Epoch 2/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.3784 - acc: 0.8826 - val_loss: 0.4099 - val_acc: 0.8748\n",
            "Epoch 3/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.3882 - acc: 0.8786 - val_loss: 0.4451 - val_acc: 0.8733\n",
            "Epoch 4/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.3549 - acc: 0.8883 - val_loss: 0.4261 - val_acc: 0.8714\n",
            "Epoch 5/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.3470 - acc: 0.8925 - val_loss: 0.4225 - val_acc: 0.8719\n",
            "Epoch 6/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.3355 - acc: 0.8950 - val_loss: 0.3731 - val_acc: 0.8924\n",
            "Epoch 7/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.3312 - acc: 0.8980 - val_loss: 0.4318 - val_acc: 0.8743\n",
            "Epoch 8/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.3270 - acc: 0.8984 - val_loss: 0.3949 - val_acc: 0.8881\n",
            "Epoch 9/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.3118 - acc: 0.9043 - val_loss: 0.3651 - val_acc: 0.8969\n",
            "Epoch 10/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.3072 - acc: 0.9062 - val_loss: 0.3817 - val_acc: 0.8910\n",
            "Epoch 11/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.3099 - acc: 0.9037 - val_loss: 0.3742 - val_acc: 0.8919\n",
            "Epoch 12/100\n",
            "34020/34020 [==============================] - 1s 30us/step - loss: 0.3060 - acc: 0.9069 - val_loss: 0.4178 - val_acc: 0.8821\n",
            "Epoch 13/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2942 - acc: 0.9088 - val_loss: 0.3473 - val_acc: 0.9048\n",
            "Epoch 14/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.2816 - acc: 0.9136 - val_loss: 0.3782 - val_acc: 0.8833\n",
            "Epoch 15/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2763 - acc: 0.9134 - val_loss: 0.3982 - val_acc: 0.8817\n",
            "Epoch 16/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2766 - acc: 0.9151 - val_loss: 0.3694 - val_acc: 0.8917\n",
            "Epoch 17/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2768 - acc: 0.9155 - val_loss: 0.4195 - val_acc: 0.8812\n",
            "Epoch 18/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.2758 - acc: 0.9155 - val_loss: 0.3616 - val_acc: 0.8979\n",
            "Epoch 19/100\n",
            "34020/34020 [==============================] - 1s 27us/step - loss: 0.2572 - acc: 0.9220 - val_loss: 0.3552 - val_acc: 0.9019\n",
            "Epoch 20/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2519 - acc: 0.9236 - val_loss: 0.3899 - val_acc: 0.8924\n",
            "Epoch 21/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.2560 - acc: 0.9208 - val_loss: 0.3858 - val_acc: 0.8943\n",
            "Epoch 22/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.2489 - acc: 0.9223 - val_loss: 0.3759 - val_acc: 0.8971\n",
            "Epoch 23/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2462 - acc: 0.9239 - val_loss: 0.3714 - val_acc: 0.8990\n",
            "Epoch 24/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2378 - acc: 0.9273 - val_loss: 0.3413 - val_acc: 0.9026\n",
            "Epoch 25/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.2332 - acc: 0.9281 - val_loss: 0.4352 - val_acc: 0.8829\n",
            "Epoch 26/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2359 - acc: 0.9270 - val_loss: 0.3403 - val_acc: 0.9062\n",
            "Epoch 27/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2334 - acc: 0.9284 - val_loss: 0.3547 - val_acc: 0.9074\n",
            "Epoch 28/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2286 - acc: 0.9282 - val_loss: 0.3386 - val_acc: 0.9090\n",
            "Epoch 29/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2316 - acc: 0.9290 - val_loss: 0.3671 - val_acc: 0.8983\n",
            "Epoch 30/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.2211 - acc: 0.9305 - val_loss: 0.3519 - val_acc: 0.9000\n",
            "Epoch 31/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2188 - acc: 0.9332 - val_loss: 0.3822 - val_acc: 0.9012\n",
            "Epoch 32/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2172 - acc: 0.9347 - val_loss: 0.3849 - val_acc: 0.8917\n",
            "Epoch 33/100\n",
            "34020/34020 [==============================] - 1s 30us/step - loss: 0.2118 - acc: 0.9350 - val_loss: 0.3528 - val_acc: 0.9040\n",
            "Epoch 34/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2093 - acc: 0.9349 - val_loss: 0.3670 - val_acc: 0.9031\n",
            "Epoch 35/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2044 - acc: 0.9368 - val_loss: 0.3506 - val_acc: 0.9043\n",
            "Epoch 36/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2100 - acc: 0.9335 - val_loss: 0.3879 - val_acc: 0.8971\n",
            "Epoch 37/100\n",
            "34020/34020 [==============================] - 1s 30us/step - loss: 0.1993 - acc: 0.9385 - val_loss: 0.3361 - val_acc: 0.9079\n",
            "Epoch 38/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2023 - acc: 0.9380 - val_loss: 0.3438 - val_acc: 0.9102\n",
            "Epoch 39/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.2014 - acc: 0.9382 - val_loss: 0.3790 - val_acc: 0.8998\n",
            "Epoch 40/100\n",
            "34020/34020 [==============================] - 1s 30us/step - loss: 0.1985 - acc: 0.9378 - val_loss: 0.3522 - val_acc: 0.9079\n",
            "Epoch 41/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1884 - acc: 0.9406 - val_loss: 0.3399 - val_acc: 0.9081\n",
            "Epoch 42/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1967 - acc: 0.9395 - val_loss: 0.4007 - val_acc: 0.8969\n",
            "Epoch 43/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1965 - acc: 0.9390 - val_loss: 0.3560 - val_acc: 0.9083\n",
            "Epoch 44/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1825 - acc: 0.9431 - val_loss: 0.3377 - val_acc: 0.9093\n",
            "Epoch 45/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1816 - acc: 0.9436 - val_loss: 0.3633 - val_acc: 0.9052\n",
            "Epoch 46/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1809 - acc: 0.9445 - val_loss: 0.3647 - val_acc: 0.9026\n",
            "Epoch 47/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1833 - acc: 0.9427 - val_loss: 0.3500 - val_acc: 0.9069\n",
            "Epoch 48/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1811 - acc: 0.9446 - val_loss: 0.3358 - val_acc: 0.9081\n",
            "Epoch 49/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1805 - acc: 0.9441 - val_loss: 0.3422 - val_acc: 0.9069\n",
            "Epoch 50/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1766 - acc: 0.9442 - val_loss: 0.3365 - val_acc: 0.9110\n",
            "Epoch 51/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1670 - acc: 0.9479 - val_loss: 0.3473 - val_acc: 0.9064\n",
            "Epoch 52/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1767 - acc: 0.9442 - val_loss: 0.3524 - val_acc: 0.9086\n",
            "Epoch 53/100\n",
            "34020/34020 [==============================] - 1s 30us/step - loss: 0.1646 - acc: 0.9489 - val_loss: 0.3477 - val_acc: 0.9100\n",
            "Epoch 54/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1686 - acc: 0.9481 - val_loss: 0.3554 - val_acc: 0.9064\n",
            "Epoch 55/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1608 - acc: 0.9504 - val_loss: 0.3554 - val_acc: 0.9098\n",
            "Epoch 56/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1604 - acc: 0.9500 - val_loss: 0.3440 - val_acc: 0.9136\n",
            "Epoch 57/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1595 - acc: 0.9497 - val_loss: 0.3549 - val_acc: 0.9093\n",
            "Epoch 58/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1614 - acc: 0.9498 - val_loss: 0.3546 - val_acc: 0.9112\n",
            "Epoch 59/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1604 - acc: 0.9500 - val_loss: 0.3710 - val_acc: 0.9038\n",
            "Epoch 60/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1639 - acc: 0.9490 - val_loss: 0.3823 - val_acc: 0.9071\n",
            "Epoch 61/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1590 - acc: 0.9509 - val_loss: 0.3538 - val_acc: 0.9098\n",
            "Epoch 62/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1471 - acc: 0.9541 - val_loss: 0.3452 - val_acc: 0.9098\n",
            "Epoch 63/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1510 - acc: 0.9544 - val_loss: 0.3594 - val_acc: 0.9090\n",
            "Epoch 64/100\n",
            "34020/34020 [==============================] - 1s 30us/step - loss: 0.1427 - acc: 0.9556 - val_loss: 0.3563 - val_acc: 0.9112\n",
            "Epoch 65/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1522 - acc: 0.9529 - val_loss: 0.3637 - val_acc: 0.9100\n",
            "Epoch 66/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1493 - acc: 0.9537 - val_loss: 0.3708 - val_acc: 0.9102\n",
            "Epoch 67/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1463 - acc: 0.9542 - val_loss: 0.3570 - val_acc: 0.9102\n",
            "Epoch 68/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1459 - acc: 0.9544 - val_loss: 0.3592 - val_acc: 0.9083\n",
            "Epoch 69/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1560 - acc: 0.9514 - val_loss: 0.3694 - val_acc: 0.9069\n",
            "Epoch 70/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1449 - acc: 0.9551 - val_loss: 0.3728 - val_acc: 0.9062\n",
            "Epoch 71/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1538 - acc: 0.9519 - val_loss: 0.3752 - val_acc: 0.9076\n",
            "Epoch 72/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1455 - acc: 0.9546 - val_loss: 0.3686 - val_acc: 0.9105\n",
            "Epoch 73/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1376 - acc: 0.9562 - val_loss: 0.3871 - val_acc: 0.9019\n",
            "Epoch 74/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1366 - acc: 0.9567 - val_loss: 0.3563 - val_acc: 0.9131\n",
            "Epoch 75/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1400 - acc: 0.9567 - val_loss: 0.3801 - val_acc: 0.9026\n",
            "Epoch 76/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1418 - acc: 0.9577 - val_loss: 0.3774 - val_acc: 0.9100\n",
            "Epoch 77/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1349 - acc: 0.9580 - val_loss: 0.4141 - val_acc: 0.9029\n",
            "Epoch 78/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1363 - acc: 0.9566 - val_loss: 0.3923 - val_acc: 0.9107\n",
            "Epoch 79/100\n",
            "34020/34020 [==============================] - 1s 30us/step - loss: 0.1349 - acc: 0.9579 - val_loss: 0.4073 - val_acc: 0.8988\n",
            "Epoch 80/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1398 - acc: 0.9566 - val_loss: 0.3643 - val_acc: 0.9126\n",
            "Epoch 81/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1286 - acc: 0.9603 - val_loss: 0.3887 - val_acc: 0.9067\n",
            "Epoch 82/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1279 - acc: 0.9601 - val_loss: 0.3955 - val_acc: 0.9050\n",
            "Epoch 83/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1399 - acc: 0.9563 - val_loss: 0.3811 - val_acc: 0.9124\n",
            "Epoch 84/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1319 - acc: 0.9588 - val_loss: 0.3772 - val_acc: 0.9088\n",
            "Epoch 85/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1270 - acc: 0.9608 - val_loss: 0.3872 - val_acc: 0.9052\n",
            "Epoch 86/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1333 - acc: 0.9580 - val_loss: 0.3702 - val_acc: 0.9074\n",
            "Epoch 87/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1374 - acc: 0.9571 - val_loss: 0.3912 - val_acc: 0.9088\n",
            "Epoch 88/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1209 - acc: 0.9623 - val_loss: 0.3661 - val_acc: 0.9100\n",
            "Epoch 89/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1286 - acc: 0.9599 - val_loss: 0.3615 - val_acc: 0.9124\n",
            "Epoch 90/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1227 - acc: 0.9611 - val_loss: 0.3761 - val_acc: 0.9055\n",
            "Epoch 91/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1199 - acc: 0.9618 - val_loss: 0.3705 - val_acc: 0.9129\n",
            "Epoch 92/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1171 - acc: 0.9624 - val_loss: 0.3806 - val_acc: 0.9060\n",
            "Epoch 93/100\n",
            "34020/34020 [==============================] - 1s 28us/step - loss: 0.1268 - acc: 0.9618 - val_loss: 0.3926 - val_acc: 0.9112\n",
            "Epoch 94/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1154 - acc: 0.9633 - val_loss: 0.3867 - val_acc: 0.9133\n",
            "Epoch 95/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1180 - acc: 0.9635 - val_loss: 0.3742 - val_acc: 0.9129\n",
            "Epoch 96/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1170 - acc: 0.9628 - val_loss: 0.3721 - val_acc: 0.9138\n",
            "Epoch 97/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1168 - acc: 0.9635 - val_loss: 0.3875 - val_acc: 0.9133\n",
            "Epoch 98/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1210 - acc: 0.9632 - val_loss: 0.3758 - val_acc: 0.9143\n",
            "Epoch 99/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1178 - acc: 0.9634 - val_loss: 0.3902 - val_acc: 0.9071\n",
            "Epoch 100/100\n",
            "34020/34020 [==============================] - 1s 29us/step - loss: 0.1206 - acc: 0.9621 - val_loss: 0.3738 - val_acc: 0.9140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPHpp1nNgAQa",
        "colab_type": "code",
        "outputId": "ca11dabf-87f4-442c-82f5-f1e65cb0545c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3780/3780 [==============================] - 0s 103us/step\n",
            "Test accuracy:  0.9095238095238095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEC4mSyw6VtS",
        "colab_type": "code",
        "outputId": "91c53a02-c200-4d3a-f9ba-63f0d68fce44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "NN_val_loss = history.history['val_loss']\n",
        "NN_train_loss = history.history['loss']\n",
        "NN_val_acc = history.history['val_acc']\n",
        "NN_train_acc = history.history['acc']\n",
        "\n",
        "\n",
        "epochs = range(1,101)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, NN_val_loss, 'b', label='Validation Loss')\n",
        "plt.plot(epochs, NN_train_loss, 'r', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, NN_val_acc, 'b', label='Validation Acc')\n",
        "plt.plot(epochs, NN_train_acc, 'r', label='Training Acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f96a3622978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAE9CAYAAABOT8UdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlbZ3cE_XljQ",
        "colab_type": "text"
      },
      "source": [
        "Best results achieved so far with an accuracy of 91% on validation data.\n",
        "\n",
        "#Lets Try adding data augmentation to see if the performance improves any further"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-W6_E7_6Vo1",
        "colab_type": "code",
        "outputId": "80a7cf51-2dcb-4466-f6e2-09e315f7177b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "ds=h5py.File('/content/drive/My Drive/Colab Notebooks/AIML/ANN/Project SVNH - NN & DL/SVHN_single_grey1.h5','r')\n",
        "\n",
        "X_train = ds['X_train'][:]\n",
        "y_train = ds['y_train'][:]\n",
        "X_test = ds['X_test'][:]\n",
        "y_test = ds['y_test'][:]\n",
        "\n",
        "# Close this file\n",
        "ds.close()\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# converting y data into categorical (one-hot encoding)\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, random_state=1, test_size=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train,y_train, random_state=1, test_size=0.1)\n",
        "\n",
        "X_train.shape, X_test.shape , X_val.shape,y_train.shape, y_test.shape, y_val.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((34020, 32, 32),\n",
              " (3780, 32, 32),\n",
              " (4200, 32, 32),\n",
              " (34020, 10),\n",
              " (3780, 10),\n",
              " (4200, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4PRF3s9r_dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=X_train.reshape((34020,32,32,1))\n",
        "X_train=X_train.astype('float32')/255\n",
        "\n",
        "X_test=X_test.reshape((3780,32,32,1))\n",
        "X_test=X_test.astype('float32')/255\n",
        "\n",
        "X_val=X_val.reshape((4200,32,32,1))\n",
        "X_val=X_val.astype('float32')/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVquRU3TXuKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (5, 5), input_shape = (32, 32, 1), activation = 'relu', padding = 'same'))\n",
        "model.add(MaxPooling2D(pool_size = (3, 3)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation = 'relu', padding = 'same'))\n",
        "model.add(MaxPooling2D(pool_size = (3, 3)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512, kernel_initializer='he_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu')) \n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Dense(256, kernel_initializer='he_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu')) \n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Dense(10, kernel_initializer='he_normal'))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "adam = optimizers.adam(lr = 0.001)\n",
        "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frssMgsRsbfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fl1b0G_XuNV",
        "colab_type": "code",
        "outputId": "33ae853c-d6c0-4bf2-a76c-6ae3b60f9d98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " \n",
        "batch_size = 256\n",
        "epochs = 100\n",
        "datagen = ImageDataGenerator(rescale=1./255,\n",
        "        zoom_range=0.1, # randomly zoom into images\n",
        "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "  \n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "                              steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))),\n",
        "                              epochs=epochs,\n",
        "                              validation_data=(X_val, y_val),\n",
        "                              workers=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "133/133 [==============================] - 21s 161ms/step - loss: 1.8650 - acc: 0.3318 - val_loss: 11.3260 - val_acc: 0.2807\n",
            "Epoch 2/100\n",
            "133/133 [==============================] - 11s 85ms/step - loss: 1.2284 - acc: 0.5865 - val_loss: 11.4187 - val_acc: 0.2833\n",
            "Epoch 3/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 1.0538 - acc: 0.6452 - val_loss: 11.9896 - val_acc: 0.2502\n",
            "Epoch 4/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.9692 - acc: 0.6779 - val_loss: 12.3609 - val_acc: 0.2281\n",
            "Epoch 5/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.8911 - acc: 0.7049 - val_loss: 12.4025 - val_acc: 0.2238\n",
            "Epoch 6/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.8291 - acc: 0.7244 - val_loss: 11.6846 - val_acc: 0.2679\n",
            "Epoch 7/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.7956 - acc: 0.7377 - val_loss: 10.5659 - val_acc: 0.3314\n",
            "Epoch 8/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.7595 - acc: 0.7472 - val_loss: 11.2331 - val_acc: 0.2910\n",
            "Epoch 9/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.7245 - acc: 0.7616 - val_loss: 11.0982 - val_acc: 0.2998\n",
            "Epoch 10/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.7151 - acc: 0.7654 - val_loss: 11.4087 - val_acc: 0.2788\n",
            "Epoch 11/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.6898 - acc: 0.7749 - val_loss: 9.5680 - val_acc: 0.3862\n",
            "Epoch 12/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.6758 - acc: 0.7804 - val_loss: 10.4781 - val_acc: 0.3390\n",
            "Epoch 13/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.6541 - acc: 0.7885 - val_loss: 10.1523 - val_acc: 0.3514\n",
            "Epoch 14/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.6331 - acc: 0.7937 - val_loss: 10.5180 - val_acc: 0.3333\n",
            "Epoch 15/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.6239 - acc: 0.8001 - val_loss: 11.3028 - val_acc: 0.2888\n",
            "Epoch 16/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.6111 - acc: 0.8015 - val_loss: 10.6086 - val_acc: 0.3319\n",
            "Epoch 17/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.5984 - acc: 0.8034 - val_loss: 10.7819 - val_acc: 0.3219\n",
            "Epoch 18/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.5894 - acc: 0.8101 - val_loss: 10.5789 - val_acc: 0.3321\n",
            "Epoch 19/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.5835 - acc: 0.8123 - val_loss: 10.7870 - val_acc: 0.3162\n",
            "Epoch 20/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.5672 - acc: 0.8148 - val_loss: 10.9663 - val_acc: 0.3079\n",
            "Epoch 21/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.5569 - acc: 0.8203 - val_loss: 10.7927 - val_acc: 0.3193\n",
            "Epoch 22/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.5475 - acc: 0.8223 - val_loss: 10.2439 - val_acc: 0.3483\n",
            "Epoch 23/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.5551 - acc: 0.8189 - val_loss: 11.1026 - val_acc: 0.3000\n",
            "Epoch 24/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.5519 - acc: 0.8214 - val_loss: 11.1325 - val_acc: 0.2981\n",
            "Epoch 25/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.5292 - acc: 0.8281 - val_loss: 10.7414 - val_acc: 0.3214\n",
            "Epoch 26/100\n",
            "133/133 [==============================] - 12s 93ms/step - loss: 0.5445 - acc: 0.8245 - val_loss: 11.1128 - val_acc: 0.2964\n",
            "Epoch 27/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.5278 - acc: 0.8291 - val_loss: 10.3327 - val_acc: 0.3433\n",
            "Epoch 28/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.5207 - acc: 0.8304 - val_loss: 11.1411 - val_acc: 0.2960\n",
            "Epoch 29/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.5316 - acc: 0.8279 - val_loss: 10.5012 - val_acc: 0.3360\n",
            "Epoch 30/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.5091 - acc: 0.8327 - val_loss: 10.0072 - val_acc: 0.3660\n",
            "Epoch 31/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.5148 - acc: 0.8361 - val_loss: 11.0740 - val_acc: 0.3005\n",
            "Epoch 32/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.5023 - acc: 0.8360 - val_loss: 10.1994 - val_acc: 0.3536\n",
            "Epoch 33/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4981 - acc: 0.8383 - val_loss: 10.2610 - val_acc: 0.3424\n",
            "Epoch 34/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4967 - acc: 0.8391 - val_loss: 10.5235 - val_acc: 0.3319\n",
            "Epoch 35/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4892 - acc: 0.8402 - val_loss: 10.2373 - val_acc: 0.3517\n",
            "Epoch 36/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.4860 - acc: 0.8427 - val_loss: 10.8087 - val_acc: 0.3133\n",
            "Epoch 37/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4785 - acc: 0.8425 - val_loss: 11.1170 - val_acc: 0.3029\n",
            "Epoch 38/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4762 - acc: 0.8452 - val_loss: 10.7728 - val_acc: 0.3198\n",
            "Epoch 39/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4824 - acc: 0.8431 - val_loss: 12.1351 - val_acc: 0.2398\n",
            "Epoch 40/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.5045 - acc: 0.8362 - val_loss: 11.2702 - val_acc: 0.2895\n",
            "Epoch 41/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4890 - acc: 0.8415 - val_loss: 10.6326 - val_acc: 0.3314\n",
            "Epoch 42/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.4871 - acc: 0.8433 - val_loss: 11.4166 - val_acc: 0.2807\n",
            "Epoch 43/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4864 - acc: 0.8411 - val_loss: 11.1129 - val_acc: 0.2945\n",
            "Epoch 44/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4826 - acc: 0.8433 - val_loss: 11.1217 - val_acc: 0.2979\n",
            "Epoch 45/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.4938 - acc: 0.8399 - val_loss: 11.2372 - val_acc: 0.2898\n",
            "Epoch 46/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4886 - acc: 0.8406 - val_loss: 10.4387 - val_acc: 0.3405\n",
            "Epoch 47/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.4756 - acc: 0.8466 - val_loss: 11.5603 - val_acc: 0.2719\n",
            "Epoch 48/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4801 - acc: 0.8458 - val_loss: 10.1814 - val_acc: 0.3571\n",
            "Epoch 49/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.4689 - acc: 0.8492 - val_loss: 10.1884 - val_acc: 0.3555\n",
            "Epoch 50/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4719 - acc: 0.8456 - val_loss: 10.2662 - val_acc: 0.3483\n",
            "Epoch 51/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4633 - acc: 0.8479 - val_loss: 9.8197 - val_acc: 0.3740\n",
            "Epoch 52/100\n",
            "133/133 [==============================] - 12s 94ms/step - loss: 0.4584 - acc: 0.8502 - val_loss: 10.9026 - val_acc: 0.3117\n",
            "Epoch 53/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4521 - acc: 0.8519 - val_loss: 10.7880 - val_acc: 0.3176\n",
            "Epoch 54/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4467 - acc: 0.8551 - val_loss: 10.7210 - val_acc: 0.3224\n",
            "Epoch 55/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4439 - acc: 0.8551 - val_loss: 10.0560 - val_acc: 0.3648\n",
            "Epoch 56/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4500 - acc: 0.8549 - val_loss: 10.3491 - val_acc: 0.3443\n",
            "Epoch 57/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4631 - acc: 0.8474 - val_loss: 11.5391 - val_acc: 0.2764\n",
            "Epoch 58/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4823 - acc: 0.8399 - val_loss: 10.8766 - val_acc: 0.3188\n",
            "Epoch 59/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4614 - acc: 0.8498 - val_loss: 11.3112 - val_acc: 0.2898\n",
            "Epoch 60/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4557 - acc: 0.8514 - val_loss: 10.8538 - val_acc: 0.3183\n",
            "Epoch 61/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4487 - acc: 0.8551 - val_loss: 10.7091 - val_acc: 0.3281\n",
            "Epoch 62/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4369 - acc: 0.8581 - val_loss: 10.8803 - val_acc: 0.3160\n",
            "Epoch 63/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4347 - acc: 0.8582 - val_loss: 11.1216 - val_acc: 0.3019\n",
            "Epoch 64/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4403 - acc: 0.8563 - val_loss: 10.1919 - val_acc: 0.3576\n",
            "Epoch 65/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4365 - acc: 0.8592 - val_loss: 10.5598 - val_acc: 0.3340\n",
            "Epoch 66/100\n",
            "133/133 [==============================] - 12s 89ms/step - loss: 0.4260 - acc: 0.8615 - val_loss: 10.7109 - val_acc: 0.3274\n",
            "Epoch 67/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4229 - acc: 0.8649 - val_loss: 10.9875 - val_acc: 0.3119\n",
            "Epoch 68/100\n",
            "133/133 [==============================] - 12s 89ms/step - loss: 0.4252 - acc: 0.8621 - val_loss: 11.2242 - val_acc: 0.2952\n",
            "Epoch 69/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.4247 - acc: 0.8608 - val_loss: 11.0966 - val_acc: 0.2990\n",
            "Epoch 70/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.4227 - acc: 0.8638 - val_loss: 10.5031 - val_acc: 0.3402\n",
            "Epoch 71/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4228 - acc: 0.8631 - val_loss: 11.0029 - val_acc: 0.3090\n",
            "Epoch 72/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4414 - acc: 0.8572 - val_loss: 10.4971 - val_acc: 0.3398\n",
            "Epoch 73/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4267 - acc: 0.8621 - val_loss: 10.6373 - val_acc: 0.3314\n",
            "Epoch 74/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4177 - acc: 0.8652 - val_loss: 9.4076 - val_acc: 0.4036\n",
            "Epoch 75/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4179 - acc: 0.8626 - val_loss: 9.5307 - val_acc: 0.3964\n",
            "Epoch 76/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4177 - acc: 0.8645 - val_loss: 9.0348 - val_acc: 0.4236\n",
            "Epoch 77/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.4167 - acc: 0.8640 - val_loss: 9.1201 - val_acc: 0.4183\n",
            "Epoch 78/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.4142 - acc: 0.8660 - val_loss: 9.4335 - val_acc: 0.4038\n",
            "Epoch 79/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4181 - acc: 0.8619 - val_loss: 9.4063 - val_acc: 0.4014\n",
            "Epoch 80/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4046 - acc: 0.8679 - val_loss: 9.9387 - val_acc: 0.3695\n",
            "Epoch 81/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.4049 - acc: 0.8680 - val_loss: 10.3668 - val_acc: 0.3450\n",
            "Epoch 82/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4083 - acc: 0.8663 - val_loss: 9.7707 - val_acc: 0.3814\n",
            "Epoch 83/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.3982 - acc: 0.8694 - val_loss: 10.0555 - val_acc: 0.3624\n",
            "Epoch 84/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.4055 - acc: 0.8675 - val_loss: 10.0791 - val_acc: 0.3598\n",
            "Epoch 85/100\n",
            "133/133 [==============================] - 12s 89ms/step - loss: 0.3970 - acc: 0.8714 - val_loss: 9.3954 - val_acc: 0.4040\n",
            "Epoch 86/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.3977 - acc: 0.8692 - val_loss: 9.3151 - val_acc: 0.4057\n",
            "Epoch 87/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.3967 - acc: 0.8717 - val_loss: 9.4953 - val_acc: 0.3979\n",
            "Epoch 88/100\n",
            "133/133 [==============================] - 12s 89ms/step - loss: 0.3973 - acc: 0.8705 - val_loss: 9.8079 - val_acc: 0.3812\n",
            "Epoch 89/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.3994 - acc: 0.8679 - val_loss: 9.9559 - val_acc: 0.3676\n",
            "Epoch 90/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.3995 - acc: 0.8702 - val_loss: 9.9238 - val_acc: 0.3736\n",
            "Epoch 91/100\n",
            "133/133 [==============================] - 12s 89ms/step - loss: 0.3992 - acc: 0.8695 - val_loss: 10.5641 - val_acc: 0.3324\n",
            "Epoch 92/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4036 - acc: 0.8685 - val_loss: 9.9591 - val_acc: 0.3664\n",
            "Epoch 93/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.3899 - acc: 0.8729 - val_loss: 10.1129 - val_acc: 0.3617\n",
            "Epoch 94/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4172 - acc: 0.8617 - val_loss: 9.5678 - val_acc: 0.3993\n",
            "Epoch 95/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4459 - acc: 0.8551 - val_loss: 10.0134 - val_acc: 0.3695\n",
            "Epoch 96/100\n",
            "133/133 [==============================] - 12s 92ms/step - loss: 0.4453 - acc: 0.8537 - val_loss: 9.4323 - val_acc: 0.4029\n",
            "Epoch 97/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4314 - acc: 0.8596 - val_loss: 9.5621 - val_acc: 0.3960\n",
            "Epoch 98/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4309 - acc: 0.8601 - val_loss: 10.2874 - val_acc: 0.3517\n",
            "Epoch 99/100\n",
            "133/133 [==============================] - 12s 91ms/step - loss: 0.4219 - acc: 0.8618 - val_loss: 9.9979 - val_acc: 0.3700\n",
            "Epoch 100/100\n",
            "133/133 [==============================] - 12s 90ms/step - loss: 0.4187 - acc: 0.8625 - val_loss: 10.7120 - val_acc: 0.3283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aSgjscyXuW0",
        "colab_type": "code",
        "outputId": "6e1e689d-e6bc-4106-c73d-0a711ff80e8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3780/3780 [==============================] - 0s 102us/step\n",
            "Test accuracy:  0.5425925925925926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3dC-NWVKRXr",
        "colab_type": "text"
      },
      "source": [
        "#Try Ensemble of CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeVhRl1fXufX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CREATE MORE IMAGES VIA DATA AUGMENTATION\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=10,  \n",
        "        zoom_range = 0.10,  \n",
        "        width_shift_range=0.1, \n",
        "        height_shift_range=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7AQp_DCKn7d",
        "colab_type": "text"
      },
      "source": [
        "#Build 7 Convolutional Neural Networks!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrWqFkV3Xulm",
        "colab_type": "code",
        "outputId": "dbc1de00-9fcd-4dea-f203-f1c851fc4e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "nets = 7\n",
        "model = [0] *nets\n",
        "for j in range(nets):\n",
        "    model[j] = Sequential()\n",
        "\n",
        "    model[j].add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (32, 32, 1)))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Dropout(0.4))\n",
        "\n",
        "    model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Dropout(0.4))\n",
        "\n",
        "    model[j].add(Conv2D(128, kernel_size = 4, activation='relu'))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dropout(0.4))\n",
        "    model[j].add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzH-JYIIKxcM",
        "colab_type": "text"
      },
      "source": [
        "#Architectural highlights\n",
        "\n",
        "\n",
        "The CNNs in this kernel follow LeNet5's design (pictured above) with the following improvements:\n",
        "\n",
        "Two stacked 3x3 filters replace the single 5x5 filters. These become nonlinear 5x5 convolutions\n",
        "A convolution with stride 2 replaces pooling layers. These become learnable pooling layers.\n",
        "ReLU activation replaces sigmoid.\n",
        "Batch normalization is added\n",
        "Dropout is added\n",
        "More feature maps (channels) are added\n",
        "An ensemble of 15 CNNs with bagging is used\n",
        "Experiments (here) show that each of these changes improve classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3tmeSdhXujd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "8b3c73b0-2edf-46fd-a33e-9dfc20bcce8a"
      },
      "source": [
        "# DECREASE LEARNING RATE EACH EPOCH\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
        "# TRAIN NETWORKS\n",
        "history = [0] * nets\n",
        "epochs = 10 # changed from 45 to 10\n",
        "\n",
        "for j in range(nets):\n",
        "    X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, y_train, test_size = 0.1)\n",
        "    history[j] = model[j].fit_generator(datagen.flow(X_train2,Y_train2, batch_size=64),\n",
        "        epochs = epochs, steps_per_epoch = X_train2.shape[0]//64,  \n",
        "        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
        "        j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "CNN 1: Epochs=10, Train accuracy=0.90793, Validation accuracy=0.93886\n",
            "CNN 2: Epochs=10, Train accuracy=0.90630, Validation accuracy=0.94092\n",
            "CNN 3: Epochs=10, Train accuracy=0.90450, Validation accuracy=0.94327\n",
            "CNN 4: Epochs=10, Train accuracy=0.90695, Validation accuracy=0.94092\n",
            "CNN 5: Epochs=10, Train accuracy=0.90338, Validation accuracy=0.94562\n",
            "CNN 6: Epochs=10, Train accuracy=0.90293, Validation accuracy=0.93710\n",
            "CNN 7: Epochs=10, Train accuracy=0.90132, Validation accuracy=0.94503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El8UkJeaXucn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHIpnm_VXuZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ENSEMBLE PREDICTIONS AND SUBMIT\n",
        "results = np.zeros( (X_test.shape[0],10) ) \n",
        "for j in range(nets):\n",
        "    results = results + model[j].predict(X_test)\n",
        "results = np.argmax(results,axis = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hraLDzsRc1G3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bba102b1-bc54-435e-8058-ade57daab24d"
      },
      "source": [
        "X_train = ds['X_train'][:]\n",
        "y_train = ds['y_train'][:]\n",
        "X_test = ds['X_test'][:]\n",
        "y_test = ds['y_test'][:]\n",
        "\n",
        "# Close this file\n",
        "ds.close()\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, random_state=1, test_size=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train,y_train, random_state=1, test_size=0.1)\n",
        "\n",
        "X_train.shape, X_test.shape , X_val.shape,y_train.shape, y_test.shape, y_val.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((34020, 32, 32), (3780, 32, 32), (4200, 32, 32), (34020,), (3780,), (4200,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS6O4mlWd4bL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3e4eb393-6e94-4cd3-ed03-3e5881f2b957"
      },
      "source": [
        "results"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 3, 9, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBMfWW-Nc706",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f1fcf515-8ee5-4ee2-f288-71191078b0dc"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 1, 1, ..., 3, 9, 0], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po_y1SJ3eY1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "27333c68-be60-4e53-cc21-4ad480cc9845"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(results, y_test))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.91      0.94       391\n",
            "           1       0.94      0.93      0.93       396\n",
            "           2       0.96      0.97      0.97       379\n",
            "           3       0.94      0.95      0.94       388\n",
            "           4       0.97      0.93      0.95       392\n",
            "           5       0.97      0.96      0.96       374\n",
            "           6       0.91      0.96      0.93       355\n",
            "           7       0.98      0.96      0.97       383\n",
            "           8       0.92      0.96      0.94       364\n",
            "           9       0.94      0.96      0.95       358\n",
            "\n",
            "    accuracy                           0.95      3780\n",
            "   macro avg       0.95      0.95      0.95      3780\n",
            "weighted avg       0.95      0.95      0.95      3780\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UW0IoB6eku0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Best Results achived with Ensemble"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}